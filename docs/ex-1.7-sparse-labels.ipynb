{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1.7: Sparse labels (per-sample regularization)\n",
    "\n",
    "In previous experiments, we imposed structure on latent space with curriculum learning: varying the training data and hyperparameters over the course of the training run ([Ex 1.3](./ex-1.3-color-mlp-curriculum.ipynb), [Ex 1.5](./ex-1.5-color-mlp-anchoring.ipynb), [Ex 1.6](./ex-1.6-curriculum-comparison.ipynb)). It worked (in that the latent space looked _ok_), but we are unsure whether it worked because the color wheel was found and anchored _before_ expanding the data to include all colors (i.e. due to the curriculum), or whether it was just that the primary and secondary colors were regularized differently.\n",
    "\n",
    "In this experiment, we do away with the phased data curriculum, and instead apply per-sample regularization. Our hypothesis is that this will in fact outperform the curriculum-based methods, because the model will have access to the data full distribution from the start (limited only by batch size).\n",
    "\n",
    "We chose color as a domain because it's easy to reason about and visualize. But since our eventual goal is to apply these techniques to LLM training, we should consider how to constrain the labels in a way that could realistically be replicated for text. We assume that:\n",
    "\n",
    "1. An LLM would be trained with something like internet text\n",
    "2. Sentiment analysis could be run over it to generate labels — attributes that we care about, such as \"malicious\", \"benign\", \"honest\", etc.\n",
    "3. Such labelling would not be entirely accurate.\n",
    "\n",
    "Therefore for this experiment, we will apply the following regigme:\n",
    "\n",
    "1. An autoencoder trained on a color cube, as an analog for an LLM\n",
    "2. Certain colors are given labels (e.g. $(1,0,0)=red$)\n",
    "3. The labelling will be noisy, e.g. $(1,0,0)$ won't _always_ be labelled as $red$, and sometimes other colors close to red will be given that label.\n",
    "\n",
    "Certain regularizers will be activated based on those labels, e.g. colors labelled $red$ could be penalized for not being embedded at $(1,0,0,0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from utils.logging import SimpleLoggingConfig\n",
    "\n",
    "logging_config = (\n",
    "    SimpleLoggingConfig()\n",
    "    .info('notebook', 'utils', 'mini', 'ex_color')\n",
    "    .error('matplotlib.axes')  # Silence warnings about set_aspect\n",
    ")\n",
    "logging_config.apply()\n",
    "\n",
    "# ID for tagging assets\n",
    "nbid = '1.7'\n",
    "# This is the logger for this notebook\n",
    "log = logging.getLogger(f'notebook.{nbid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "We use the same simple 2-layer MLP autoencoder with a 4D bottleneck as in previous experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "E = 4\n",
    "\n",
    "\n",
    "class ColorMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # RGB input (3D) → hidden layer → bottleneck → hidden layer → RGB output\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.GELU(),\n",
    "            # nn.Linear(16, 16),\n",
    "            # nn.GELU(),\n",
    "            nn.Linear(16, E),  # Our critical bottleneck!\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(E, 16),\n",
    "            nn.GELU(),\n",
    "            # nn.Linear(16, 16),\n",
    "            # nn.GELU(),\n",
    "            nn.Linear(16, 3),\n",
    "            nn.Sigmoid(),  # Keep RGB values in [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Get our bottleneck representation\n",
    "        bottleneck = self.encoder(x)\n",
    "\n",
    "        # Decode back to RGB\n",
    "        output = self.decoder(bottleneck)\n",
    "        return output, bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training machinery with timeline and events\n",
    "\n",
    "The `train_color_model` function orchestrates the training process based on a `Timeline` derived from the dopesheet. It handles:\n",
    "\n",
    "- Iterating through training steps.\n",
    "- Fetching the correct data loader for the current phase.\n",
    "- Updating hyperparameters (like learning rate and loss weights) smoothly based on the timeline state.\n",
    "- Calculating the combined loss from reconstruction and various regularizers.\n",
    "- Executing the optimizer step.\n",
    "- Emitting events at different points (phase start/end, pre-step, actions like 'anchor', step metrics) to trigger callbacks like plotting, recording, or updating loss terms.\n",
    "\n",
    "Improvement over Ex 1.6: regularizers are applied with diffent weights for each sample based on the sample labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Protocol, runtime_checkable\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "from mini.temporal.timeline import State\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InferenceResult:\n",
    "    outputs: Tensor\n",
    "    latents: Tensor\n",
    "\n",
    "    def detach(self):\n",
    "        return InferenceResult(self.outputs.detach(), self.latents.detach())\n",
    "\n",
    "    def clone(self):\n",
    "        return InferenceResult(self.outputs.clone(), self.latents.clone())\n",
    "\n",
    "    def cpu(self):\n",
    "        return InferenceResult(self.outputs.cpu(), self.latents.cpu())\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class LossCriterion(Protocol):\n",
    "    def __call__(self, data: Tensor, res: InferenceResult) -> Tensor: ...\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RegularizerConfig:\n",
    "    \"\"\"Configuration for a regularizer, including label affinities.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    \"\"\"Matched with hyperparameter for weighting\"\"\"\n",
    "    criterion: LossCriterion\n",
    "    label_affinities: dict[str, float] | None\n",
    "    \"\"\"Maps label names to affinity strengths\"\"\"\n",
    "\n",
    "\n",
    "@dataclass(eq=False, frozen=True)\n",
    "class Event:\n",
    "    name: str\n",
    "    step: int\n",
    "    model: ColorMLP\n",
    "    timeline_state: State\n",
    "    optimizer: optim.Optimizer\n",
    "\n",
    "\n",
    "@dataclass(eq=False, frozen=True)\n",
    "class PhaseEndEvent(Event):\n",
    "    validation_data: Tensor\n",
    "    inference_result: InferenceResult\n",
    "\n",
    "\n",
    "@dataclass(eq=False, frozen=True)\n",
    "class StepMetricsEvent(Event):\n",
    "    \"\"\"Event carrying metrics calculated during a training step.\"\"\"\n",
    "\n",
    "    train_batch: Tensor\n",
    "    total_loss: float\n",
    "    losses: dict[str, float]\n",
    "\n",
    "\n",
    "class EventHandler[T](Protocol):\n",
    "    def __call__(self, event: T) -> None: ...\n",
    "\n",
    "\n",
    "class EventBinding[T]:\n",
    "    \"\"\"A class to bind events to handlers.\"\"\"\n",
    "\n",
    "    def __init__(self, event_name: str):\n",
    "        self.event_name = event_name\n",
    "        self.handlers: list[tuple[str, EventHandler[T]]] = []\n",
    "\n",
    "    def add_handler(self, event_name: str, handler: EventHandler[T]) -> None:\n",
    "        self.handlers.append((event_name, handler))\n",
    "\n",
    "    def emit(self, event_name: str, event: T) -> None:\n",
    "        for name, handler in self.handlers:\n",
    "            if name == event_name:\n",
    "                handler(event)\n",
    "\n",
    "\n",
    "class EventHandlers:\n",
    "    \"\"\"A simple event system to allow for custom callbacks.\"\"\"\n",
    "\n",
    "    phase_start: EventBinding[Event]\n",
    "    pre_step: EventBinding[Event]\n",
    "    action: EventBinding[Event]\n",
    "    phase_end: EventBinding[PhaseEndEvent]\n",
    "    step_metrics: EventBinding[StepMetricsEvent]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.phase_start = EventBinding[Event]('phase-start')\n",
    "        self.pre_step = EventBinding[Event]('pre-step')\n",
    "        self.action = EventBinding[Event]('action')\n",
    "        self.phase_end = EventBinding[PhaseEndEvent]('phase-end')\n",
    "        self.step_metrics = EventBinding[StepMetricsEvent]('step-metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from skimage import metrics\n",
    "import torch\n",
    "from typing import Iterable, Iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from mini.temporal.dopesheet import Dopesheet\n",
    "from mini.temporal.timeline import Timeline\n",
    "from utils.progress import RichProgress\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    log.info(f'Global random seed set to {seed}')\n",
    "\n",
    "\n",
    "def set_deterministic_mode(seed: int):\n",
    "    \"\"\"Make experiments reproducible.\"\"\"\n",
    "    seed_everything(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    log.info('PyTorch set to deterministic mode')\n",
    "\n",
    "\n",
    "def reiterate[T](it: Iterable[T]) -> Iterator[T]:\n",
    "    \"\"\"\n",
    "    Iterates over an iterable indefinitely.\n",
    "\n",
    "    When the iterable is exhausted, it starts over from the beginning. Unlike\n",
    "    `itertools.cycle`, yielded values are not cached — so each iteration may be\n",
    "    different.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        yield from it\n",
    "\n",
    "\n",
    "def train_color_model(  # noqa: C901\n",
    "    model: ColorMLP,\n",
    "    train_loader: DataLoader,\n",
    "    val_data: Tensor,\n",
    "    dopesheet: Dopesheet,\n",
    "    loss_criterion: LossCriterion,\n",
    "    regularizers: list[RegularizerConfig],\n",
    "    event_handlers: EventHandlers | None = None,\n",
    "):\n",
    "    if event_handlers is None:\n",
    "        event_handlers = EventHandlers()\n",
    "\n",
    "    # --- Validate inputs ---\n",
    "    if 'lr' not in dopesheet.props:\n",
    "        raise ValueError(\"Dopesheet must define the 'lr' property column.\")\n",
    "    # --- End Validation ---\n",
    "\n",
    "    timeline = Timeline(dopesheet)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    train_data = iter(reiterate(train_loader))\n",
    "\n",
    "    total_steps = len(timeline)\n",
    "\n",
    "    with RichProgress(total=total_steps, description='Training Steps') as pbar:\n",
    "        for step in range(total_steps):\n",
    "            # Get state *before* advancing timeline for this step's processing\n",
    "            current_state = timeline.state\n",
    "            current_phase_name = current_state.phase\n",
    "\n",
    "            batch_data, batch_labels = next(train_data)\n",
    "            # Should already be on device\n",
    "            # batch_data = batch_data.to(device)\n",
    "            # batch_labels = batch_labels.to(device)\n",
    "\n",
    "            # --- Event Handling ---\n",
    "            event_template = {\n",
    "                'step': step,\n",
    "                'model': model,\n",
    "                'timeline_state': current_state,\n",
    "                'optimizer': optimizer,\n",
    "            }\n",
    "\n",
    "            if current_state.is_phase_start:\n",
    "                event = Event(name=f'phase-start:{current_phase_name}', **event_template)\n",
    "                event_handlers.phase_start.emit(event.name, event)\n",
    "                event_handlers.phase_start.emit('phase-start', event)\n",
    "\n",
    "            for action in current_state.actions:\n",
    "                event = Event(name=f'action:{action}', **event_template)\n",
    "                event_handlers.action.emit(event.name, event)\n",
    "                event_handlers.action.emit('action', event)\n",
    "\n",
    "            event = Event(name='pre-step', **event_template)\n",
    "            event_handlers.pre_step.emit('pre-step', event)\n",
    "\n",
    "            # --- Training Step ---\n",
    "            # ... (get data, update LR, zero grad, forward pass, calculate loss, backward, step) ...\n",
    "\n",
    "            current_lr = current_state.props['lr']\n",
    "            # REF_BATCH_SIZE = 32\n",
    "            # lr_scale_factor = batch.shape[0] / REF_BATCH_SIZE\n",
    "            # current_lr = current_lr * lr_scale_factor\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs, latents = model(batch_data)\n",
    "            current_results = InferenceResult(outputs, latents)\n",
    "\n",
    "            primary_loss = loss_criterion(batch_data, current_results).mean()\n",
    "            losses = {'recon': primary_loss.item()}\n",
    "            total_loss = primary_loss\n",
    "            zeros = torch.tensor(0.0, device=batch_data.device)\n",
    "\n",
    "            for regularizer in regularizers:\n",
    "                name = regularizer.name\n",
    "                criterion = regularizer.criterion\n",
    "\n",
    "                weight = current_state.props.get(name, 1.0)\n",
    "                if weight == 0:\n",
    "                    continue\n",
    "\n",
    "                if regularizer.label_affinities is not None:\n",
    "                    # Soft labels that indicate how much effect this regularizer has, based on its affinity with the label\n",
    "                    label_probs = [\n",
    "                        batch_labels[k] * v\n",
    "                        for k, v in regularizer.label_affinities.items()\n",
    "                        if k in batch_labels  #\n",
    "                    ]\n",
    "                    if not label_probs:\n",
    "                        continue\n",
    "\n",
    "                    sample_affinities = torch.stack(label_probs, dim=0).sum(dim=0)\n",
    "                    sample_affinities = torch.clamp(sample_affinities, 0.0, 1.0)\n",
    "                    if torch.allclose(sample_affinities, zeros):\n",
    "                        continue\n",
    "                else:\n",
    "                    sample_affinities = torch.ones(batch_data.shape[0], device=batch_data.device)\n",
    "\n",
    "                per_sample_loss = criterion(batch_data, current_results)\n",
    "                if len(per_sample_loss.shape) == 0:\n",
    "                    # If the loss is a scalar, we need to expand it to match the batch size\n",
    "                    per_sample_loss = per_sample_loss.expand(batch_data.shape[0])\n",
    "                assert per_sample_loss.shape[0] == batch_data.shape[0], f'Loss should be per-sample OR scalar: {name}'\n",
    "\n",
    "                # Apply sample affinities\n",
    "                weighted_loss = per_sample_loss * sample_affinities\n",
    "\n",
    "                # Apply sample importance weights\n",
    "                # weighted_loss *= batch_weights\n",
    "\n",
    "                # Calculate mean only over selected samples. If we used torch.mean, it would average over all samples, including those with 0 weight\n",
    "                term_loss = weighted_loss.sum() / (sample_affinities.sum() + 1e-8)\n",
    "\n",
    "                losses[name] = term_loss.item()\n",
    "                if not torch.isfinite(term_loss):\n",
    "                    log.warning(f'Loss term {name} at step {step} is not finite: {term_loss}')\n",
    "                    continue\n",
    "                total_loss += term_loss * weight\n",
    "\n",
    "            if total_loss > 0:\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "            # --- End Training Step ---\n",
    "\n",
    "            # Emit step metrics event\n",
    "            step_metrics_event = StepMetricsEvent(\n",
    "                name='step-metrics',\n",
    "                **event_template,\n",
    "                train_batch=batch_data,\n",
    "                total_loss=total_loss.item(),\n",
    "                losses=losses,\n",
    "            )\n",
    "            event_handlers.step_metrics.emit('step-metrics', step_metrics_event)\n",
    "\n",
    "            # --- Post-Step Event Handling ---\n",
    "            if current_state.is_phase_end:\n",
    "                # Trigger phase-end for the *current* phase\n",
    "                # validation_data = batch_data\n",
    "                with torch.no_grad():\n",
    "                    val_outputs, val_latents = model(val_data.to(device))\n",
    "                event = PhaseEndEvent(\n",
    "                    name=f'phase-end:{current_phase_name}',\n",
    "                    **event_template,\n",
    "                    validation_data=val_data,\n",
    "                    inference_result=InferenceResult(val_outputs, val_latents),\n",
    "                )\n",
    "                event_handlers.phase_end.emit(event.name, event)\n",
    "                event_handlers.phase_end.emit('phase-end', event)\n",
    "            # --- End Event Handling ---\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(\n",
    "                metrics={\n",
    "                    'PHASE': current_phase_name,\n",
    "                    'lr': f'{current_lr:.6f}',\n",
    "                    'loss': f'{total_loss.item():.4f}',\n",
    "                    **{name: f'{lt:.4f}' for name, lt in losses.items()},\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # Advance timeline *after* processing the current step\n",
    "            if step < total_steps:  # Avoid stepping past the end\n",
    "                timeline.step()\n",
    "\n",
    "    log.info('Training finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "We define an event handler that periodically draws scatter plots of the latent embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from matplotlib.axes import Axes\n",
    "import numpy.typing as npt\n",
    "from torch import Tensor\n",
    "from IPython.display import HTML\n",
    "\n",
    "from utils.nb import save_fig\n",
    "\n",
    "\n",
    "def hide_decorations(ax: Axes, background: bool = True, ticks: bool = True, border: bool = True) -> None:\n",
    "    \"\"\"Remove all decorations from the axes.\"\"\"\n",
    "    if background:\n",
    "        ax.patch.set_alpha(0)\n",
    "    if ticks:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    if border:\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "\n",
    "\n",
    "def draw_latent_slice(\n",
    "    ax: Axes | tuple[Axes, Axes],  # Axes for background and foreground\n",
    "    latents: npt.NDArray,\n",
    "    colors: npt.NDArray,\n",
    "    dot_size: float = 200,\n",
    "    clip_on: bool = False,\n",
    "):\n",
    "    \"\"\"Draw a slice of the latent space.\"\"\"\n",
    "    assert latents.ndim == 2, 'Latents should be 2D'\n",
    "    assert latents.shape[1] == 2, 'Latents should have 2 dimensions'\n",
    "\n",
    "    bg, fg = ax if isinstance(ax, tuple) else (ax, ax)\n",
    "    return {\n",
    "        'circ': bg.add_patch(\n",
    "            Circle((0, 0), 1, fill=True, facecolor='#111', edgecolor='#0000', clip_on=clip_on, zorder=-1)\n",
    "        ),\n",
    "        'scatter': fg.scatter(latents[:, 0], latents[:, 1], c=colors, s=dot_size, alpha=0.7, clip_on=clip_on),\n",
    "    }\n",
    "\n",
    "\n",
    "def geometric_frame_progression(samples: int, n: int, offset: int = 10) -> npt.NDArray[np.int_]:\n",
    "    \"\"\"\n",
    "    Generate a geometric progression of indices for sampling frames.\n",
    "\n",
    "    Useful for creating videos of simulations in which there is more detail at the beginning.\n",
    "\n",
    "    Args:\n",
    "        samples (int): Number of samples to generate. If larger than n, it will be capped to n.\n",
    "        n (int): Total number of frames to sample from.\n",
    "        offset (int): Offset to apply during generation. Increase this to get more samples at the start of short sequences; larger values will result in \"flatter\" sequences.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of indices.\n",
    "    \"\"\"\n",
    "    samples = min(samples, n)\n",
    "    if samples <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    return np.unique(np.geomspace(offset, n + offset, samples, endpoint=False, dtype=int) - offset)\n",
    "\n",
    "\n",
    "def bezier_frame_progression(\n",
    "    samples: int,\n",
    "    n: int,\n",
    "    cp1: tuple[float, float],\n",
    "    cp2: tuple[float, float],\n",
    "    lookup_table_size: int = 200,\n",
    ") -> npt.NDArray[np.int_]:\n",
    "    \"\"\"\n",
    "    Generate a bezier curve for sampling frames.\n",
    "\n",
    "    Args:\n",
    "        samples: Number of samples to generate (inclusive upper bound).\n",
    "        n: Total number of frames to sample from.\n",
    "        cp1: Control point 1 (t_in, t_out), e.g. (0, 0) for a linear curve, or (0.42, 0) for a classic ease-in.\n",
    "        cp2: Control point 2 (t_in, t_out), e.g. (1, 1) for a linear curve, or (0.58, 1) for a classic ease-out.\n",
    "        lookup_table_size: Number of points to sample on the bezier curve. Higher values will result in smoother curves, but will take longer to compute.\n",
    "    \"\"\"\n",
    "    from matplotlib.bezier import BezierSegment\n",
    "\n",
    "    if samples <= 0 or n <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    if n == 1:\n",
    "        return np.array([0], dtype=int)\n",
    "    control_points = np.array([(0, 0), cp1, cp2, (1, 1)])\n",
    "    bezier_curve = BezierSegment(control_points)\n",
    "    t_lookup = np.linspace(0, 1, lookup_table_size)\n",
    "    points_on_curve = bezier_curve(t_lookup)\n",
    "    xs = points_on_curve[:, 0]\n",
    "    ys = points_on_curve[:, 1]\n",
    "    assert np.all(np.diff(xs) >= 0), 't is not monotonic'\n",
    "\n",
    "    # Generate linearly spaced input times for sampling our easing function\n",
    "    t_linear_input = np.linspace(0, 1, samples, endpoint=True)\n",
    "\n",
    "    # Interpolate to get eased time:\n",
    "    # For each t_linear_input (our desired \"progress through animation\"),\n",
    "    # find the corresponding y-value on the Bézier curve (our \"eased progress\").\n",
    "    t_eased = np.interp(t_linear_input, xs, ys)\n",
    "\n",
    "    # Clip eased time to [0,1] in case control points caused overshoot/undershoot\n",
    "    # and we want to strictly map to frame indices.\n",
    "    t_eased = np.clip(t_eased, 0.0, 1.0)\n",
    "\n",
    "    # Scale to frame indices and ensure they are unique and sorted\n",
    "    frame_indices = np.round(t_eased * (n - 1)).astype(int)\n",
    "    unique_indices = np.unique(frame_indices)\n",
    "\n",
    "    return unique_indices\n",
    "\n",
    "\n",
    "class PhasePlotter:\n",
    "    \"\"\"Event handler to plot latent space at the end of each phase.\"\"\"\n",
    "\n",
    "    def __init__(self, val_data: Tensor, *, dim_pairs: list[tuple[int, int]], interval: int = 100):\n",
    "        from utils.nb import displayer\n",
    "\n",
    "        # Store (phase_name, end_step, data, result) - data comes from event now\n",
    "        self.val_data = val_data\n",
    "        self.history: list[tuple[str, int, Tensor, Tensor]] = []\n",
    "        self.display = displayer()\n",
    "        self.dim_pairs = dim_pairs\n",
    "        self.interval = interval\n",
    "\n",
    "    def __call__(self, event: Event):\n",
    "        # TODO: Don't assume device = CPU\n",
    "        # TODO: Split this class so that the event handler is separate from the plotting, and so the plotting can happen locally with @run.hither\n",
    "        if event.step % self.interval != 0:\n",
    "            return\n",
    "        phase_name = event.timeline_state.phase\n",
    "        step = event.step\n",
    "        output, latents = event.model(self.val_data)\n",
    "\n",
    "        log.debug(f'Plotting end of phase: {phase_name} at step {step} using provided results.')\n",
    "\n",
    "        # Append to history\n",
    "        self.history.append((phase_name, step, output.detach().cpu(), latents.detach().cpu()))\n",
    "\n",
    "        # Plotting logic remains the same as it already expected CPU tensors\n",
    "        fig = self._plot_phase_history()\n",
    "        self.display(\n",
    "            HTML(\n",
    "                save_fig(\n",
    "                    fig,\n",
    "                    f'large-assets/ex-{nbid}-color-phase-history.png',\n",
    "                    alt_text='Visualizations of latent space at the end of each curriculum phase.',\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _plot_phase_history(self):\n",
    "        if not self.history:\n",
    "            fig, ax = plt.subplots()\n",
    "            fig.set_facecolor('#333')\n",
    "            ax.set_facecolor('#222')\n",
    "            ax.text(0.5, 0.5, 'Waiting...', ha='center', va='center')\n",
    "            return fig\n",
    "\n",
    "        plt.style.use('dark_background')\n",
    "\n",
    "        # Number of dimension pairs\n",
    "        num_dim_pairs = len(self.dim_pairs)\n",
    "\n",
    "        # Cap the number of thumbnails to a maximum for readability\n",
    "        max_thumbnails = 10\n",
    "        indices = geometric_frame_progression(max_thumbnails, len(self.history), offset=10)\n",
    "        history_to_show = [self.history[i] for i in indices]\n",
    "\n",
    "        # Create figure with gridspec for flexible layout\n",
    "        fig = plt.figure(figsize=(12, 5), facecolor='#333')\n",
    "\n",
    "        # Create two separate gridspecs - one for thumbnails, one for latest state\n",
    "        gs = fig.add_gridspec(2, 1, hspace=0.1, height_ratios=[4, 1])\n",
    "\n",
    "        # Thumbnail gridspec (bottom row) - only first dimension pair\n",
    "        # Remove spacing between thumbnails by setting wspace=0\n",
    "        thumbnail_gs = gs[1].subgridspec(2, max_thumbnails, wspace=0, hspace=0.1, height_ratios=[1, 0])\n",
    "\n",
    "        # Latest state gridspec (top row) - all dimension pairs\n",
    "        latest_gs = gs[0].subgridspec(2, num_dim_pairs, wspace=0, hspace=0.1, height_ratios=[0, 1])\n",
    "\n",
    "        # Get the data\n",
    "        _colors = self.val_data.numpy()\n",
    "\n",
    "        # Create thumbnail axes and plot history\n",
    "        for i, (_, step, _, latents) in enumerate(history_to_show):\n",
    "            _latents = latents.numpy()\n",
    "\n",
    "            # Only plot the first dimension pair for thumbnails\n",
    "            dim1, dim2 = self.dim_pairs[0]\n",
    "\n",
    "            # Create title for the thumbnail as its own axes, so that it's aligned with the other titles\n",
    "            axt = fig.add_subplot(thumbnail_gs[1, i])\n",
    "            axt.text(\n",
    "                0,\n",
    "                0,\n",
    "                f'{step}',\n",
    "                # transform=axt.transAxes,\n",
    "                horizontalalignment='center',\n",
    "                verticalalignment='top',\n",
    "                fontsize=7,\n",
    "            )\n",
    "            # Remove all decorations\n",
    "            hide_decorations(axt)\n",
    "\n",
    "            # Create thumbnail axis\n",
    "            ax = fig.add_subplot(thumbnail_gs[0, i])\n",
    "            ax.sharex(axt)\n",
    "            draw_latent_slice(ax, _latents[:, [dim1, dim2]], _colors, dot_size=20)\n",
    "            hide_decorations(ax)\n",
    "\n",
    "            # Ensure square aspect ratio\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_adjustable('box')\n",
    "\n",
    "        # Plot latest state\n",
    "        # Get the latest data\n",
    "        phase_name, step, output, latents = self.history[-1]\n",
    "        _latents = latents.numpy()\n",
    "\n",
    "        prev_ax = None\n",
    "        for i, (dim1, dim2) in enumerate(self.dim_pairs):\n",
    "            # Create title for the thumbnail as its own axes, so that it's aligned with the other titles\n",
    "            axt = fig.add_subplot(latest_gs[0, i])\n",
    "            axt.text(\n",
    "                0,\n",
    "                0,\n",
    "                f'[{dim1}, {dim2}]',\n",
    "                # transform=axt.transAxes,\n",
    "                horizontalalignment='center',\n",
    "                fontsize=10,\n",
    "            )\n",
    "            hide_decorations(axt)\n",
    "\n",
    "            # Plot\n",
    "            ax = fig.add_subplot(latest_gs[1, i])\n",
    "            ax.sharex(axt)\n",
    "            if prev_ax is not None:\n",
    "                ax.sharey(prev_ax)\n",
    "            prev_ax = ax\n",
    "            draw_latent_slice(ax, _latents[:, [dim1, dim2]], _colors)\n",
    "            hide_decorations(ax)\n",
    "\n",
    "            # Ensure square aspect ratio\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_adjustable('box')\n",
    "\n",
    "        # Add overall title\n",
    "        fig.suptitle(f'Latent space — step {step}', fontsize=12, color='white')\n",
    "\n",
    "        # Use subplots_adjust instead of tight_layout to avoid warnings\n",
    "        fig.subplots_adjust(top=0.9, bottom=0.1, left=0.1, right=0.95)\n",
    "\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter dopesheet\n",
    "\n",
    "As in previous experiments, we'll define [a dopesheet](./ex-1.7-dopesheet.csv) (timelines) to allow hyperparameters to vary over time. We tried to train the model with constant hyperparameters, but it was difficult to get the training to be stable.\n",
    "\n",
    "Unlike previous experiments, this one has only one phase, because the full training dataset is used throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.7-color-timeline.png?v=4K5by8m_djnPJxc9gQx_gJ6SyRFVVncqeGKFBQKekR0\" alt=\"Line chart showing the hyperparameter schedule over time.\" style=\"max-width: 70rem;\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "from mini.temporal.vis import plot_timeline, realize_timeline, ParamGroup\n",
    "from mini.temporal.dopesheet import Dopesheet\n",
    "from mini.temporal.timeline import Timeline\n",
    "from utils.nb import save_fig\n",
    "\n",
    "\n",
    "line_styles = [\n",
    "    (re.compile(r'^data-'), {'linewidth': 5, 'zorder': -1, 'alpha': 0.5}),\n",
    "    # (re.compile(r'-(anchor|norm)$'), {'linewidth': 2, 'linestyle': (0, (8, 1, 1, 1))}),\n",
    "]\n",
    "\n",
    "\n",
    "def load_dopesheet():\n",
    "    dopesheet = Dopesheet.from_csv(f'ex-{nbid}-dopesheet.csv')\n",
    "    # display(Markdown(f\"\"\"## Parameter schedule ({variant})\\n{dopesheet.to_markdown()}\"\"\"))\n",
    "\n",
    "    timeline = Timeline(dopesheet)\n",
    "    history_df = realize_timeline(timeline)\n",
    "    keyframes_df = dopesheet.as_df()\n",
    "\n",
    "    groups = (\n",
    "        ParamGroup(\n",
    "            name='',\n",
    "            params=[p for p in dopesheet.props if p not in {'lr'}],\n",
    "            height_ratio=2,\n",
    "        ),\n",
    "        ParamGroup(\n",
    "            name='',\n",
    "            params=[p for p in dopesheet.props if p in {'lr'}],\n",
    "            height_ratio=1,\n",
    "        ),\n",
    "    )\n",
    "    fig, ax = plot_timeline(history_df, keyframes_df, groups, line_styles=line_styles)\n",
    "    # Add assertion to satisfy type checker\n",
    "    assert isinstance(fig, Figure), 'plot_timeline should return a Figure'\n",
    "    display(\n",
    "        HTML(\n",
    "            save_fig(\n",
    "                fig,\n",
    "                f'large-assets/ex-{nbid}-color-timeline.png',\n",
    "                alt_text='Line chart showing the hyperparameter schedule over time.',\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return dopesheet\n",
    "\n",
    "\n",
    "dopesheet = load_dopesheet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions and regularizers\n",
    "\n",
    "Like Ex 1.6, we use mean squared error for the main reconstruction loss (`loss-recon`), and regularizers that encourage embeddings of unit length, and for primary colors to be on the plane of the first two dimensions. However this time, the regularizers can have different strengths depending on which sample they're evaluating. See _Labelling_ and _Train_ below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import linalg as LA\n",
    "\n",
    "from ex_color.data.color_cube import ColorCube\n",
    "from ex_color.data.cyclic import arange_cyclic\n",
    "\n",
    "\n",
    "def objective(fn):\n",
    "    \"\"\"Adapt loss function to look like a regularizer\"\"\"\n",
    "\n",
    "    def wrapper(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "        loss = fn(data, res.outputs)\n",
    "        # Reduce element-wise loss to per-sample loss by averaging over feature dimensions\n",
    "        if loss.ndim > 1:\n",
    "            # Calculate mean over all dimensions except the first (batch) dimension\n",
    "            reduce_dims = tuple(range(1, loss.ndim))\n",
    "            loss = torch.mean(loss, dim=reduce_dims)\n",
    "        return loss\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def unitarity(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "    \"\"\"Regularize latents to have unit norm (vectors of length 1)\"\"\"\n",
    "    norms = LA.vector_norm(res.latents, dim=-1)\n",
    "    # Return per-sample loss, shape [B]\n",
    "    return (norms - 1.0) ** 2\n",
    "\n",
    "\n",
    "def planarity(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "    \"\"\"Regularize latents to be planar in the first two channels (so zero in other channels)\"\"\"\n",
    "    if res.latents.shape[1] <= 2:\n",
    "        # No dimensions beyond the first two, return zero loss per sample\n",
    "        return torch.zeros(res.latents.shape[0], device=res.latents.device)\n",
    "    # Sum squares across the extra dimensions for each sample, shape [B]\n",
    "    return torch.sum(res.latents[:, 2:] ** 2, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pin (anchor)\n",
    "\n",
    "In Ex 1.5, we used concept anchoring for the primary and secondary colors from the end of the first phase. This time, we'll associate some colors with specific points in latent space right from the start.\n",
    "\n",
    "The `Pin` function penalizes points for being far from the chosen location. By itself it would act on all points equally. We tell it which points to apply to when we configure the regularizers (see _Train_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pin(LossCriterion):\n",
    "    def __init__(self, anchor_point: Tensor):\n",
    "        self.anchor_point = anchor_point\n",
    "\n",
    "    def __call__(self, data: Tensor, res: InferenceResult) -> Tensor:\n",
    "        \"\"\"\n",
    "        Regularize latents to be close to the anchor point.\n",
    "\n",
    "        Returns:\n",
    "            loss: Per-sample loss, shape [B].\n",
    "        \"\"\"\n",
    "        # Calculate squared distances to the anchor\n",
    "        sq_dists = torch.sum((res.latents - self.anchor_point) ** 2, dim=-1)  # [B]\n",
    "        return sq_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate\n",
    "\n",
    "Another regularization term encourages the embeddings to have unit length, but by default, that causes them to bunch up. The `Separate` function counters that. A similar function was used in Ex 1.5 and 1.6, but it applied a linear (Euclidean) repulsion term — which fought against the unit length normalization. Instead, this one uses cosine similarity to create a repulsive force along the surface of the hypersphere. Since that's orthogonal to the normalization term, they no longer fight against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import EllipsisType\n",
    "\n",
    "\n",
    "class Separate(LossCriterion):\n",
    "    \"\"\"Regularize latents to be rotationally separated from each other.\"\"\"\n",
    "\n",
    "    def __init__(self, channels: tuple[int, ...] | EllipsisType = ..., power: float = 1.0, shift: bool = True):\n",
    "        self.channels = channels\n",
    "        self.power = power\n",
    "        self.shift = shift\n",
    "\n",
    "    def __call__(self, data: Tensor, res: InferenceResult) -> Tensor:\n",
    "        embeddings = res.latents[:, self.channels]  # [B, C]\n",
    "\n",
    "        # Normalize to unit hypersphere, so it's only the angular distance that matters\n",
    "        embeddings = embeddings / (torch.norm(embeddings, dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "        # Find the angular distance as cosine similarity\n",
    "        cos_sim = torch.matmul(embeddings, embeddings.T)  # [B, B]\n",
    "\n",
    "        # Nullify self-repulsion.\n",
    "        # We can't use torch.eye, because some points in the batch may be duplicates due to the use of random sampling with replacement.\n",
    "        cos_sim[torch.isclose(cos_sim, torch.ones_like(cos_sim))] = 0.0\n",
    "        if self.shift:\n",
    "            # Shift the cosine similarity to be in the range [0, 1]\n",
    "            cos_sim = (cos_sim + 1.0) / 2.0\n",
    "\n",
    "        # Sum over all other points\n",
    "        return torch.sum(cos_sim**self.power, dim=-1)  # [B]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading, sampling, and event handling\n",
    "\n",
    "Here we set up:\n",
    "\n",
    "- **Datasets:** Define the datasets used (primary/secondary colors, full color grid).\n",
    "- **Sampler:** Use `DynamicWeightedRandomBatchSampler` for the full dataset. Its weights are updated by the `update_sampler_weights` callback, which responds to the `data-fraction` parameter from the dopesheet. This smoothly shifts the sampling focus from highly vibrant colors early on to the full range of colors later.\n",
    "- **Recorders:** `ModelRecorder` and `MetricsRecorder` are event handlers that save the model state and loss values at each step.\n",
    "- **Event bindings:** Connect event handlers to specific events (e.g., `plotter` to `phase-end`, `reg_anchor.on_anchor` to `action:anchor`, recorders to `pre-step` and `step-metrics`).\n",
    "- **Training execution:** Finally, call `train_color_model` with the model, datasets, dopesheet, loss criteria, and configured event handlers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ModelRecorder(EventHandler):\n",
    "    \"\"\"Event handler to record model parameters.\"\"\"\n",
    "\n",
    "    history: list[tuple[int, dict[str, Tensor]]]\n",
    "    \"\"\"A list of tuples (step, state_dict) where state_dict is a copy of the model's state dict.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def __call__(self, event: Event):\n",
    "        # Get a *copy* of the state dict and move it to the CPU\n",
    "        # so we don't hold onto GPU memory or track gradients unnecessarily.\n",
    "        model_state = {k: v.cpu().clone() for k, v in event.model.state_dict().items()}\n",
    "        self.history.append((event.step, model_state))\n",
    "        log.debug(f'Recorded model state at step {event.step}')\n",
    "\n",
    "\n",
    "class BatchRecorder(EventHandler):\n",
    "    \"\"\"Event handler to record the exact batches used at each step.\"\"\"\n",
    "\n",
    "    history: list[tuple[int, Tensor]]\n",
    "    \"\"\"A list of tuples (step, train_batch).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def __call__(self, event: StepMetricsEvent):\n",
    "        if not isinstance(event, StepMetricsEvent):\n",
    "            log.warning(f'BatchRecorder received unexpected event type: {type(event)}')\n",
    "            return\n",
    "        self.history.append((event.step, event.train_batch.cpu().clone()))\n",
    "        log.debug(f'Recorded batch at step {event.step}')\n",
    "\n",
    "\n",
    "class MetricsRecorder(EventHandler):\n",
    "    \"\"\"Event handler to record training metrics.\"\"\"\n",
    "\n",
    "    history: list[tuple[int, float, dict[str, float]]]\n",
    "    \"\"\"A list of tuples (step, total_loss, losses_dict).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def __call__(self, event: StepMetricsEvent):\n",
    "        if not isinstance(event, StepMetricsEvent):\n",
    "            log.warning(f'MetricsRecorder received unexpected event type: {type(event)}')\n",
    "            return\n",
    "\n",
    "        self.history.append((event.step, event.total_loss, event.losses.copy()))\n",
    "        log.debug(f'Recorded metrics at step {event.step}: loss={event.total_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling\n",
    "\n",
    "The training dataset has a fixed and somewhat small size. We want to simulate noisy labels — e.g. RGB 1,0,0 should not always be labelled \"red\", and colors close to red should also sometimes attract that label.\n",
    "\n",
    "Here we define a collation function for use with DataLoader. It takes a batch and produces labels for the samples on the fly, which means the model may see identical samples with different labels during training.\n",
    "\n",
    "Labels are assigned based on proximity to certain colors. The raw distance is not used; instead it is raised to a power to sharpen the association, and weaken the label for colors that are futher away. Initially the labels are smooth $(0..1)$. They are then converted to binary $\\lbrace 0,1 \\rbrace$ by comparison to random numbers.\n",
    "\n",
    "The label \"red\" (i.e. proximity to pure red) is calculated as:\n",
    "\n",
    "$$\\text{red} = \\left(r - \\frac{rg}{2} - \\frac{rb}{2}\\right) ^{10}$$\n",
    "\n",
    "While \"vibrant\" (proximity to any pure hue) is:\n",
    "\n",
    "$$\\text{vibrant} = \\left(s \\times v\\right)^{100}$$\n",
    "\n",
    "Where $r$, $g$, and $b$ are the red, green, and blue channels, and $s$ and $v$ are the saturation and value — all of which are real numbers between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "# TODO: remove forced reload\n",
    "if True:\n",
    "    import importlib\n",
    "    import ex_color.data.cube_sampler\n",
    "\n",
    "    importlib.reload(ex_color.data.cube_sampler)\n",
    "\n",
    "\n",
    "def generate_color_labels(data: Tensor, vibrancies: Tensor) -> dict[str, Tensor]:\n",
    "    \"\"\"\n",
    "    Generate label probabilities based on RGB values.\n",
    "\n",
    "    Args:\n",
    "        data: Batch of RGB values [B, 3]\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping label names to probabilities str -> [B]\n",
    "    \"\"\"\n",
    "    labels: dict[str, Tensor] = {}\n",
    "\n",
    "    # Labels are assigned based on proximity to certain colors.\n",
    "    # Distance is raised to a power to sharpen the association (i.e. weaken the label for colors that are futher away).\n",
    "\n",
    "    # Proximity to primary colors\n",
    "    r, g, b = data[:, 0], data[:, 1], data[:, 2]\n",
    "    labels['red'] = (r * (1 - g / 2 - b / 2)) ** 10\n",
    "    # labels['green'] = g * (1 - r / 2 - b / 2)\n",
    "    # labels['blue'] = b * (1 - r / 2 - g / 2)\n",
    "\n",
    "    # Proximity to any fully-saturated, fully-bright color\n",
    "    labels['vibrant'] = vibrancies**100\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def collate_with_generated_labels(\n",
    "    batch,\n",
    "    *,\n",
    "    soft: bool = True,\n",
    "    scale: dict[str, float] | None = None,\n",
    ") -> tuple[Tensor, dict[str, Tensor]]:\n",
    "    \"\"\"\n",
    "    Custom collate function that generates labels for the samples.\n",
    "\n",
    "    Args:\n",
    "        batch: A list of ((data_tensor,), index_tensor) tuples from TensorDataset.\n",
    "               Note: TensorDataset wraps single tensors in a tuple.\n",
    "        soft: If True, return soft labels (0..1). Otherwise, return hard labels (0 or 1).\n",
    "        scale: Linear scaling factors for the labels (applied before discretizing).\n",
    "\n",
    "    Returns:\n",
    "        A tuple: (collated_data_tensor, collated_labels_tensor)\n",
    "    \"\"\"\n",
    "    # Separate data and indices\n",
    "    # TensorDataset yields tuples like ((data_point_tensor,), index_scalar_tensor)\n",
    "    data_tuple_list = [item[0] for item in batch]  # List of (data_tensor,) tuples\n",
    "    vibrancies = [item[1] for item in batch]\n",
    "\n",
    "    # Collate the data points using the default collate function\n",
    "    # default_collate handles the list of (data_tensor,) tuples correctly\n",
    "    collated_data = default_collate(data_tuple_list)\n",
    "    vibrancies = default_collate(vibrancies)\n",
    "    label_probs = generate_color_labels(collated_data, vibrancies)\n",
    "    for k, v in (scale or {}).items():\n",
    "        label_probs[k] = label_probs[k] * v\n",
    "\n",
    "    if soft:\n",
    "        # Return the probabilities directly\n",
    "        return collated_data, label_probs\n",
    "    else:\n",
    "        # Sample labels stochastically\n",
    "        labels = {k: discretize(v) for k, v in label_probs.items()}\n",
    "        return collated_data, labels\n",
    "\n",
    "\n",
    "def discretize(probs: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Discretize probabilities into binary labels.\n",
    "\n",
    "    Args:\n",
    "        probs: Tensor of probabilities [B]\n",
    "\n",
    "    Returns:\n",
    "        Tensor of binary labels [B]\n",
    "    \"\"\"\n",
    "    # Sample from a uniform distribution\n",
    "    rand = torch.rand_like(probs)\n",
    "    return (rand < probs).float()  # Convert to float for compatibility with loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "Like Ex 1.5, we'll train on the HSV cube (with RGB values) and validate with the RGB cube. Unlike Ex 1.5, we don't define a special dataset for the primary and secondary colors. And unlike Ex 1.6, we don't start with a subset of the training data: we train on the whole HSV cube, right from the start. We still use a sampling bias to prevent over-sampling of dark and desaturated colors, but it's held constant throughout training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from ex_color.data.cube_sampler import vibrancy\n",
    "\n",
    "hsv_cube = ColorCube.from_hsv(\n",
    "    h=arange_cyclic(step_size=10 / 360),\n",
    "    s=np.linspace(0, 1, 10),\n",
    "    v=np.linspace(0, 1, 10),\n",
    ")\n",
    "hsv_tensor = torch.tensor(hsv_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "vibrancy_tensor = torch.tensor(vibrancy(hsv_cube).flatten(), dtype=torch.float32)\n",
    "hsv_dataset = TensorDataset(hsv_tensor, vibrancy_tensor)\n",
    "\n",
    "labeller = partial(\n",
    "    collate_with_generated_labels,\n",
    "    soft=False,  # Use binary labels (stochastic) to simulate the labelling of internet text\n",
    "    scale={'red': 0.5, 'vibrant': 0.5},\n",
    ")\n",
    "# Desaturated and dark colors are over-represented in the cube, so we use a weighted sampler to balance them out\n",
    "hsv_loader = DataLoader(\n",
    "    hsv_dataset,\n",
    "    batch_size=64,\n",
    "    sampler=WeightedRandomSampler(\n",
    "        weights=hsv_cube.bias.flatten().tolist(),\n",
    "        num_samples=len(hsv_dataset),\n",
    "        replacement=True,\n",
    "    ),\n",
    "    collate_fn=labeller,\n",
    ")\n",
    "\n",
    "rgb_cube = ColorCube.from_rgb(\n",
    "    r=np.linspace(0, 1, 8),\n",
    "    g=np.linspace(0, 1, 8),\n",
    "    b=np.linspace(0, 1, 8),\n",
    ")\n",
    "rgb_tensor = torch.tensor(rgb_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "#### Regularizer configuration\n",
    "\n",
    "The training process employs several regularizers, each designed to impose specific structural properties on the latent space. The strength of these regularizers can be modulated by hyperparameters defined in the dopesheet, and their application to individual samples can be influenced by per-sample labels.\n",
    "\n",
    "Here's a breakdown of each configured regularizer:\n",
    "\n",
    "*   **`reg-polar`**: Concept anchor for the color _red_.\n",
    "    *   **Criterion**: `Pin` to the latent coordinate `(1, 0, 0, 0)`. This encourages specific embeddings to move towards this \"polar\" anchor point.\n",
    "    *   **Label affinity**: `{'red': 1.0}`. This regularizer is fully active (strength 1.0) for samples that are labeled as 'red'. Its goal is to anchor the concept of \"red\" to a specific location in the latent space.\n",
    "\n",
    "*   **`reg-separate`**: Encourages the full space to be used.\n",
    "    *   **Criterion**: `Separate` (with `power=10.0`, `shift=False`). This term encourages all embeddings in a batch to be rotationally distinct from each other. It calculates the cosine similarity between embeddings and applies a repulsive force, powered up to make the effect greater for pairs of points that are very close. The `shift=False` means it uses the raw cosine similarity (ranging from -1 to 1).\n",
    "    *   **Label affinity**: `None`. This regularizer applies equally to all samples in the batch, irrespective of their labels, aiming for a general separation of all learned representations.\n",
    "\n",
    "*   **`reg-planar`**: Defines a \"hue\" plane, onto which the color wheel should emerge.\n",
    "    *   **Criterion**: `planarity`. This penalizes embeddings for having non-zero values in latent dimensions beyond the first two (i.e., dimensions 2 and 3, given a 4D latent space). It encourages these embeddings to lie on the plane defined by the first two latent dimensions.\n",
    "    *   **Label affinity**: `{'vibrant': 1.0}`. This regularizer is fully active for samples labeled as 'vibrant'. The idea is to map vibrant, pure hues primarily onto a 2D manifold within the latent space, potentially representing a color wheel.\n",
    "\n",
    "*   **`reg-norm-v`**: Encourages a circular color wheel, rather than hexagonal.\n",
    "    *   **Criterion**: `unitarity`. This encourages the latent embeddings to have a norm (length) of 1, pushing them towards the surface of a hypersphere.\n",
    "    *   **Label affinity**: `{'vibrant': 1.0}`. This normalization is specifically applied with full strength to samples labeled as 'vibrant'. This works in conjunction with `reg-planar` to organize vibrant colors on a 2D spherical surface.\n",
    "\n",
    "*   **`reg-norm`**: Encourages embeddings to lie on the surface of a hypersphere, so they can be compared with cosine distance alone.\n",
    "    *   **Criterion**: `unitarity`. This also encourages latent embeddings to have a unit norm.\n",
    "    *   **Label affinity**: `None`. This regularizer applies the unit norm constraint to *all* samples in the batch, regardless of their labels. This ensures that even non-vibrant colors are normalized, contributing to a more uniform distribution of embeddings on the hypersphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dopesheet: Dopesheet):\n",
    "    \"\"\"Train the model with the given dopesheet and variant.\"\"\"\n",
    "    log.info('Training')\n",
    "    recorder = ModelRecorder()\n",
    "    metrics_recorder = MetricsRecorder()\n",
    "    batch_recorder = BatchRecorder()\n",
    "\n",
    "    # seed = 0\n",
    "    # set_deterministic_mode(seed)\n",
    "\n",
    "    model = ColorMLP()\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    log.info(f'Model initialized with {total_params:,} trainable parameters.')\n",
    "\n",
    "    event_handlers = EventHandlers()\n",
    "    event_handlers.pre_step.add_handler('pre-step', recorder)\n",
    "    event_handlers.step_metrics.add_handler('step-metrics', metrics_recorder)\n",
    "    event_handlers.step_metrics.add_handler('step-metrics', batch_recorder)\n",
    "\n",
    "    plotter = PhasePlotter(rgb_tensor, dim_pairs=[(1, 0), (1, 2), (1, 3)], interval=200)\n",
    "    event_handlers.pre_step.add_handler('pre-step', plotter)\n",
    "\n",
    "    regularizers = [\n",
    "        RegularizerConfig(\n",
    "            name='reg-polar',\n",
    "            criterion=Pin(torch.tensor([1, 0, 0, 0], dtype=torch.float32, device=hsv_tensor.device)),\n",
    "            label_affinities={'red': 1.0},\n",
    "        ),\n",
    "        RegularizerConfig(\n",
    "            name='reg-separate',\n",
    "            criterion=Separate(power=10.0, shift=False),\n",
    "            label_affinities=None,\n",
    "        ),\n",
    "        RegularizerConfig(\n",
    "            name='reg-planar',\n",
    "            criterion=planarity,\n",
    "            label_affinities={'vibrant': 1.0},\n",
    "        ),\n",
    "        RegularizerConfig(\n",
    "            name='reg-norm-v',\n",
    "            criterion=unitarity,\n",
    "            label_affinities={'vibrant': 1.0},\n",
    "        ),\n",
    "        RegularizerConfig(\n",
    "            name='reg-norm',\n",
    "            criterion=unitarity,\n",
    "            label_affinities=None,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    train_color_model(\n",
    "        model,\n",
    "        hsv_loader,\n",
    "        rgb_tensor,\n",
    "        dopesheet,\n",
    "        # loss_criterion=objective(nn.MSELoss(reduction='none')),  # No reduction; allows per-sample loss weights\n",
    "        loss_criterion=objective(nn.MSELoss()),\n",
    "        regularizers=regularizers,\n",
    "        event_handlers=event_handlers,\n",
    "    )\n",
    "\n",
    "    return recorder, metrics_recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 3.1 no.1.7:  Training\n",
      "I 3.1 no.1.7:  Model initialized with 263 trainable parameters.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 100%; padding: 5px 0; font-family: monospace;\">\n",
       "            <!-- Progress bar container -->\n",
       "            <div style=\"position: relative; height: calc(1em * 5/3); width: 100%;\">\n",
       "                <!-- Triangle indicator -->\n",
       "                <div style=\"position: absolute; bottom: -4px; left: calc(100.0% - 4px);\">\n",
       "                    <div style=\"\n",
       "                        width: 0;\n",
       "                        height: 0;\n",
       "                        border-left: 4px solid transparent;\n",
       "                        border-right: 4px solid transparent;\n",
       "                        border-bottom: 4px solid currentColor;\n",
       "                    \"></div>\n",
       "                </div>\n",
       "                <!-- Progress bar -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    height: 100%;\n",
       "                    width: 100.0%;\n",
       "                    background-color: color(from currentColor srgb r g b / 0.1);\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                \"></div>\n",
       "                <!-- Text overlay -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    width: 100%;\n",
       "                    height: 100%;\n",
       "                    text-align: center;\n",
       "                    line-height: calc(1em * 5/3);\n",
       "                    font-size: 0.9em;\n",
       "                    white-space: nowrap;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5);\n",
       "                \">\n",
       "                    <b>Training Steps</b>: 100.0% [<b>10001</b>/10001] [<b>00:54</b>/<00:00, 182.18 it/s]\n",
       "                </div>\n",
       "            </div>\n",
       "        \n",
       "            <div style=\"\n",
       "                display: grid;\n",
       "                grid-template-columns: repeat(8, minmax(80px, 1fr));\n",
       "                gap: 5px 0px;\n",
       "                width: 100%;\n",
       "                margin-top: 10px;\n",
       "                font-size: 0.85em;\n",
       "            \"><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">PHASE</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">lr</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">loss</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">recon</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">reg-separate</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">reg-planar</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">reg-norm-v</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">reg-norm</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">Train</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.005000</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0034</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0003</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">2.7890</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0099</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0002</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0006</div></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.7-color-phase-history.png?v=e5jyjJRajklJ2A22wzus3qaz8ubqPRSALaxR_aUfLt8\" alt=\"Visualizations of latent space at the end of each curriculum phase.\" style=\"max-width: 70rem;\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 58.7 no.1.7: Training finished!\n"
     ]
    }
   ],
   "source": [
    "recorder, metrics = train(dopesheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trained well! It's interesting that it starts out with quite a regular shape, and then becomes contorted before settling down to a smooth sphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space evolution analysis\n",
    "\n",
    "Let's visualize how the latent spaces evolved over time. Like Ex 1.5 and 1.6, we'll use the `ModelRecorder`'s history to load the model state at each recorded step and evaluate the latent positions for a fixed set of input colors (the full RGB grid). This gives us a sequence of latent space snapshots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 100%; padding: 5px 0; font-family: monospace;\">\n",
       "            <!-- Progress bar container -->\n",
       "            <div style=\"position: relative; height: calc(1em * 5/3); width: 100%;\">\n",
       "                <!-- Triangle indicator -->\n",
       "                <div style=\"position: absolute; bottom: -4px; left: calc(0.0% - 4px);\">\n",
       "                    <div style=\"\n",
       "                        width: 0;\n",
       "                        height: 0;\n",
       "                        border-left: 4px solid transparent;\n",
       "                        border-right: 4px solid transparent;\n",
       "                        border-bottom: 4px solid currentColor;\n",
       "                    \"></div>\n",
       "                </div>\n",
       "                <!-- Progress bar -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    height: 100%;\n",
       "                    width: 0.0%;\n",
       "                    background-color: color(from currentColor srgb r g b / 0.1);\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                \"></div>\n",
       "                <!-- Text overlay -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    width: 100%;\n",
       "                    height: 100%;\n",
       "                    text-align: center;\n",
       "                    line-height: calc(1em * 5/3);\n",
       "                    font-size: 0.9em;\n",
       "                    white-space: nowrap;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5);\n",
       "                \">\n",
       "                    <b>Evaluating latents</b>: 0.0% [<b>0</b>/10001] [<b>00:00</b>/<00:00, 0.00 it/s]\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def eval_latent_history(\n",
    "    recorder: ModelRecorder,\n",
    "    rgb_tensor: Tensor,\n",
    "):\n",
    "    \"\"\"Evaluate the latent space for each step in the recorder's history.\"\"\"\n",
    "    # Create a new model instance\n",
    "    from utils.progress import RichProgress\n",
    "\n",
    "    model = ColorMLP()\n",
    "\n",
    "    latent_history: list[tuple[int, np.ndarray]] = []\n",
    "    # Iterate over the recorded history\n",
    "    for step, state_dict in RichProgress(recorder.history, description='Evaluating latents'):\n",
    "        # Load the model state dict\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get the latents for the RGB tensor\n",
    "            _, latents = model(rgb_tensor.to(next(model.parameters()).device))\n",
    "            latents = latents.cpu().numpy()\n",
    "            latent_history.append((step, latents))\n",
    "    return latent_history\n",
    "\n",
    "\n",
    "latent_history = eval_latent_history(recorder, rgb_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animation of latent space\n",
    "\n",
    "This final visualization combines multiple views into a single animation:\n",
    "\n",
    "- **Latent space:** Shows 2D projections of the latent embeddings for the full RGB color grid, colored by their true RGB values. Note that these projections were _not_ searched for: we knew where to look because we intentionally structured the latent space.\n",
    "- **Hyperparameters:** Plots the parameter schedule from the dopesheet, with a vertical line indicating the current step in the animation.\n",
    "- **Training metrics:** Plots the total loss and the contribution of each individual loss/regularization term (on a log scale), again with a vertical line for the current step.\n",
    "\n",
    "A variable stride is used for sampling frames to focus on periods of rapid change.\n",
    "\n",
    "Improvements over Ex 1.5 and 1.6:\n",
    "- Better layout: embedding scatter plots are much more prominent, and show multiple views\n",
    "- Motion blur: \"skipped\" frames are all rendered on top of each other to accurately show the motion of the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy._typing._array_like import NDArray\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import imageio_ffmpeg\n",
    "from matplotlib import rcParams\n",
    "import pandas as pd\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.collections import PathCollection\n",
    "from matplotlib.text import Text\n",
    "from matplotlib.typing import ColorType\n",
    "\n",
    "from mini.temporal.dopesheet import RESERVED_COLS\n",
    "from utils.progress import RichProgress\n",
    "\n",
    "if True:\n",
    "    import importlib\n",
    "    import mini.temporal.vis\n",
    "\n",
    "    importlib.reload(mini.temporal.vis)\n",
    "from mini.temporal.vis import group_properties_by_scale, plot_timeline\n",
    "\n",
    "rcParams['animation.ffmpeg_path'] = imageio_ffmpeg.get_ffmpeg_exe()\n",
    "\n",
    "\n",
    "def animate_latent_evolution_with_metrics(\n",
    "    # Data\n",
    "    sampled_indices: NDArray[np.int_],\n",
    "    latent_history: list[tuple[int, np.ndarray]],\n",
    "    metrics_history: list[tuple[int, float, dict[str, float]]],\n",
    "    param_history_df: pd.DataFrame,\n",
    "    param_keyframes_df: pd.DataFrame,\n",
    "    # Settings\n",
    "    colors: np.ndarray,\n",
    "    dimensions: list[tuple[str, tuple[int, int]]],  # [D, 2]\n",
    "    interval=1 / 30,  # FPS\n",
    "    loss_smooth_window: int = 100,\n",
    "    alpha=0.7,\n",
    "):\n",
    "    \"\"\"Create an animation of latent space evolution, hyperparameters, and metrics.\"\"\"\n",
    "    plt.style.use('dark_background')\n",
    "    # Aim for 16:9 aspect ratio, give latent plots more height\n",
    "    fig = plt.figure(figsize=(19.20, 10.80))\n",
    "    fig.patch.set_facecolor('#333')\n",
    "\n",
    "    fig.subplots_adjust(\n",
    "        left=0.05,  # Smaller left margin\n",
    "        right=0.95,  # Smaller right margin\n",
    "        bottom=0.08,  # Smaller bottom margin (leave room for x-label)\n",
    "        top=0.93,  # Smaller top margin (leave room for titles)\n",
    "    )\n",
    "\n",
    "    # Top row: Latent space evolution [D]\n",
    "    # Bottom row: Hyperparameters and metrics [2]\n",
    "    gs = GridSpec(2, 1, height_ratios=[5, 1], hspace=0)\n",
    "    latent_gs = gs[0].subgridspec(2, len(dimensions), wspace=0, hspace=0, height_ratios=[0, 1])\n",
    "    metrics_gs = gs[1].subgridspec(1, 2, wspace=0.1)\n",
    "\n",
    "    # --- Set up latent space axes, and draw once ---\n",
    "\n",
    "    axs_latent_title: list[Axes] = [fig.add_subplot(latent_gs[0, i], frameon=False) for i, _ in enumerate(dimensions)]\n",
    "    add_projection_titles(axs_latent_title, dimensions)\n",
    "\n",
    "    # With clipping off, the backgrounds will overlap. So, add TWO axes for each dimension pair: one for the background and one for the foreground. Create them up-front to set the stacking order.\n",
    "    axs_latent_bg: list[Axes] = [\n",
    "        fig.add_subplot(latent_gs[1, i], frameon=False)\n",
    "        for i, _ in enumerate(dimensions)  # Stacking doesn't matter for the background\n",
    "    ]\n",
    "    axs_latent_fg: list[Axes] = list(reversed([\n",
    "        fig.add_subplot(latent_gs[1, i], frameon=False)\n",
    "        for i, _ in reversed(list(enumerate(dimensions)))  # Stack in reverse, so the first one is on top\n",
    "    ]))  # fmt: skip\n",
    "    scatters = add_projection_plots(axs_latent_bg, axs_latent_fg, dimensions, colors)\n",
    "\n",
    "    # Parameter plot\n",
    "    ax_params = fig.add_subplot(metrics_gs[0])\n",
    "    param_vline, params_legend = add_parameter_plot(ax_params, param_history_df, param_keyframes_df)\n",
    "\n",
    "    # Metrics plot\n",
    "    ax_metrics = fig.add_subplot(metrics_gs[1])\n",
    "    line_colors = {text.get_text(): text.get_color() for text in params_legend}\n",
    "    metrics_vline, _ = add_metrics_plot(ax_metrics, metrics_history, 0, loss_smooth_window, line_colors)\n",
    "\n",
    "    # --- Set common X limits ---\n",
    "    # Only set xlim for the timeline plots (params and metrics)\n",
    "    max_step = param_history_df['STEP'].max()\n",
    "    ax_params.set_xlim(left=0, right=max_step)\n",
    "    ax_metrics.set_xlim(left=0, right=max_step)\n",
    "\n",
    "    def update(frame: int):\n",
    "        i1 = sampled_indices[frame]\n",
    "        i2 = sampled_indices[frame + 1] if frame + 1 < len(sampled_indices) else i1 + 1\n",
    "        subframes = latent_history[i1:i2]\n",
    "        if not subframes:\n",
    "            return ()\n",
    "        step, _ = subframes[-1]\n",
    "        # log.info(f'Step {step} ({i1} -> {i2})')\n",
    "\n",
    "        # --- MOTION BLUR ---\n",
    "        # Gather the latents for this frame across all included steps.\n",
    "        # We want to draw the points in the order [B,S], i.e. the _step_ should vary first, so that subsequent points are drawn on top of previous ones.\n",
    "        # If the order was [S,B], then all points from later steps would obscure earlier ones.\n",
    "        _latents = [h[1] for h in subframes]  # [S][B, D]\n",
    "        _colors = [colors] * len(subframes)  # [S][B, C]\n",
    "        _latents = np.stack(_latents, axis=1)  # [B, S, D]\n",
    "        _colors = np.stack(_colors, axis=1)  # [B, S, C]\n",
    "        _latents = np.reshape(_latents, (-1, _latents.shape[-1]))  # [B*S, D]\n",
    "        _colors = np.reshape(_colors, (-1, _colors.shape[-1]))  # [B*S, C]\n",
    "        _alpha = distribute_alpha(alpha, len(subframes))\n",
    "\n",
    "        for scatter, (_, dim_pair) in zip(scatters, dimensions, strict=True):\n",
    "            scatter.set_offsets(_latents[:, dim_pair])\n",
    "            scatter.set_color(_colors[:, :3])  # type: ignore\n",
    "            scatter.set_alpha(_alpha)\n",
    "\n",
    "        speed = subframes[-1][0] - subframes[0][0] + 1\n",
    "        speed_text = f'▸▸{speed}x' if speed > 1 else f' ▸{speed}x'\n",
    "\n",
    "        fig.suptitle(f'Latent space @ {step} {speed_text}')\n",
    "        param_vline.set_xdata([step])\n",
    "        metrics_vline.set_xdata([step])\n",
    "\n",
    "        # Signal that these artists have changed\n",
    "        return (param_vline, metrics_vline, *scatters)\n",
    "\n",
    "    # Use the length of the (potentially strided) latent_history for frames\n",
    "    num_frames = len(sampled_indices)\n",
    "    ani = animation.FuncAnimation(fig, update, frames=num_frames, interval=interval * 1000, blit=True)\n",
    "    return fig, ani\n",
    "\n",
    "\n",
    "def unobtrusive_legend(ax: Axes):\n",
    "    \"\"\"Create a legend that doesn't block the plot.\"\"\"\n",
    "    lines = [l for l in ax.get_lines() if l.get_label() and not str(l.get_label()).startswith('_')]\n",
    "    labels = [str(line.get_label()) for line in lines]\n",
    "    colors = [line.get_color() for line in lines]\n",
    "    xs = np.linspace(0, 1, len(labels), endpoint=False)\n",
    "    xs += 0.5 / len(labels)\n",
    "\n",
    "    artists: list[Text] = []\n",
    "    for xpos, label, color in zip(xs, labels, colors, strict=True):\n",
    "        artist = ax.text(\n",
    "            xpos,\n",
    "            0.98,\n",
    "            label,\n",
    "            transform=ax.transAxes,\n",
    "            horizontalalignment='center',\n",
    "            verticalalignment='top',\n",
    "            fontsize='small',\n",
    "            color=color,\n",
    "        )\n",
    "        artists.append(artist)\n",
    "    return artists\n",
    "\n",
    "\n",
    "def add_projection_titles(axs_latent_title: list[Axes], dimensions: list[tuple[str, tuple[int, int]]]):\n",
    "    # Titles get their own axes, so they can be aligned with the other titles\n",
    "    for axt, (label, dim_pair) in zip(axs_latent_title, dimensions, strict=True):\n",
    "        ax_title = f'[{dim_pair[0]},{dim_pair[1]}]'\n",
    "        axt.text(\n",
    "            0.5,\n",
    "            0,\n",
    "            f'{ax_title} ({label})' if label else ax_title,\n",
    "            horizontalalignment='center',\n",
    "            verticalalignment='top',\n",
    "            fontsize=12,\n",
    "            color='white',\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        hide_decorations(axt)\n",
    "\n",
    "\n",
    "def add_projection_plots(\n",
    "    axs_latent_bg: list[Axes],\n",
    "    axs_latent_fg: list[Axes],\n",
    "    dimensions: list[tuple[str, tuple[int, int]]],\n",
    "    colors: np.ndarray,\n",
    "):\n",
    "    scatters: list[PathCollection] = []\n",
    "    prev_ax = None\n",
    "    for ax_bg, ax_fg, (_, dim_pair) in zip(axs_latent_bg, axs_latent_fg, dimensions, strict=True):\n",
    "        # Draw first frame\n",
    "        _, _latents = latent_history[0]\n",
    "\n",
    "        ax_bg.set_aspect('equal', adjustable='datalim')\n",
    "        ax_fg.set_aspect('equal', adjustable='datalim')\n",
    "        # ax_bg.sharex(ax_fg)\n",
    "        ax_bg.sharey(ax_fg)\n",
    "        if prev_ax:\n",
    "            ax_fg.sharey(prev_ax)\n",
    "        ax_bg.set_xlim(-1.5, 1.5)\n",
    "        ax_bg.set_ylim(-1.5, 1.5)\n",
    "        ax_fg.set_xlim(-1.5, 1.5)\n",
    "        ax_fg.set_ylim(-1.5, 1.5)\n",
    "        hide_decorations(ax_bg)\n",
    "        hide_decorations(ax_fg)\n",
    "        M = ax_fg.transData.get_matrix()\n",
    "        xscale = M[0, 0]\n",
    "        # yscale = M[1,1]\n",
    "\n",
    "        _latents = _latents[:, dim_pair]\n",
    "        scatter = draw_latent_slice((ax_bg, ax_fg), _latents, colors, dot_size=(xscale * 0.2) ** 2, clip_on=False)[\n",
    "            'scatter'\n",
    "        ]\n",
    "\n",
    "        prev_ax = ax_fg\n",
    "        # ax.patch.set_alpha(1)\n",
    "        scatters.append(scatter)\n",
    "    return scatters\n",
    "\n",
    "\n",
    "def add_parameter_plot(\n",
    "    ax: Axes,\n",
    "    param_history_df: pd.DataFrame,\n",
    "    param_keyframes_df: pd.DataFrame,\n",
    "):\n",
    "    ax.patch.set_facecolor('#222')\n",
    "    ax.tick_params(colors='#aaa')\n",
    "    hide_decorations(ax, ticks=False, background=False)\n",
    "    param_props = param_keyframes_df.columns.difference(list(RESERVED_COLS)).tolist()\n",
    "    param_groups = group_properties_by_scale(param_keyframes_df[param_props])\n",
    "    plot_timeline(\n",
    "        param_history_df,\n",
    "        param_keyframes_df,\n",
    "        [param_groups[0]],\n",
    "        ax=ax,\n",
    "        show_legend=False,\n",
    "        show_phase_labels=False,\n",
    "        line_styles=line_styles,\n",
    "    )\n",
    "    ax.set_xlabel('Step', fontsize='small')\n",
    "    ax.set_ylabel('Param value', fontsize='small')\n",
    "    # ax.set_yscale('log')\n",
    "    param_vline = ax.axvline(0, color='white', linestyle='--', lw=1)\n",
    "    params_legend = unobtrusive_legend(ax)\n",
    "    return param_vline, params_legend\n",
    "\n",
    "\n",
    "def add_metrics_plot(\n",
    "    ax: Axes,\n",
    "    metrics_history: list[tuple[int, float, dict[str, float]]],\n",
    "    step: int,\n",
    "    loss_smooth_window: int,\n",
    "    line_colors: dict[str, ColorType],\n",
    "):\n",
    "    ax.patch.set_facecolor('#222')\n",
    "    ax.yaxis.tick_right()\n",
    "    ax.yaxis.set_label_position('right')\n",
    "    ax.tick_params(colors='#aaa')\n",
    "    hide_decorations(ax, ticks=False, background=False)\n",
    "\n",
    "    metrics_steps = [h[0] for h in metrics_history]\n",
    "\n",
    "    loss_components = {\n",
    "        k: np.array([h[2].get(k, np.nan) for h in metrics_history])  #\n",
    "        for k in metrics_history[0][2].keys()\n",
    "    }\n",
    "    loss_components = {\n",
    "        k: pd.Series(v).ffill().to_numpy()  #\n",
    "        for k, v in loss_components.items()\n",
    "    }\n",
    "    loss_components = {\n",
    "        k: np.convolve(v, np.ones(loss_smooth_window) / loss_smooth_window, mode='same')  #\n",
    "        for k, v in loss_components.items()\n",
    "    }\n",
    "\n",
    "    # ax.plot(metrics_steps, total_losses, label='Total Loss', lw=1)\n",
    "    for name, values in loss_components.items():\n",
    "        ax.plot(metrics_steps, values, label=name, lw=1, alpha=0.8, color=line_colors.get(name))\n",
    "    ax.set_xlabel('Step', fontsize='small')\n",
    "    ax.set_ylabel('Loss', fontsize='small')\n",
    "    ax.set_yscale('log')\n",
    "    metrics_vline = ax.axvline(step, color='white', linestyle='--', lw=1)\n",
    "    metrics_legend = unobtrusive_legend(ax)\n",
    "    return metrics_vline, metrics_legend\n",
    "\n",
    "\n",
    "def distribute_alpha(alpha: float, n_subframes: int) -> float:\n",
    "    \"\"\"Calculate an alpha value to use for each subframe, such that the perceptual alpha for stationary objects is roughly the same as the original.\"\"\"\n",
    "    # Base calculation works well for transparent objects (alpha << 1), but for opaque objects it's too opaque.\n",
    "    base_alpha = 1 - (1 - alpha) ** (1 / n_subframes)\n",
    "    opaque_alpha = 0.99 / (1 + np.log(n_subframes))\n",
    "\n",
    "    opaque_influence = alpha**5  # To make it affect mostly opaque objects\n",
    "    _alpha = np.interp(opaque_influence, [0, 1], [base_alpha, opaque_alpha])\n",
    "\n",
    "    log.debug(f'N: {n_subframes:3d} Alpha: {alpha:.2f} -> {_alpha:.2f} (base: {base_alpha:.2f}, opaque: {opaque_alpha:.2f})')  # fmt: skip\n",
    "    return _alpha\n",
    "\n",
    "\n",
    "# distribute_alpha(1.0, 1)\n",
    "# distribute_alpha(1.0, 2)\n",
    "# distribute_alpha(1.0, 3)\n",
    "# distribute_alpha(1.0, 10)\n",
    "# distribute_alpha(1.0, 100)\n",
    "\n",
    "# distribute_alpha(0.95, 1)\n",
    "# distribute_alpha(0.95, 2)\n",
    "# distribute_alpha(0.95, 3)\n",
    "# distribute_alpha(0.95, 10)\n",
    "# distribute_alpha(0.95, 100)\n",
    "\n",
    "# distribute_alpha(0.9, 1)\n",
    "# distribute_alpha(0.9, 2)\n",
    "# distribute_alpha(0.9, 3)\n",
    "# distribute_alpha(0.9, 10)\n",
    "# distribute_alpha(0.9, 100)\n",
    "\n",
    "# distribute_alpha(0.5, 1)\n",
    "# distribute_alpha(0.5, 2)\n",
    "# distribute_alpha(0.5, 3)\n",
    "# distribute_alpha(0.5, 10)\n",
    "# distribute_alpha(0.5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 67.4 no.1.7: Sampled 987 of 10001 frames (0..10000).\n",
      "I 67.4 no.1.7: Timestep at start: 1, for 22 frames.\n",
      "I 67.4 no.1.7: Largest timestep: 15.\n",
      "I 67.4 no.1.7: Timestep at end: 1, for 2 frames.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 100%; padding: 5px 0; font-family: monospace;\">\n",
       "            <!-- Progress bar container -->\n",
       "            <div style=\"position: relative; height: calc(1em * 5/3); width: 100%;\">\n",
       "                <!-- Triangle indicator -->\n",
       "                <div style=\"position: absolute; bottom: -4px; left: calc(100.0% - 4px);\">\n",
       "                    <div style=\"\n",
       "                        width: 0;\n",
       "                        height: 0;\n",
       "                        border-left: 4px solid transparent;\n",
       "                        border-right: 4px solid transparent;\n",
       "                        border-bottom: 4px solid currentColor;\n",
       "                    \"></div>\n",
       "                </div>\n",
       "                <!-- Progress bar -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    height: 100%;\n",
       "                    width: 100.0%;\n",
       "                    background-color: color(from currentColor srgb r g b / 0.1);\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                \"></div>\n",
       "                <!-- Text overlay -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    width: 100%;\n",
       "                    height: 100%;\n",
       "                    text-align: center;\n",
       "                    line-height: calc(1em * 5/3);\n",
       "                    font-size: 0.9em;\n",
       "                    white-space: nowrap;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5);\n",
       "                \">\n",
       "                    <b>Rendering video</b>: 100.0% [<b>987</b>/987] [<b>15:23</b>/<00:00, 1.07 it/s]\n",
       "                </div>\n",
       "            </div>\n",
       "        \n",
       "            <div style=\"\n",
       "                display: grid;\n",
       "                grid-template-columns: repeat(2, minmax(80px, 1fr));\n",
       "                gap: 5px 0px;\n",
       "                width: 100%;\n",
       "                margin-top: 10px;\n",
       "                font-size: 0.85em;\n",
       "            \"><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">step</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">subframes</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">10000</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">1</div></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <video width=\"960\" height=\"540\" controls loop>\n",
       "            <source src=\"large-assets/ex-1.7-latent-evolution.mp4?v=qs955Rk_VDMKhhPXV682UVAOAUmThC-MRdlFiMNdo4w\" type=\"video/mp4\">\n",
       "            Your browser does not support the video tag.\n",
       "        </video>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampled_indices = bezier_frame_progression(\n",
    "    1000,\n",
    "    len(latent_history),\n",
    "    (0.42, 0.0),  # Ease-in (soft)\n",
    "    (0.95, 1.0),  # Ease-out (sharp)\n",
    ")\n",
    "\n",
    "# sampled_indices = sampled_indices[500:510]  # Limit the number of samples during development\n",
    "# sampled_indices = sampled_indices[-10:]  # Limit the number of samples during development\n",
    "\n",
    "steps = [latent_history[i][0] for i in sampled_indices]\n",
    "delta_steps = np.diff(steps)\n",
    "# Find out how many frames at the start have the same timestep\n",
    "n_ease_in_frames = np.argmax(delta_steps != delta_steps[0]) + 1\n",
    "n_ease_out_frames = np.argmax(delta_steps[::-1] != delta_steps[-1]) + 1\n",
    "log.info(f'Sampled {len(sampled_indices)} of {len(latent_history)} frames ({steps[0]}..{steps[-1]}).')\n",
    "log.info(f'Timestep at start: {delta_steps[0]}, for {n_ease_in_frames} frames.')\n",
    "log.info(f'Largest timestep: {max(delta_steps)}.')\n",
    "log.info(f'Timestep at end: {delta_steps[-1]}, for {n_ease_out_frames} frames.')\n",
    "\n",
    "# Realize timelines for both dopesheets\n",
    "timeline = Timeline(dopesheet)\n",
    "history_df = realize_timeline(timeline)\n",
    "keyframes_df = dopesheet.as_df()\n",
    "\n",
    "# --- Render ---\n",
    "fig, ani = animate_latent_evolution_with_metrics(\n",
    "    sampled_indices=sampled_indices,\n",
    "    latent_history=latent_history,\n",
    "    metrics_history=metrics.history,\n",
    "    param_history_df=history_df,\n",
    "    param_keyframes_df=keyframes_df,\n",
    "    colors=rgb_tensor.cpu().numpy(),\n",
    "    dimensions=[('hue', (1, 0)), ('', (1, 2)), ('', (3, 2))],\n",
    "    loss_smooth_window=25,\n",
    "    alpha=0.9,\n",
    ")\n",
    "\n",
    "# --- Save the video ---\n",
    "video_file = f'large-assets/ex-{nbid}-latent-evolution.mp4'\n",
    "step_sizes = np.concatenate((delta_steps, [1]))\n",
    "with RichProgress(total=len(sampled_indices), description='Rendering video') as pbar:\n",
    "    ani.save(\n",
    "        video_file,\n",
    "        fps=30,\n",
    "        extra_args=['-vcodec', 'libx264'],\n",
    "        progress_callback=lambda i, n: pbar.update(1, metrics={'step': sampled_indices[i], 'subframes': step_sizes[i]}),\n",
    "    )\n",
    "plt.close(fig)\n",
    "\n",
    "# --- Display the video ---\n",
    "import secrets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "cache_buster = secrets.token_urlsafe()\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        f\"\"\"\n",
    "        <video width=\"960\" height=\"540\" controls loop>\n",
    "            <source src=\"{video_file}?v={cache_buster}\" type=\"video/mp4\">\n",
    "            Your browser does not support the video tag.\n",
    "        </video>\n",
    "        \"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "The model trained really nicely! It's nice having a simple curriculum compared to before: it makes it a lot easier to interpret and iterate on parameter values.\n",
    "\n",
    "As an aside: I'm loving these animated visualizations of latent space. Seeing how the space changes _for every batch_ has given me insights into potential hyperparameter tweaks that would have been hard to find otherwise. Perhaps carefully selected metrics could have given similar insight (as a static plot), but I don't know which ones they would be, nor how you could know in advance which metrics to choose.\n",
    "\n",
    "#### More predictable results\n",
    "\n",
    "This curriculum seems to never become tangled, unlike the earlier curricula that started with a phase of just the primary and secondary colors. In those notebooks, sometimes (25%?) the first phase would result in the hues being out of order on the color wheel. Subsequent phases would then fail, i.e. the resulting latent space was highly distorted instead of being a smooth ball. I haven't seen that happen once with this notebook. I think it's probably because 6 points were simply not enough to inform the optimizer how the space could be unfolded; whereas by showing it all the data from the start (and particularly less vibrant colors like black, white, and gray), any misstep towards a crumpled space would immediately result in higher reconstruction loss.\n",
    "\n",
    "Perhaps it would be possible to train this simple MLP by starting with a phase consisting of all eight corners of the RGB cube: the primary and secondary colors, plus white and black. However for larger models such as LLMs, this is likely to be impossible: a key assumption of this work is that you will know of some concepts ahead of time that you care about (such as \"maliciousness\"), and you would like to regularize those without having to also identify all the other concepts.\n",
    "\n",
    "#### Stochastic labelling works just as well as smooth labels\n",
    "\n",
    "I've run this notebook several times with both:\n",
    "\n",
    "- Smooth labels, in which the labels were continuous (0..1), and\n",
    "- Hard labels, in which the the labels were stochastically discretized (0 or 1).\n",
    "\n",
    "The regularizers use the labels as weights — so, for example:\n",
    "\n",
    "- In the smooth case, vibrant colors would _always_ be regularized toward the hue plane with some constant weight, while less-saturated colors would always receive a lower weight. So vibrant colors are pushed toward the plane with more force.\n",
    "- In the hard case, vibrant colors are _sometimes_ regularized toward the hue plane, and less-saturated colors are _less frequently_ regularized in the same way. But when a sample is selected to be regularized, it always has the same weight. So vibrant and less-saturated colors are both pushed toward the plane with the same force, but vibrant colors experience that force more often.\n",
    "\n",
    "Despite these differences, the overall force experienced by a particular color should be the same with both smooth and hard labels. But I wondered whether the stochastic case might cause too much noise, and would lead to unstable training dynamics and a lumpy latent space. **This was not the case:** there is no observable diffence in training dynamics between the smooth and hard labels. It's possible that there may have been a difference if higher values had been used for the global regularization weights (hyperparameters), but doing so would not have led to good training dynamics anyway.\n",
    "\n",
    "#### Varying hyperparameters seems to be required (?)\n",
    "\n",
    "I put a reasonable effort into finding _constant_ hyperparameter values for the regularization terms, but I couldn't find any that worked as well as the curriculum shown above. For example, if the _polar_ term (which pushes red toward $1,0,0,0$) is held constant, it needs to be lower so that it doesn't excessively attract nearby colors. But then it's not strong enough to pull pure red to the target point. Perhaps it would work if I let it train for longer, but I worry that that might make using these methods on larger models infeasible.\n",
    "\n",
    "I think that, while the data curriculum is unneeded, the hyperparameter curriculum does indeed help to give good training dynamics. Of note, though: it looks like the latent space is noisiest (most jiggly) during periods of rapid hyperparameter change. I guess that's because the loss landscape is changing, and the optimizer suddenly finds it needs to make a correction. I wondered if that might be bad, but on the other hand it might be helping the optimizer to thread a path to a more optimal solution: paths would be available to it that would simply not be there in a static loss landscape. But this is just conjecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This experiment seems to confirm our hypothesis: beginning training with a small set of core concepts does _not_ help, not even when your goal is to use those concepts to impose a structure on latent space. In fact, unless you capture all key concepts in the data — including those you don't care about — then training on the reduced set of concepts can cause your latent space to become \"crumpled\". Training with the full dataset right from the start encourages the optimizer to discover a smooth latent space, which should make later interpretability efforts easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
