{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1.8: Regularizer combinations\n",
    "\n",
    "In [Ex 1.7](./ex-1.7-sparse-labels.ipynb), we successfully imposed structure on latent space using several regularizers and weak supervision. In this experiment, we'll test what the latent space looks like with various combinations of regularizers, to see what they each contribute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from utils.logging import SimpleLoggingConfig\n",
    "\n",
    "logging_config = (\n",
    "    SimpleLoggingConfig()\n",
    "    .info('notebook', 'utils', 'mini', 'ex_color')\n",
    "    .error('matplotlib.axes')  # Silence warnings about set_aspect\n",
    ")\n",
    "logging_config.apply()\n",
    "\n",
    "# ID for tagging assets\n",
    "nbid = '1.8'\n",
    "# This is the logger for this notebook\n",
    "log = logging.getLogger(f'notebook.{nbid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 0.9 ut.re:   Selected 147 of 169 dependencies\n",
      "I 0.9 ut.re:   Found 3 local packages: ex_color, mini, utils\n"
     ]
    }
   ],
   "source": [
    "import modal\n",
    "from mini.experiment import Experiment\n",
    "from infra.requirements import freeze, project_packages\n",
    "\n",
    "run = Experiment(f'ex-color-{nbid}')\n",
    "run.image = modal.Image.debian_slim().pip_install(*freeze(all=True)).add_local_python_source(*project_packages())\n",
    "run.before_each(logging_config.apply)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "We use the same simple 2-layer MLP autoencoder with a 4D bottleneck as in previous experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "E = 4\n",
    "\n",
    "\n",
    "class ColorMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # RGB input (3D) → hidden layer → bottleneck → hidden layer → RGB output\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.GELU(),\n",
    "            # nn.Linear(16, 16),\n",
    "            # nn.GELU(),\n",
    "            nn.Linear(16, E),  # Our critical bottleneck!\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(E, 16),\n",
    "            nn.GELU(),\n",
    "            # nn.Linear(16, 16),\n",
    "            # nn.GELU(),\n",
    "            nn.Linear(16, 3),\n",
    "            nn.Sigmoid(),  # Keep RGB values in [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Get our bottleneck representation\n",
    "        bottleneck = self.encoder(x)\n",
    "\n",
    "        # Decode back to RGB\n",
    "        output = self.decoder(bottleneck)\n",
    "        return output, bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training machinery with timeline and events\n",
    "\n",
    "The training loop stays the same.\n",
    "\n",
    "The `train_color_model` function orchestrates the training process based on a `Timeline` derived from the dopesheet. It handles:\n",
    "\n",
    "- Iterating through training steps.\n",
    "- Fetching the correct data loader for the current phase.\n",
    "- Updating hyperparameters (like learning rate and loss weights) smoothly based on the timeline state.\n",
    "- Calculating the combined loss from reconstruction and various regularizers.\n",
    "- Regularizers are applied with diffent weights for each sample based on the sample labels.\n",
    "- Executing the optimizer step.\n",
    "- Emitting events at different points (phase start/end, pre-step, actions like 'anchor', step metrics) to trigger callbacks like plotting, recording, or updating loss terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Protocol, runtime_checkable\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "from mini.temporal.timeline import TStep\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InferenceResult:\n",
    "    outputs: Tensor\n",
    "    latents: Tensor\n",
    "\n",
    "    def detach(self):\n",
    "        return InferenceResult(self.outputs.detach(), self.latents.detach())\n",
    "\n",
    "    def clone(self):\n",
    "        return InferenceResult(self.outputs.clone(), self.latents.clone())\n",
    "\n",
    "    def cpu(self):\n",
    "        return InferenceResult(self.outputs.cpu(), self.latents.cpu())\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class LossCriterion(Protocol):\n",
    "    def __call__(self, data: Tensor, res: InferenceResult) -> Tensor: ...\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RegularizerConfig:\n",
    "    \"\"\"Configuration for a regularizer, including label affinities.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    \"\"\"Matched with hyperparameter for weighting\"\"\"\n",
    "    criterion: LossCriterion\n",
    "    label_affinities: dict[str, float] | None\n",
    "    \"\"\"Maps label names to affinity strengths\"\"\"\n",
    "\n",
    "\n",
    "@dataclass(eq=False, frozen=True)\n",
    "class Event:\n",
    "    name: str\n",
    "    step: int\n",
    "    model: ColorMLP\n",
    "    timeline_state: TStep\n",
    "    optimizer: optim.Optimizer\n",
    "\n",
    "\n",
    "@dataclass(eq=False, frozen=True)\n",
    "class PhaseEndEvent(Event):\n",
    "    validation_data: Tensor\n",
    "    inference_result: InferenceResult\n",
    "\n",
    "\n",
    "@dataclass(eq=False, frozen=True)\n",
    "class StepMetricsEvent(Event):\n",
    "    \"\"\"Event carrying metrics calculated during a training step.\"\"\"\n",
    "\n",
    "    train_batch: Tensor\n",
    "    total_loss: float\n",
    "    losses: dict[str, float]\n",
    "\n",
    "\n",
    "class EventHandler[T](Protocol):\n",
    "    def __call__(self, event: T) -> None: ...\n",
    "\n",
    "\n",
    "class EventBinding[T]:\n",
    "    \"\"\"A class to bind events to handlers.\"\"\"\n",
    "\n",
    "    def __init__(self, event_name: str):\n",
    "        self.event_name = event_name\n",
    "        self.handlers: list[tuple[str, EventHandler[T]]] = []\n",
    "\n",
    "    def add_handler(self, event_name: str, handler: EventHandler[T]) -> None:\n",
    "        self.handlers.append((event_name, handler))\n",
    "\n",
    "    def emit(self, event_name: str, event: T) -> None:\n",
    "        for name, handler in self.handlers:\n",
    "            if name == event_name:\n",
    "                handler(event)\n",
    "\n",
    "\n",
    "class EventHandlers:\n",
    "    \"\"\"A simple event system to allow for custom callbacks.\"\"\"\n",
    "\n",
    "    phase_start: EventBinding[Event]\n",
    "    pre_step: EventBinding[Event]\n",
    "    action: EventBinding[Event]\n",
    "    phase_end: EventBinding[PhaseEndEvent]\n",
    "    step_metrics: EventBinding[StepMetricsEvent]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.phase_start = EventBinding[Event]('phase-start')\n",
    "        self.pre_step = EventBinding[Event]('pre-step')\n",
    "        self.action = EventBinding[Event]('action')\n",
    "        self.phase_end = EventBinding[PhaseEndEvent]('phase-end')\n",
    "        self.step_metrics = EventBinding[StepMetricsEvent]('step-metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "\n",
    "def periodic[T: Event](\n",
    "    handler: EventHandler[T], *, interval: int, offset: int = 0, use_step: bool = True\n",
    ") -> EventHandler[T]:\n",
    "    \"\"\"Decorator to run a handler at regular intervals.\"\"\"\n",
    "    i = 0\n",
    "\n",
    "    @wraps(handler)\n",
    "    def handler_wrapper(event: T):\n",
    "        nonlocal i\n",
    "        if use_step:\n",
    "            i = event.step\n",
    "        try:\n",
    "            if (i + offset) % interval == 0:\n",
    "                handler(event)\n",
    "        finally:\n",
    "            i += 1\n",
    "\n",
    "    return handler_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from typing import Iterable, Iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from mini.temporal.dopesheet import Dopesheet\n",
    "from mini.temporal.timeline import Timeline\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    log.debug(f'Global random seed set to {seed}')\n",
    "\n",
    "\n",
    "def set_deterministic_mode(seed: int):\n",
    "    \"\"\"Make experiments reproducible.\"\"\"\n",
    "    import torch\n",
    "\n",
    "    seed_everything(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    log.debug('PyTorch set to deterministic mode')\n",
    "\n",
    "\n",
    "def reiterate[T](it: Iterable[T]) -> Iterator[T]:\n",
    "    \"\"\"\n",
    "    Iterates over an iterable indefinitely.\n",
    "\n",
    "    When the iterable is exhausted, it starts over from the beginning. Unlike\n",
    "    `itertools.cycle`, yielded values are not cached — so each iteration may be\n",
    "    different.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        yield from it\n",
    "\n",
    "\n",
    "def train_color_model(  # noqa: C901\n",
    "    model: ColorMLP,\n",
    "    train_loader: DataLoader,\n",
    "    val_data: Tensor,\n",
    "    dopesheet: Dopesheet,\n",
    "    loss_criterion: LossCriterion,\n",
    "    regularizers: list[RegularizerConfig],\n",
    "    event_handlers: EventHandlers | None = None,\n",
    "):\n",
    "    if event_handlers is None:\n",
    "        event_handlers = EventHandlers()\n",
    "\n",
    "    # --- Validate inputs ---\n",
    "    if 'lr' not in dopesheet.props:\n",
    "        raise ValueError(\"Dopesheet must define the 'lr' property column.\")\n",
    "    # --- End Validation ---\n",
    "\n",
    "    timeline = Timeline(dopesheet)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    train_data = iter(reiterate(train_loader))\n",
    "\n",
    "    total_steps = len(timeline)\n",
    "\n",
    "    for step in range(total_steps):\n",
    "        # Get state *before* advancing timeline for this step's processing\n",
    "        current_state = timeline.state\n",
    "        current_phase_name = current_state.phase\n",
    "\n",
    "        batch_data, batch_labels = next(train_data)\n",
    "        # Should already be on device\n",
    "        # batch_data = batch_data.to(device)\n",
    "        # batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # --- Event Handling ---\n",
    "        event_template = {\n",
    "            'step': step,\n",
    "            'model': model,\n",
    "            'timeline_state': current_state,\n",
    "            'optimizer': optimizer,\n",
    "        }\n",
    "\n",
    "        if current_state.is_phase_start:\n",
    "            event = Event(name=f'phase-start:{current_phase_name}', **event_template)\n",
    "            event_handlers.phase_start.emit(event.name, event)\n",
    "            event_handlers.phase_start.emit('phase-start', event)\n",
    "\n",
    "        for action in current_state.actions:\n",
    "            event = Event(name=f'action:{action}', **event_template)\n",
    "            event_handlers.action.emit(event.name, event)\n",
    "            event_handlers.action.emit('action', event)\n",
    "\n",
    "        event = Event(name='pre-step', **event_template)\n",
    "        event_handlers.pre_step.emit('pre-step', event)\n",
    "\n",
    "        # --- Training Step ---\n",
    "        # ... (get data, update LR, zero grad, forward pass, calculate loss, backward, step) ...\n",
    "\n",
    "        current_lr = current_state.props['lr']\n",
    "        # REF_BATCH_SIZE = 32\n",
    "        # lr_scale_factor = batch.shape[0] / REF_BATCH_SIZE\n",
    "        # current_lr = current_lr * lr_scale_factor\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = current_lr\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs, latents = model(batch_data)\n",
    "        current_results = InferenceResult(outputs, latents)\n",
    "\n",
    "        primary_loss = loss_criterion(batch_data, current_results).mean()\n",
    "        losses = {'recon': primary_loss.item()}\n",
    "        total_loss = primary_loss\n",
    "        zeros = torch.tensor(0.0, device=batch_data.device)\n",
    "\n",
    "        for regularizer in regularizers:\n",
    "            name = regularizer.name\n",
    "            criterion = regularizer.criterion\n",
    "\n",
    "            weight = current_state.props.get(name, 1.0)\n",
    "            if weight == 0:\n",
    "                continue\n",
    "\n",
    "            if regularizer.label_affinities is not None:\n",
    "                # Soft labels that indicate how much effect this regularizer has, based on its affinity with the label\n",
    "                label_probs = [\n",
    "                    batch_labels[k] * v\n",
    "                    for k, v in regularizer.label_affinities.items()\n",
    "                    if k in batch_labels  #\n",
    "                ]\n",
    "                if not label_probs:\n",
    "                    continue\n",
    "\n",
    "                sample_affinities = torch.stack(label_probs, dim=0).sum(dim=0)\n",
    "                sample_affinities = torch.clamp(sample_affinities, 0.0, 1.0)\n",
    "                if torch.allclose(sample_affinities, zeros):\n",
    "                    continue\n",
    "            else:\n",
    "                sample_affinities = torch.ones(batch_data.shape[0], device=batch_data.device)\n",
    "\n",
    "            per_sample_loss = criterion(batch_data, current_results)\n",
    "            if len(per_sample_loss.shape) == 0:\n",
    "                # If the loss is a scalar, we need to expand it to match the batch size\n",
    "                per_sample_loss = per_sample_loss.expand(batch_data.shape[0])\n",
    "            assert per_sample_loss.shape[0] == batch_data.shape[0], f'Loss should be per-sample OR scalar: {name}'\n",
    "\n",
    "            # Apply sample affinities\n",
    "            weighted_loss = per_sample_loss * sample_affinities\n",
    "\n",
    "            # Apply sample importance weights\n",
    "            # weighted_loss *= batch_weights\n",
    "\n",
    "            # Calculate mean only over selected samples. If we used torch.mean, it would average over all samples, including those with 0 weight\n",
    "            term_loss = weighted_loss.sum() / (sample_affinities.sum() + 1e-8)\n",
    "\n",
    "            losses[name] = term_loss.item()\n",
    "            if not torch.isfinite(term_loss):\n",
    "                log.warning(f'Loss term {name} at step {step} is not finite: {term_loss}')\n",
    "                continue\n",
    "            total_loss += term_loss * weight\n",
    "\n",
    "        if total_loss > 0:\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "        # --- End Training Step ---\n",
    "\n",
    "        # Emit step metrics event\n",
    "        step_metrics_event = StepMetricsEvent(\n",
    "            name='step-metrics',\n",
    "            **event_template,\n",
    "            train_batch=batch_data,\n",
    "            total_loss=total_loss.item(),\n",
    "            losses=losses,\n",
    "        )\n",
    "        event_handlers.step_metrics.emit('step-metrics', step_metrics_event)\n",
    "\n",
    "        # --- Post-Step Event Handling ---\n",
    "        if current_state.is_phase_end:\n",
    "            # Trigger phase-end for the *current* phase\n",
    "            # validation_data = batch_data\n",
    "            with torch.no_grad():\n",
    "                val_outputs, val_latents = model(val_data.to(device))\n",
    "            event = PhaseEndEvent(\n",
    "                name=f'phase-end:{current_phase_name}',\n",
    "                **event_template,\n",
    "                validation_data=val_data,\n",
    "                inference_result=InferenceResult(val_outputs, val_latents),\n",
    "            )\n",
    "            event_handlers.phase_end.emit(event.name, event)\n",
    "            event_handlers.phase_end.emit('phase-end', event)\n",
    "        # --- End Event Handling ---\n",
    "\n",
    "        # Advance timeline *after* processing the current step\n",
    "        if step < total_steps:  # Avoid stepping past the end\n",
    "            timeline.step()\n",
    "\n",
    "    log.debug('Training finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "We define an event handler that periodically draws scatter plots of the latent embeddings.\n",
    "\n",
    "This has been refactored compared to earlier experiments: it has been split into several parts:\n",
    "- `evaluate_latents`: The event handler (remote)\n",
    "- `HistoryStore`: Receives latents from the event handler (local)\n",
    "- `history_plotter`: Callback that displays the contents of the `HistoryStore` (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "if typing.TYPE_CHECKING:\n",
    "    from matplotlib.axes import Axes\n",
    "    import numpy.typing as npt\n",
    "\n",
    "\n",
    "def hide_decorations(ax: Axes, background: bool = True, ticks: bool = True, border: bool = True) -> None:\n",
    "    \"\"\"Remove all decorations from the axes.\"\"\"\n",
    "    if background:\n",
    "        ax.patch.set_alpha(0)\n",
    "    if ticks:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    if border:\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "\n",
    "\n",
    "def draw_latent_slice(\n",
    "    ax: Axes | tuple[Axes, Axes],  # Axes for background and foreground\n",
    "    latents: npt.NDArray,\n",
    "    colors: npt.NDArray,\n",
    "    dot_size: float = 200,\n",
    "    clip_on: bool = False,\n",
    "):\n",
    "    \"\"\"Draw a slice of the latent space.\"\"\"\n",
    "    from matplotlib.patches import Circle\n",
    "\n",
    "    assert latents.ndim == 2, 'Latents should be 2D'\n",
    "    assert latents.shape[1] == 2, 'Latents should have 2 dimensions'\n",
    "\n",
    "    bg, fg = ax if isinstance(ax, tuple) else (ax, ax)\n",
    "    return {\n",
    "        'circ': bg.add_patch(\n",
    "            Circle((0, 0), 1, facecolor='#111', edgecolor='#0000', fill=True, zorder=-1, clip_on=clip_on)\n",
    "        ),\n",
    "        'scatter': fg.scatter(latents[:, 0], latents[:, 1], c=colors, s=dot_size, alpha=0.7, clip_on=clip_on),\n",
    "        'circ-top': fg.add_patch(Circle((0, 0), 1, edgecolor='#1118', fill=False, clip_on=clip_on)),\n",
    "    }\n",
    "\n",
    "\n",
    "def geometric_frame_progression(samples: int, n: int, offset: int = 10) -> npt.NDArray[np.int_]:\n",
    "    \"\"\"\n",
    "    Generate a geometric progression of indices for sampling frames.\n",
    "\n",
    "    Useful for creating videos of simulations in which there is more detail at the beginning.\n",
    "\n",
    "    Args:\n",
    "        samples (int): Number of samples to generate. If larger than n, it will be capped to n.\n",
    "        n (int): Total number of frames to sample from.\n",
    "        offset (int): Offset to apply during generation. Increase this to get more samples at the start of short sequences; larger values will result in \"flatter\" sequences.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of indices.\n",
    "    \"\"\"\n",
    "    samples = min(samples, n)\n",
    "    if samples <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    return np.unique(np.geomspace(offset, n + offset, samples, endpoint=False, dtype=int) - offset)\n",
    "\n",
    "\n",
    "def bezier_frame_progression(\n",
    "    samples: int,\n",
    "    n: int,\n",
    "    cp1: tuple[float, float],\n",
    "    cp2: tuple[float, float],\n",
    "    lookup_table_size: int = 200,\n",
    ") -> npt.NDArray[np.int_]:\n",
    "    \"\"\"\n",
    "    Generate a bezier curve for sampling frames.\n",
    "\n",
    "    Args:\n",
    "        samples: Number of samples to generate (inclusive upper bound).\n",
    "        n: Total number of frames to sample from.\n",
    "        cp1: Control point 1 (t_in, t_out), e.g. (0, 0) for a linear curve, or (0.42, 0) for a classic ease-in.\n",
    "        cp2: Control point 2 (t_in, t_out), e.g. (1, 1) for a linear curve, or (0.58, 1) for a classic ease-out.\n",
    "        lookup_table_size: Number of points to sample on the bezier curve. Higher values will result in smoother curves, but will take longer to compute.\n",
    "    \"\"\"\n",
    "    from matplotlib.bezier import BezierSegment\n",
    "\n",
    "    if samples <= 0 or n <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    if n == 1:\n",
    "        return np.array([0], dtype=int)\n",
    "    control_points = np.array([(0, 0), cp1, cp2, (1, 1)])\n",
    "    bezier_curve = BezierSegment(control_points)\n",
    "    t_lookup = np.linspace(0, 1, lookup_table_size)\n",
    "    points_on_curve = bezier_curve(t_lookup)\n",
    "    xs = points_on_curve[:, 0]\n",
    "    ys = points_on_curve[:, 1]\n",
    "    assert np.all(np.diff(xs) >= 0), 't is not monotonic'\n",
    "\n",
    "    # Generate linearly spaced input times for sampling our easing function\n",
    "    t_linear_input = np.linspace(0, 1, samples, endpoint=True)\n",
    "\n",
    "    # Interpolate to get eased time:\n",
    "    # For each t_linear_input (our desired \"progress through animation\"),\n",
    "    # find the corresponding y-value on the Bézier curve (our \"eased progress\").\n",
    "    t_eased = np.interp(t_linear_input, xs, ys)\n",
    "\n",
    "    # Clip eased time to [0,1] in case control points caused overshoot/undershoot\n",
    "    # and we want to strictly map to frame indices.\n",
    "    t_eased = np.clip(t_eased, 0.0, 1.0)\n",
    "\n",
    "    # Scale to frame indices and ensure they are unique and sorted\n",
    "    frame_indices = np.round(t_eased * (n - 1)).astype(int)\n",
    "    unique_indices = np.unique(frame_indices)\n",
    "\n",
    "    return unique_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "from IPython.display import HTML\n",
    "\n",
    "from utils.coro import debounced\n",
    "from utils.nb import save_fig\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LatentEvaluation:\n",
    "    variant: str\n",
    "    step: int\n",
    "    latents: np.ndarray\n",
    "\n",
    "\n",
    "def evaluate_latents(data: Tensor, variant_name: str, callback: Callable[[LatentEvaluation]]) -> EventHandler[Event]:\n",
    "    # Runs remotely, in the training loop.\n",
    "\n",
    "    def _evaluate_latents(event: Event):\n",
    "        from utils.torch.training import mode\n",
    "\n",
    "        log.debug(f'Evaluating latents at step {event.step} for variant {variant_name}')\n",
    "\n",
    "        with torch.no_grad(), mode(event.model, 'eval'):\n",
    "            _output, latents = event.model(data)\n",
    "\n",
    "        callback(\n",
    "            LatentEvaluation(\n",
    "                variant=variant_name,\n",
    "                step=event.step,\n",
    "                latents=latents.detach().cpu().numpy(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return _evaluate_latents\n",
    "\n",
    "\n",
    "class HistoryStore:\n",
    "    # Exists locally\n",
    "    def __init__(self):\n",
    "        self.histories: dict[str, list[LatentEvaluation]] = {}\n",
    "        self.observers: dict[str, list[Callable[[list[LatentEvaluation]]]]] = {}\n",
    "\n",
    "    def __reduce__(self):\n",
    "        raise RuntimeError('HistoryStore cannot be pickled - it should only exist locally')\n",
    "\n",
    "    def add(self, latent_eval: LatentEvaluation):\n",
    "        if latent_eval.variant not in self.histories:\n",
    "            self.histories[latent_eval.variant] = []\n",
    "        self.histories[latent_eval.variant].append(latent_eval)\n",
    "        self.notify(latent_eval.variant)\n",
    "\n",
    "    def notify(self, variant: str):\n",
    "        for observer in self.observers.get(variant, []):\n",
    "            observer(self.histories.get(variant, []))\n",
    "\n",
    "    def register_observer(self, variant: str, observer: Callable[[list[LatentEvaluation]]]):\n",
    "        \"\"\"Register an observer for a specific variant.\"\"\"\n",
    "        if variant not in self.observers:\n",
    "            self.observers[variant] = []\n",
    "        self.observers[variant].append(observer)\n",
    "\n",
    "\n",
    "@run.hither\n",
    "def store_latents(store: HistoryStore):\n",
    "    # Sends latents from remote to the local store.\n",
    "    async def _store_latents(latent_eval: LatentEvaluation):\n",
    "        store.add(latent_eval)\n",
    "\n",
    "    return _store_latents\n",
    "\n",
    "\n",
    "def safe_filename(name: str) -> str:\n",
    "    \"\"\"Convert a name to a safe filename by replacing non-alphanumeric characters.\"\"\"\n",
    "    import re\n",
    "\n",
    "    return re.sub(r'[^a-zA-Z0-9_-]', '_', name)\n",
    "\n",
    "\n",
    "def history_plotter(colors: np.ndarray, *, dim_pairs: list[tuple[int, int]], variant_name: str = ''):\n",
    "    \"\"\"Plot latent space\"\"\"\n",
    "    from utils.nb import displayer\n",
    "\n",
    "    # Store (phase_name, end_step, data, result) - data comes from event now\n",
    "    display = displayer()\n",
    "    _colors = colors.copy()\n",
    "\n",
    "    @debounced\n",
    "    def plot_history(history: list[LatentEvaluation]):\n",
    "        try:\n",
    "            fig = _plot_history(history, _colors, dim_pairs, variant_name)\n",
    "            # display(fig)\n",
    "            suffix = safe_filename(variant_name or '')\n",
    "            display(\n",
    "                HTML(\n",
    "                    save_fig(\n",
    "                        fig,\n",
    "                        f'large-assets/ex-{nbid}-color-phase-history{suffix}.png',\n",
    "                        alt_text='Visualizations of latent space at the end of each curriculum phase.',\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        finally:\n",
    "            plt.close()\n",
    "\n",
    "    return plot_history\n",
    "\n",
    "\n",
    "def _plot_history(\n",
    "    history: list[LatentEvaluation], colors: np.ndarray, dim_pairs: list[tuple[int, int]], variant_name: str\n",
    "):\n",
    "    if not history:\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_facecolor('#333')\n",
    "        ax.set_facecolor('#222')\n",
    "        ax.text(0.5, 0.5, 'Waiting...', ha='center', va='center')\n",
    "        return fig\n",
    "\n",
    "    plt.style.use('dark_background')\n",
    "\n",
    "    # Number of dimension pairs\n",
    "    num_dim_pairs = len(dim_pairs)\n",
    "\n",
    "    # Cap the number of thumbnails to a maximum for readability\n",
    "    max_thumbnails = 10\n",
    "    indices = geometric_frame_progression(max_thumbnails, len(history), offset=10)\n",
    "    history_to_show = [history[i] for i in indices]\n",
    "\n",
    "    # Create figure with gridspec for flexible layout\n",
    "    fig = plt.figure(figsize=(12, 5), facecolor='#333')\n",
    "\n",
    "    # Create two separate gridspecs - one for thumbnails, one for latest state\n",
    "    gs = fig.add_gridspec(3, 1, hspace=0.1, height_ratios=[0.5, 4, 1])\n",
    "\n",
    "    # Latest state gridspec (top row) - all dimension pairs\n",
    "    latest_gs = gs[1].subgridspec(2, num_dim_pairs, wspace=0, hspace=0.1, height_ratios=[0, 1])\n",
    "\n",
    "    # Thumbnail gridspec (bottom row) - only first dimension pair\n",
    "    thumbnail_gs = gs[2].subgridspec(2, max_thumbnails, wspace=0, hspace=0.1, height_ratios=[1, 0])\n",
    "\n",
    "    # Create thumbnail axes and plot history\n",
    "    for i, le in enumerate(history_to_show):\n",
    "        latents, step = le.latents, le.step\n",
    "\n",
    "        # Only plot the first dimension pair for thumbnails\n",
    "        dim1, dim2 = dim_pairs[0]\n",
    "\n",
    "        # Create title for the thumbnail as its own axes, so that it's aligned with the other titles\n",
    "        axt = fig.add_subplot(thumbnail_gs[1, i])\n",
    "        axt.text(\n",
    "            0,\n",
    "            0,\n",
    "            f'{step}',\n",
    "            # transform=axt.transAxes,\n",
    "            horizontalalignment='center',\n",
    "            verticalalignment='top',\n",
    "            fontsize=7,\n",
    "        )\n",
    "        # Remove all decorations\n",
    "        hide_decorations(axt)\n",
    "\n",
    "        # Create thumbnail axis\n",
    "        ax = fig.add_subplot(thumbnail_gs[0, i])\n",
    "        ax.sharex(axt)\n",
    "        draw_latent_slice(ax, latents[:, [dim1, dim2]], colors, dot_size=20)\n",
    "        hide_decorations(ax)\n",
    "\n",
    "        # Ensure square aspect ratio\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_adjustable('box')\n",
    "\n",
    "    # Plot latest state\n",
    "    # Get the latest data\n",
    "    le = history[-1]\n",
    "    latents, step = le.latents, le.step\n",
    "\n",
    "    prev_ax = None\n",
    "    for i, (dim1, dim2) in enumerate(dim_pairs):\n",
    "        # Create title for the thumbnail as its own axes, so that it's aligned with the other titles\n",
    "        axt = fig.add_subplot(latest_gs[0, i])\n",
    "        axt.text(\n",
    "            0,\n",
    "            0,\n",
    "            f'[{dim1}, {dim2}]',\n",
    "            # transform=axt.transAxes,\n",
    "            horizontalalignment='center',\n",
    "            fontsize=10,\n",
    "        )\n",
    "        hide_decorations(axt)\n",
    "\n",
    "        # Plot\n",
    "        ax = fig.add_subplot(latest_gs[1, i])\n",
    "        ax.sharex(axt)\n",
    "        if prev_ax is not None:\n",
    "            ax.sharey(prev_ax)\n",
    "        prev_ax = ax\n",
    "        draw_latent_slice(ax, latents[:, [dim1, dim2]], colors)\n",
    "        hide_decorations(ax)\n",
    "\n",
    "        # Ensure square aspect ratio\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_adjustable('box')\n",
    "\n",
    "    # Add overall title\n",
    "    fig.suptitle(f'Latent space — step {step}', fontsize=12, color='white')\n",
    "\n",
    "    # Subtitle\n",
    "    ax = fig.add_subplot(gs[0])\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        1,\n",
    "        f'{variant_name}',\n",
    "        horizontalalignment='center',\n",
    "        verticalalignment='top',\n",
    "        fontsize='small',\n",
    "        color='white',\n",
    "    )\n",
    "    hide_decorations(ax)\n",
    "\n",
    "    # Use subplots_adjust instead of tight_layout to avoid warnings\n",
    "    fig.subplots_adjust(top=0.9, bottom=0.1, left=0.1, right=0.95)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter dopesheet\n",
    "\n",
    "As in previous experiments, we'll define a dopesheet (timeline) to allow hyperparameters to vary over time. Not all of the hyperparameters will be used for each run: it depends on which regularizers are in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "from mini.temporal.vis import plot_timeline, realize_timeline, ParamGroup\n",
    "from mini.temporal.dopesheet import Dopesheet\n",
    "from mini.temporal.timeline import Timeline\n",
    "from utils.nb import save_fig\n",
    "\n",
    "\n",
    "line_styles = [\n",
    "    (re.compile(r'^data-'), {'linewidth': 5, 'zorder': -1, 'alpha': 0.5}),\n",
    "    # (re.compile(r'-(anchor|norm)$'), {'linewidth': 2, 'linestyle': (0, (8, 1, 1, 1))}),\n",
    "]\n",
    "\n",
    "\n",
    "def load_dopesheet():\n",
    "    return Dopesheet.from_csv(f'ex-{nbid}-dopesheet.csv')\n",
    "\n",
    "\n",
    "def plot_dopesheet(dopesheet: Dopesheet):\n",
    "    # display(Markdown(f\"\"\"## Parameter schedule ({variant})\\n{dopesheet.to_markdown()}\"\"\"))\n",
    "\n",
    "    timeline = Timeline(dopesheet)\n",
    "    history_df = realize_timeline(timeline)\n",
    "    keyframes_df = dopesheet.as_df()\n",
    "\n",
    "    groups = (\n",
    "        ParamGroup(\n",
    "            name='',\n",
    "            params=[p for p in dopesheet.props if p not in {'lr'}],\n",
    "            height_ratio=2,\n",
    "        ),\n",
    "        ParamGroup(\n",
    "            name='',\n",
    "            params=[p for p in dopesheet.props if p in {'lr'}],\n",
    "            height_ratio=1,\n",
    "        ),\n",
    "    )\n",
    "    fig, ax = plot_timeline(history_df, keyframes_df, groups, line_styles=line_styles)\n",
    "    # Add assertion to satisfy type checker\n",
    "    assert isinstance(fig, Figure), 'plot_timeline should return a Figure'\n",
    "    display(\n",
    "        HTML(\n",
    "            save_fig(\n",
    "                fig,\n",
    "                f'large-assets/ex-{nbid}-color-timeline.png',\n",
    "                alt_text='Timeline visualization showing the dopesheet parameter schedule with keyframes and interpolated values over time.',\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "dopesheet = load_dopesheet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions and regularizers\n",
    "\n",
    "As in earlier experiments, we use mean squared error for the main reconstruction loss (`loss-recon`), and regularizers that encourage embeddings of unit length, and for primary colors to be on the plane of the first two dimensions. Regularizers can have different strengths depending on which sample they're evaluating. See _Labelling_ and _Train_ below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import linalg as LA\n",
    "\n",
    "from ex_color.data.color_cube import ColorCube\n",
    "from ex_color.data.cyclic import arange_cyclic\n",
    "\n",
    "\n",
    "def objective(fn):\n",
    "    \"\"\"Adapt loss function to look like a regularizer\"\"\"\n",
    "\n",
    "    def wrapper(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "        loss = fn(data, res.outputs)\n",
    "        # Reduce element-wise loss to per-sample loss by averaging over feature dimensions\n",
    "        if loss.ndim > 1:\n",
    "            # Calculate mean over all dimensions except the first (batch) dimension\n",
    "            reduce_dims = tuple(range(1, loss.ndim))\n",
    "            loss = torch.mean(loss, dim=reduce_dims)\n",
    "        return loss\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def unitarity(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "    \"\"\"Regularize latents to have unit norm (vectors of length 1)\"\"\"\n",
    "    norms = LA.vector_norm(res.latents, dim=-1)\n",
    "    # Return per-sample loss, shape [B]\n",
    "    return (norms - 1.0) ** 2\n",
    "\n",
    "\n",
    "def planarity(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "    \"\"\"Regularize latents to be planar in the first two channels (so zero in other channels)\"\"\"\n",
    "    if res.latents.shape[1] <= 2:\n",
    "        # No dimensions beyond the first two, return zero loss per sample\n",
    "        return torch.zeros(res.latents.shape[0], device=res.latents.device)\n",
    "    # Sum squares across the extra dimensions for each sample, shape [B]\n",
    "    return torch.sum(res.latents[:, 2:] ** 2, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pin (anchor)\n",
    "\n",
    "This regularizer causes certain samples to be attracted to a certain anchor point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pin(LossCriterion):\n",
    "    def __init__(self, anchor_point: Tensor):\n",
    "        self.anchor_point = anchor_point\n",
    "\n",
    "    def __call__(self, data: Tensor, res: InferenceResult) -> Tensor:\n",
    "        \"\"\"\n",
    "        Regularize latents to be close to the anchor point.\n",
    "\n",
    "        Returns:\n",
    "            loss: Per-sample loss, shape [B].\n",
    "        \"\"\"\n",
    "        # Calculate squared distances to the anchor\n",
    "        anchor_point = self.anchor_point.to(res.latents.device)\n",
    "        sq_dists = torch.sum((res.latents - anchor_point) ** 2, dim=-1)  # [B]\n",
    "        return sq_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate\n",
    "\n",
    "The `unitarity` term encourages embeddings to have unit length, but by default, that causes them to bunch up on one side of the hypersphere. To counter that, `Separate` adds a repulsive force along the surface of the hypersphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import EllipsisType\n",
    "\n",
    "\n",
    "class Separate(LossCriterion):\n",
    "    \"\"\"Regularize latents to be rotationally separated from each other.\"\"\"\n",
    "\n",
    "    def __init__(self, channels: tuple[int, ...] | EllipsisType = ..., power: float = 1.0, shift: bool = True):\n",
    "        self.channels = channels\n",
    "        self.power = power\n",
    "        self.shift = shift\n",
    "\n",
    "    def __call__(self, data: Tensor, res: InferenceResult) -> Tensor:\n",
    "        embeddings = res.latents[:, self.channels]  # [B, C]\n",
    "\n",
    "        # Normalize to unit hypersphere, so it's only the angular distance that matters\n",
    "        embeddings = embeddings / (torch.norm(embeddings, dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "        # Find the angular distance as cosine similarity\n",
    "        cos_sim = torch.matmul(embeddings, embeddings.T)  # [B, B]\n",
    "\n",
    "        # Nullify self-repulsion.\n",
    "        # We can't use torch.eye, because some points in the batch may be duplicates due to the use of random sampling with replacement.\n",
    "        cos_sim[torch.isclose(cos_sim, torch.ones_like(cos_sim))] = 0.0\n",
    "        if self.shift:\n",
    "            # Shift the cosine similarity to be in the range [0, 1]\n",
    "            cos_sim = (cos_sim + 1.0) / 2.0\n",
    "\n",
    "        # Sum over all other points\n",
    "        return torch.sum(cos_sim**self.power, dim=-1)  # [B]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading, sampling, and event handling\n",
    "\n",
    "Here we set up:\n",
    "\n",
    "- **Datasets:** Define the datasets used (primary/secondary colors, full color grid).\n",
    "- **Sampler:** Use `DynamicWeightedRandomBatchSampler` for the full dataset. Its weights are updated by the `update_sampler_weights` callback, which responds to the `data-fraction` parameter from the dopesheet. This smoothly shifts the sampling focus from highly vibrant colors early on to the full range of colors later.\n",
    "- **Recorders:** `ModelRecorder` and `MetricsRecorder` are event handlers that save the model state and loss values at each step.\n",
    "- **Event bindings:** Connect event handlers to specific events (e.g., `plotter` to `phase-end`, `reg_anchor.on_anchor` to `action:anchor`, recorders to `pre-step` and `step-metrics`).\n",
    "- **Training execution:** Finally, call `train_color_model` with the model, datasets, dopesheet, loss criteria, and configured event handlers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ModelRecorder(EventHandler):\n",
    "    \"\"\"Event handler to record model parameters.\"\"\"\n",
    "\n",
    "    history: list[tuple[int, dict[str, Tensor]]]\n",
    "    \"\"\"A list of tuples (step, state_dict) where state_dict is a copy of the model's state dict.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def __call__(self, event: Event):\n",
    "        # Get a *copy* of the state dict and move it to the CPU\n",
    "        # so we don't hold onto GPU memory or track gradients unnecessarily.\n",
    "        model_state = {k: v.cpu().clone() for k, v in event.model.state_dict().items()}\n",
    "        self.history.append((event.step, model_state))\n",
    "        log.debug(f'Recorded model state at step {event.step}')\n",
    "\n",
    "\n",
    "class BatchRecorder(EventHandler):\n",
    "    \"\"\"Event handler to record the exact batches used at each step.\"\"\"\n",
    "\n",
    "    history: list[tuple[int, Tensor]]\n",
    "    \"\"\"A list of tuples (step, train_batch).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def __call__(self, event: StepMetricsEvent):\n",
    "        if not isinstance(event, StepMetricsEvent):\n",
    "            log.warning(f'BatchRecorder received unexpected event type: {type(event)}')\n",
    "            return\n",
    "        self.history.append((event.step, event.train_batch.cpu().clone()))\n",
    "        log.debug(f'Recorded batch at step {event.step}')\n",
    "\n",
    "\n",
    "class MetricsRecorder(EventHandler):\n",
    "    \"\"\"Event handler to record training metrics.\"\"\"\n",
    "\n",
    "    history: list[tuple[int, float, dict[str, float]]]\n",
    "    \"\"\"A list of tuples (step, total_loss, losses_dict).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def __call__(self, event: StepMetricsEvent):\n",
    "        if not isinstance(event, StepMetricsEvent):\n",
    "            log.warning(f'MetricsRecorder received unexpected event type: {type(event)}')\n",
    "            return\n",
    "\n",
    "        self.history.append((event.step, event.total_loss, event.losses.copy()))\n",
    "        log.debug(f'Recorded metrics at step {event.step}: loss={event.total_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling\n",
    "\n",
    "Labelling remains the same.\n",
    "\n",
    "The training dataset has a fixed and somewhat small size. We want to simulate noisy labels — e.g. RGB 1,0,0 should not always be labelled \"red\", and colors close to red should also sometimes attract that label.\n",
    "\n",
    "Here we define a collation function for use with DataLoader. It takes a batch and produces labels for the samples on the fly, which means the model may see identical samples with different labels during training.\n",
    "\n",
    "Labels are assigned based on proximity to certain colors. The raw distance is not used; instead it is raised to a power to sharpen the association, and weaken the label for colors that are futher away. Initially the labels are smooth $(0..1)$. They are then converted to binary $\\lbrace 0,1 \\rbrace$ by comparison to random numbers.\n",
    "\n",
    "The label \"red\" (i.e. proximity to pure red) is calculated as:\n",
    "\n",
    "$$\\text{red} = \\left(r - \\frac{rg}{2} - \\frac{rb}{2}\\right) ^{10}$$\n",
    "\n",
    "While \"vibrant\" (proximity to any pure hue) is:\n",
    "\n",
    "$$\\text{vibrant} = \\left(s \\times v\\right)^{100}$$\n",
    "\n",
    "Where $r$, $g$, and $b$ are the red, green, and blue channels, and $s$ and $v$ are the saturation and value — all of which are real numbers between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "\n",
    "def generate_color_labels(data: Tensor, vibrancies: Tensor) -> dict[str, Tensor]:\n",
    "    \"\"\"\n",
    "    Generate label probabilities based on RGB values.\n",
    "\n",
    "    Args:\n",
    "        data: Batch of RGB values [B, 3]\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping label names to probabilities str -> [B]\n",
    "    \"\"\"\n",
    "    labels: dict[str, Tensor] = {}\n",
    "\n",
    "    # Labels are assigned based on proximity to certain colors.\n",
    "    # Distance is raised to a power to sharpen the association (i.e. weaken the label for colors that are futher away).\n",
    "\n",
    "    # Proximity to primary colors\n",
    "    r, g, b = data[:, 0], data[:, 1], data[:, 2]\n",
    "    labels['red'] = (r * (1 - g / 2 - b / 2)) ** 10\n",
    "    # labels['green'] = g * (1 - r / 2 - b / 2)\n",
    "    # labels['blue'] = b * (1 - r / 2 - g / 2)\n",
    "\n",
    "    # Proximity to any fully-saturated, fully-bright color\n",
    "    labels['vibrant'] = vibrancies**100\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def collate_with_generated_labels(\n",
    "    batch,\n",
    "    *,\n",
    "    soft: bool = True,\n",
    "    scale: dict[str, float] | None = None,\n",
    ") -> tuple[Tensor, dict[str, Tensor]]:\n",
    "    \"\"\"\n",
    "    Custom collate function that generates labels for the samples.\n",
    "\n",
    "    Args:\n",
    "        batch: A list of ((data_tensor,), index_tensor) tuples from TensorDataset.\n",
    "               Note: TensorDataset wraps single tensors in a tuple.\n",
    "        soft: If True, return soft labels (0..1). Otherwise, return hard labels (0 or 1).\n",
    "        scale: Linear scaling factors for the labels (applied before discretizing).\n",
    "\n",
    "    Returns:\n",
    "        A tuple: (collated_data_tensor, collated_labels_tensor)\n",
    "    \"\"\"\n",
    "    # Separate data and indices\n",
    "    # TensorDataset yields tuples like ((data_point_tensor,), index_scalar_tensor)\n",
    "    data_tuple_list = [item[0] for item in batch]  # List of (data_tensor,) tuples\n",
    "    vibrancies = [item[1] for item in batch]\n",
    "\n",
    "    # Collate the data points using the default collate function\n",
    "    # default_collate handles the list of (data_tensor,) tuples correctly\n",
    "    collated_data = default_collate(data_tuple_list)\n",
    "    vibrancies = default_collate(vibrancies)\n",
    "    label_probs = generate_color_labels(collated_data, vibrancies)\n",
    "    for k, v in (scale or {}).items():\n",
    "        label_probs[k] = label_probs[k] * v\n",
    "\n",
    "    if soft:\n",
    "        # Return the probabilities directly\n",
    "        return collated_data, label_probs\n",
    "    else:\n",
    "        # Sample labels stochastically\n",
    "        labels = {k: discretize(v) for k, v in label_probs.items()}\n",
    "        return collated_data, labels\n",
    "\n",
    "\n",
    "def discretize(probs: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Discretize probabilities into binary labels.\n",
    "\n",
    "    Args:\n",
    "        probs: Tensor of probabilities [B]\n",
    "\n",
    "    Returns:\n",
    "        Tensor of binary labels [B]\n",
    "    \"\"\"\n",
    "    # Sample from a uniform distribution\n",
    "    rand = torch.rand_like(probs)\n",
    "    return (rand < probs).float()  # Convert to float for compatibility with loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "Like Ex 1.7, we'll train on the HSV cube (with RGB values) and validate with the RGB cube. We train on the whole HSV cube right from the start, with a sampling bias to prevent too much importance being given to dark and desaturated colors (which are over-represented in the training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from ex_color.data.cube_sampler import vibrancy\n",
    "\n",
    "\n",
    "def prep_data():\n",
    "    hsv_cube = ColorCube.from_hsv(\n",
    "        h=arange_cyclic(step_size=10 / 360),\n",
    "        s=np.linspace(0, 1, 10),\n",
    "        v=np.linspace(0, 1, 10),\n",
    "    )\n",
    "    hsv_tensor = torch.tensor(hsv_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "    vibrancy_tensor = torch.tensor(vibrancy(hsv_cube).flatten(), dtype=torch.float32)\n",
    "    hsv_dataset = TensorDataset(hsv_tensor, vibrancy_tensor)\n",
    "\n",
    "    labeller = partial(\n",
    "        collate_with_generated_labels,\n",
    "        soft=False,  # Use binary labels (stochastic) to simulate the labelling of internet text\n",
    "        scale={'red': 0.5, 'vibrant': 0.5},\n",
    "    )\n",
    "    # Desaturated and dark colors are over-represented in the cube, so we use a weighted sampler to balance them out\n",
    "    hsv_loader = DataLoader(\n",
    "        hsv_dataset,\n",
    "        batch_size=64,\n",
    "        sampler=WeightedRandomSampler(\n",
    "            weights=hsv_cube.bias.flatten().tolist(),\n",
    "            num_samples=len(hsv_dataset),\n",
    "            replacement=True,\n",
    "        ),\n",
    "        collate_fn=labeller,\n",
    "    )\n",
    "\n",
    "    rgb_cube = ColorCube.from_rgb(\n",
    "        r=np.linspace(0, 1, 8),\n",
    "        g=np.linspace(0, 1, 8),\n",
    "        b=np.linspace(0, 1, 8),\n",
    "    )\n",
    "    rgb_tensor = torch.tensor(rgb_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "    return hsv_loader, rgb_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "#### Regularizer configuration\n",
    "\n",
    "The training process can use several regularizers, each designed to impose specific structural properties on the latent space. In this experiment, we will test various combinations of these regularizers to understand their individual and combined effects.\n",
    "\n",
    "These are the available regularizers:\n",
    "\n",
    "*   **`reg-polar`**: Concept anchor for the color _red_.\n",
    "    *   **Criterion**: `Pin` to the latent coordinate `(1, 0, 0, 0)`. This encourages specific embeddings to move towards this \"polar\" anchor point.\n",
    "    *   **Label affinity**: `{'red': 1.0}`. This regularizer is fully active (strength 1.0) for samples that are labeled as 'red'. Its goal is to anchor the concept of \"red\" to a specific location in the latent space.\n",
    "\n",
    "*   **`reg-separate`**: Encourages the full space to be used.\n",
    "    *   **Criterion**: `Separate` (with `power=10.0`, `shift=False`). This term encourages all embeddings in a batch to be rotationally distinct from each other. It calculates the cosine similarity between embeddings and applies a repulsive force, powered up to make the effect greater for pairs of points that are very close. The `shift=False` means it uses the raw cosine similarity (ranging from -1 to 1).\n",
    "    *   **Label affinity**: `None`. This regularizer applies equally to all samples in the batch, irrespective of their labels, aiming for a general separation of all learned representations.\n",
    "\n",
    "*   **`reg-planar`**: Defines a \"hue\" plane, onto which the color wheel should emerge.\n",
    "    *   **Criterion**: `planarity`. This penalizes embeddings for having non-zero values in latent dimensions beyond the first two (i.e., dimensions 2 and 3, given a 4D latent space). It encourages these embeddings to lie on the plane defined by the first two latent dimensions.\n",
    "    *   **Label affinity**: `{'vibrant': 1.0}`. This regularizer is fully active for samples labeled as 'vibrant'. The idea is to map vibrant, pure hues primarily onto a 2D manifold within the latent space, potentially representing a color wheel.\n",
    "\n",
    "*   **`reg-norm-v`**: Encourages a circular color wheel, rather than hexagonal.\n",
    "    *   **Criterion**: `unitarity`. This encourages the latent embeddings to have a norm (length) of 1, pushing them towards the surface of a hypersphere.\n",
    "    *   **Label affinity**: `{'vibrant': 1.0}`. This normalization is specifically applied with full strength to samples labeled as 'vibrant'. This works in conjunction with `reg-planar` to organize vibrant colors on a 2D spherical surface.\n",
    "\n",
    "*   **`reg-norm`**: Encourages embeddings to lie on the surface of a hypersphere, so they can be compared with cosine distance alone.\n",
    "    *   **Criterion**: `unitarity`. This also encourages latent embeddings to have a unit norm.\n",
    "    *   **Label affinity**: `None`. This regularizer applies the unit norm constraint to *all* samples in the batch, regardless of their labels. This ensures that even non-vibrant colors are normalized, contributing to a more uniform distribution of embeddings on the hypersphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_REGULARIZERS = [\n",
    "    RegularizerConfig(\n",
    "        name='reg-polar',\n",
    "        criterion=Pin(torch.tensor([1, 0, 0, 0], dtype=torch.float32)),\n",
    "        label_affinities={'red': 1.0},\n",
    "    ),\n",
    "    RegularizerConfig(\n",
    "        name='reg-separate',\n",
    "        criterion=Separate(power=10.0, shift=False),\n",
    "        label_affinities=None,\n",
    "    ),\n",
    "    RegularizerConfig(\n",
    "        name='reg-planar',\n",
    "        criterion=planarity,\n",
    "        label_affinities={'vibrant': 1.0},\n",
    "    ),\n",
    "    RegularizerConfig(\n",
    "        name='reg-norm-v',\n",
    "        criterion=unitarity,\n",
    "        label_affinities={'vibrant': 1.0},\n",
    "    ),\n",
    "    RegularizerConfig(\n",
    "        name='reg-norm',\n",
    "        criterion=unitarity,\n",
    "        label_affinities=None,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "@run.thither(max_containers=16)\n",
    "async def train(\n",
    "    dopesheet: Dopesheet,\n",
    "    regularizers: list[RegularizerConfig],\n",
    "    variant_name: str,\n",
    "    store_latents: Callable[[LatentEvaluation], None],\n",
    "):\n",
    "    \"\"\"Train the model with the given dopesheet and variant.\"\"\"\n",
    "    log.info(f'Training with: {[r.name for r in regularizers]}')\n",
    "    # recorder = ModelRecorder()\n",
    "    metrics_recorder = MetricsRecorder()\n",
    "    # batch_recorder = BatchRecorder()\n",
    "\n",
    "    seed = 0\n",
    "    set_deterministic_mode(seed)\n",
    "\n",
    "    hsv_loader, rgb_tensor = prep_data()\n",
    "    model = ColorMLP()\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    log.debug(f'Model initialized with {total_params:,} trainable parameters.')\n",
    "\n",
    "    event_handlers = EventHandlers()\n",
    "    # event_handlers.pre_step.add_handler('pre-step', recorder)\n",
    "    event_handlers.step_metrics.add_handler('step-metrics', metrics_recorder)\n",
    "    # event_handlers.step_metrics.add_handler('step-metrics', batch_recorder)\n",
    "\n",
    "    # plotter = PhasePlotter(rgb_tensor, dim_pairs=[(1, 0), (1, 2), (1, 3)], variant_name=variant_name)\n",
    "    event_handlers.pre_step.add_handler(\n",
    "        'pre-step',\n",
    "        periodic(\n",
    "            evaluate_latents(rgb_tensor, variant_name, store_latents),\n",
    "            interval=200,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    train_color_model(\n",
    "        model,\n",
    "        hsv_loader,\n",
    "        rgb_tensor,\n",
    "        dopesheet,\n",
    "        # loss_criterion=objective(nn.MSELoss(reduction='none')),  # No reduction; allows per-sample loss weights\n",
    "        loss_criterion=objective(nn.MSELoss()),\n",
    "        regularizers=regularizers,\n",
    "        event_handlers=event_handlers,\n",
    "    )\n",
    "\n",
    "    return metrics_recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 5.8 no.1.8:  Running 31/31 combinations of 5 regularizers.\n",
      "I 0.1 no.1.8:  Training with: ['reg-polar']\n",
      "I 0.1 no.1.8:  Training with: ['reg-separate']\n",
      "I 0.0 no.1.8:  Training with: ['reg-planar']\n",
      "I 0.0 no.1.8:  Training with: ['reg-norm-v']\n",
      "I 0.0 no.1.8:  Training with: ['reg-norm']\n",
      "I 0.0 no.1.8:  Training with: ['reg-polar', 'reg-separate']\n",
      "I 0.0 no.1.8:  Training with: ['reg-polar', 'reg-planar']\n",
      "I 0.0 no.1.8:  Training with: ['reg-polar', 'reg-norm-v']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_01.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_02.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_03.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_04.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_05.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_06.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_07.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_08.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 24.0 no.1.8: Training with: ['reg-polar', 'reg-norm']\n",
      "I 24.4 no.1.8: Training with: ['reg-separate', 'reg-planar']\n",
      "I 25.0 no.1.8: Training with: ['reg-separate', 'reg-norm-v']\n",
      "I 29.8 no.1.8: Training with: ['reg-separate', 'reg-norm']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_09.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_10.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_11.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_12.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 37.8 no.1.8: Training with: ['reg-planar', 'reg-norm-v']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_13.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 43.5 no.1.8: Training with: ['reg-planar', 'reg-norm']\n",
      "I 44.8 no.1.8: Training with: ['reg-norm-v', 'reg-norm']\n",
      "I 45.7 no.1.8: Training with: ['reg-polar', 'reg-separate', 'reg-planar']\n",
      "I 51.3 no.1.8: Training with: ['reg-polar', 'reg-separate', 'reg-norm-v']\n",
      "I 51.0 no.1.8: Training with: ['reg-polar', 'reg-separate', 'reg-norm']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_14.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_15.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_16.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_17.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 59.6 no.1.8: Training with: ['reg-polar', 'reg-planar', 'reg-norm-v']\n",
      "I 70.4 no.1.8: Training with: ['reg-polar', 'reg-planar', 'reg-norm']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_18.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_19.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 71.4 no.1.8: Training with: ['reg-polar', 'reg-norm-v', 'reg-norm']\n",
      "I 72.7 no.1.8: Training with: ['reg-separate', 'reg-planar', 'reg-norm-v']\n",
      "I 75.6 no.1.8: Training with: ['reg-separate', 'reg-planar', 'reg-norm']\n",
      "I 78.0 no.1.8: Training with: ['reg-separate', 'reg-norm-v', 'reg-norm']\n",
      "I 78.8 no.1.8: Training with: ['reg-planar', 'reg-norm-v', 'reg-norm']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_20.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_21.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_22.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 80.8 no.1.8: Training with: ['reg-polar', 'reg-separate', 'reg-planar', 'reg-norm-v']\n",
      "I 95.2 no.1.8: Training with: ['reg-polar', 'reg-separate', 'reg-planar', 'reg-norm']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_23.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_24.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_25.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_26.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 104.0 no.1.8:Training with: ['reg-polar', 'reg-separate', 'reg-norm-v', 'reg-norm']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_27.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_28.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 106.1 no.1.8:Training with: ['reg-polar', 'reg-planar', 'reg-norm-v', 'reg-norm']\n",
      "I 106.3 no.1.8:Training with: ['reg-separate', 'reg-planar', 'reg-norm-v', 'reg-norm']\n",
      "I 114.4 no.1.8:Training with: ['reg-polar', 'reg-separate', 'reg-planar', 'reg-norm-v', 'reg-norm']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_29.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_30.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.8-regularizer-combinations_31.png\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "from asyncio import Task, TaskGroup\n",
    "\n",
    "all_regs = ALL_REGULARIZERS\n",
    "all_combinations = list(itertools.chain(*(itertools.combinations(all_regs, i) for i in range(1, len(all_regs) + 1))))\n",
    "\n",
    "combinations = all_combinations[:]  # For testing, select a subset\n",
    "log.info(f'Running {len(combinations):d}/{len(all_combinations):d} combinations of {len(all_regs)} regularizers.')\n",
    "\n",
    "_, rgb_tensor = prep_data()\n",
    "colors = rgb_tensor.cpu().numpy()\n",
    "history_store = HistoryStore()\n",
    "\n",
    "tasks: dict[str, Task[MetricsRecorder]] = {}\n",
    "async with run(shutdown_timeout=300), store_latents(history_store) as _store_latents, TaskGroup() as tg:\n",
    "    for combo in combinations:\n",
    "        dopesheet = load_dopesheet()\n",
    "        combo_list = list(combo)\n",
    "        combo_name = ' + '.join(r.name for r in combo_list)\n",
    "        plotter = history_plotter(colors=colors, dim_pairs=[(1, 0), (1, 2), (1, 3)], variant_name=combo_name)\n",
    "        history_store.register_observer(variant=combo_name, observer=plotter)\n",
    "        task = tg.create_task(train(dopesheet, combo_list, variant_name=combo_name, store_latents=_store_latents))\n",
    "        tasks[combo_name] = task\n",
    "\n",
    "results: dict[str, MetricsRecorder] = {k: t.result() for k, t in tasks.items()}\n",
    "\n",
    "# metrics = results[list(results.keys())[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Each regularizer appears to do what we expected.\n",
    "- Without `reg-separate`, the points all bunch up on one side of the sphere (due to `reg-norm`)\n",
    "- Without `reg-norm`, the points are bunched up in the middle of the sphere\n",
    "- Without `reg-planar`, the hues are represented in dimensions other than the first two\n",
    "- Without `reg-anchor`, _red_ isn't situated at $1,0,0,0$\n",
    "\n",
    "It's unclear whether both `reg-norm` and `reg-norm-v` are needed (they both apply the unitarity constraint, but `-v` only applies it to the saturated hues). One interesting thing is that when only one of them is used, the resulting structure still has radius $~1$ but is more cube-like. Our understanding is that in nGPT, downstream transformer layers need unit-length vectors to do good Q-K lookups (so not required in this simple MLP architecture).\n",
    "\n",
    "In nGPT, the activations are explicitly normalized to have unit length, instead of using a unitarity regularization term. We chose to use regularization because we feel that normalization probably hides too much of the goal from upstream layers. Consider: if you only normalize, then the \"true\" representations might be bunched in the middle of the hypersphere, arbitrarily close to the origin. If a point was too close to the origin, small perturbations could cause it to flip to another side of the sphere. So we expect that the model will perform better on its primary objective if the upstream representations are as close as possible to what the downstream layers expect.\n",
    "\n",
    "The network probably needs extra capacity to form the sphere. A good trade-off might be to use very a weak unitarity term, so that it is regularized toward a radius one hypercube, and then explicitly normalized to a unit hypersphere."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex-color-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
