{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1.5: Smooth curriculum and anchoring\n",
    "\n",
    "This experiment combines the insights and tools from our previous work:\n",
    "- **3D Bottleneck & Curriculum:** Like [Experiment 1.3](ex-1.3-color-mlp-curriculum.ipynb), we use a low-dimensional latent space and a curriculum to encourage the model to learn hue first.\n",
    "- **Smooth Transitions:** We replace the abrupt phase changes of Ex 1.3 with the smooth parameter transitions developed in [Experiment 1.4](ex-1.4-parameter-transitions.ipynb), using the `SmoothProp` mechanism driven by a [dopesheet](ex-1.5-dopesheet.csv).\n",
    "- **Anchoring:** We introduce an \"anchor\" regularization term, also controlled via the dopesheet, to fix the positions of key colors (primaries/secondaries) after the initial phase, preventing later phases from disrupting the learned hue structure.\n",
    "\n",
    "This time we use four dimensions: two for hue, and one each for value and saturation. This is actually more than is strictly needed: consider that both HSV and RGB only use three dimensions! But they use a dense cube, whereas our latent space will be the surface of a hypersphere.\n",
    "\n",
    "We hope to achieve a stable, well-structured latent space where hue forms a planar color wheel, while value and saturation extend into the dimension remaining dimensions. We expect this approach to be less sensitive to the initial conditions and exact timing and weighting of curriculum phases compared to the discrete steps in Ex 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from utils.logging import SimpleLoggingConfig\n",
    "\n",
    "logging_config = SimpleLoggingConfig().info('notebook', 'utils', 'mini', 'ex_color')\n",
    "logging_config.apply()\n",
    "\n",
    "# This is the logger for this notebook\n",
    "log = logging.getLogger('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "We use the same simple 2-layer MLP autoencoder with a bottleneck as in previous experiments. The key difference lies not in the architecture, but in the training process governed by the smooth curriculum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "E = 4\n",
    "\n",
    "\n",
    "class ColorMLP(nn.Module):\n",
    "    def __init__(self, normalize_bottleneck=False):\n",
    "        super().__init__()\n",
    "        # RGB input (3D) → hidden layer → bottleneck → hidden layer → RGB output\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.GELU(),\n",
    "            # nn.Linear(16, 16),\n",
    "            # nn.GELU(),\n",
    "            nn.Linear(16, E),  # Our critical bottleneck!\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(E, 16),\n",
    "            nn.GELU(),\n",
    "            # nn.Linear(16, 16),\n",
    "            # nn.GELU(),\n",
    "            nn.Linear(16, 3),\n",
    "            nn.Sigmoid(),  # Keep RGB values in [0,1]\n",
    "        )\n",
    "\n",
    "        self.normalize = normalize_bottleneck\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Get our bottleneck representation\n",
    "        bottleneck = self.encoder(x)\n",
    "\n",
    "        # Optionally normalize to unit vectors (like nGPT)\n",
    "        if self.normalize:\n",
    "            norm = torch.norm(bottleneck, dim=1, keepdim=True)\n",
    "            bottleneck = bottleneck / (norm + 1e-8)  # Avoid division by zero\n",
    "\n",
    "        # Decode back to RGB\n",
    "        output = self.decoder(bottleneck)\n",
    "        return output, bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training machinery with timeline and events\n",
    "\n",
    "The `train_color_model` function orchestrates the training process based on a `Timeline` derived from the dopesheet. It handles:\n",
    "- Iterating through training steps.\n",
    "- Fetching the correct data loader for the current phase.\n",
    "- Updating hyperparameters (like learning rate and loss weights) smoothly based on the timeline state.\n",
    "- Calculating the combined loss from reconstruction and various regularizers.\n",
    "- Executing the optimizer step.\n",
    "- Emitting events at different points (phase start/end, pre-step, actions like 'anchor', step metrics) to trigger callbacks like plotting, recording, or updating loss terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Protocol, runtime_checkable\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "from mini.temporal.model import TStep\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InferenceResult:\n",
    "    outputs: Tensor\n",
    "    latents: Tensor\n",
    "\n",
    "    def detach(self):\n",
    "        return InferenceResult(self.outputs.detach(), self.latents.detach())\n",
    "\n",
    "    def clone(self):\n",
    "        return InferenceResult(self.outputs.clone(), self.latents.clone())\n",
    "\n",
    "    def cpu(self):\n",
    "        return InferenceResult(self.outputs.cpu(), self.latents.cpu())\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class LossCriterion(Protocol):\n",
    "    def __call__(self, data: Tensor, res: InferenceResult) -> Tensor: ...\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class SpecialLossCriterion(LossCriterion, Protocol):\n",
    "    def forward(self, model: ColorMLP, data: Tensor) -> InferenceResult | None: ...\n",
    "\n",
    "\n",
    "@dataclass(eq=False, frozen=True)\n",
    "class Event:\n",
    "    name: str\n",
    "    step: int\n",
    "    model: ColorMLP\n",
    "    timeline_state: TStep\n",
    "    optimizer: optim.Optimizer\n",
    "\n",
    "\n",
    "@dataclass(eq=False, frozen=True)\n",
    "class PhaseEndEvent(Event):\n",
    "    validation_data: Tensor\n",
    "    inference_result: InferenceResult\n",
    "\n",
    "\n",
    "@dataclass(eq=False, frozen=True)\n",
    "class StepMetricsEvent(Event):\n",
    "    \"\"\"Event carrying metrics calculated during a training step.\"\"\"\n",
    "\n",
    "    total_loss: float\n",
    "    losses: dict[str, float]\n",
    "\n",
    "\n",
    "class EventHandler[T](Protocol):\n",
    "    def __call__(self, event: T) -> None: ...\n",
    "\n",
    "\n",
    "class EventBinding[T]:\n",
    "    \"\"\"A class to bind events to handlers.\"\"\"\n",
    "\n",
    "    def __init__(self, event_name: str):\n",
    "        self.event_name = event_name\n",
    "        self.handlers: list[tuple[str, EventHandler[T]]] = []\n",
    "\n",
    "    def add_handler(self, event_name: str, handler: EventHandler[T]) -> None:\n",
    "        self.handlers.append((event_name, handler))\n",
    "\n",
    "    def emit(self, event_name: str, event: T) -> None:\n",
    "        for name, handler in self.handlers:\n",
    "            if name == event_name:\n",
    "                handler(event)\n",
    "\n",
    "\n",
    "class EventHandlers:\n",
    "    \"\"\"A simple event system to allow for custom callbacks.\"\"\"\n",
    "\n",
    "    phase_start: EventBinding[Event]\n",
    "    pre_step: EventBinding[Event]\n",
    "    action: EventBinding[Event]\n",
    "    phase_end: EventBinding[PhaseEndEvent]\n",
    "    step_metrics: EventBinding[StepMetricsEvent]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.phase_start = EventBinding[Event]('phase-start')\n",
    "        self.pre_step = EventBinding[Event]('pre-step')\n",
    "        self.action = EventBinding[Event]('action')\n",
    "        self.phase_end = EventBinding[PhaseEndEvent]('phase-end')\n",
    "        self.step_metrics = EventBinding[StepMetricsEvent]('step-metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from mini.temporal.dopesheet import Dopesheet\n",
    "from mini.temporal.timeline import Timeline\n",
    "from utils.progress import co_op, AsyncProgress\n",
    "\n",
    "\n",
    "def reiterate[T](it: Iterable[T]) -> Iterator[T]:\n",
    "    \"\"\"\n",
    "    Iterates over an iterable indefinitely.\n",
    "\n",
    "    When the iterable is exhausted, it starts over from the beginning. Unlike\n",
    "    `itertools.cycle`, yielded values are not cached — so each iteration may be\n",
    "    different.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        yield from it\n",
    "\n",
    "\n",
    "async def train_color_model(  # noqa: C901\n",
    "    model: ColorMLP,\n",
    "    datasets: dict[str, tuple[DataLoader, Tensor]],\n",
    "    dopesheet: Dopesheet,\n",
    "    loss_criteria: dict[str, LossCriterion | SpecialLossCriterion],\n",
    "    event_handlers: EventHandlers | None = None,\n",
    "):\n",
    "    if event_handlers is None:\n",
    "        event_handlers = EventHandlers()\n",
    "\n",
    "    # --- Validate inputs ---\n",
    "    # Check if all phases in dopesheet have corresponding data\n",
    "    dopesheet_phases = dopesheet.phases\n",
    "    missing_data = dopesheet_phases - set(datasets.keys())\n",
    "    if missing_data:\n",
    "        raise ValueError(f'Missing data for dopesheet phases: {missing_data}')\n",
    "\n",
    "    # Check if 'lr' is defined in the dopesheet properties\n",
    "    if 'lr' not in dopesheet.props:\n",
    "        raise ValueError(\"Dopesheet must define the 'lr' property column.\")\n",
    "    # --- End Validation ---\n",
    "\n",
    "    timeline = Timeline(dopesheet)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    data_iterators = {\n",
    "        phase_name: iter(reiterate(dataloader))  #\n",
    "        for phase_name, (dataloader, _) in datasets.items()\n",
    "    }\n",
    "\n",
    "    total_steps = len(timeline)\n",
    "\n",
    "    async with AsyncProgress(total=total_steps, description='Training Steps') as pbar:\n",
    "        async for step in co_op(pbar):\n",
    "            # Get state *before* advancing timeline for this step's processing\n",
    "            current_state = timeline.state\n",
    "            current_phase_name = current_state.phase\n",
    "\n",
    "            # Assuming TensorDataset yields a tuple with one element\n",
    "            (batch,) = next(data_iterators[current_phase_name])\n",
    "\n",
    "            # --- Event Handling ---\n",
    "            event_template = {\n",
    "                'step': step,\n",
    "                'model': model,\n",
    "                'timeline_state': current_state,\n",
    "                'optimizer': optimizer,\n",
    "            }\n",
    "\n",
    "            if current_state.is_phase_start:\n",
    "                event = Event(name=f'phase-start:{current_phase_name}', **event_template)\n",
    "                event_handlers.phase_start.emit(event.name, event)\n",
    "                event_handlers.phase_start.emit('phase-start', event)\n",
    "\n",
    "            for action in current_state.actions:\n",
    "                event = Event(name=f'action:{action}', **event_template)\n",
    "                event_handlers.action.emit(event.name, event)\n",
    "                event_handlers.action.emit('action', event)\n",
    "\n",
    "            event = Event(name='pre-step', **event_template)\n",
    "            event_handlers.pre_step.emit('pre-step', event)\n",
    "\n",
    "            # --- Training Step ---\n",
    "            # ... (get data, update LR, zero grad, forward pass, calculate loss, backward, step) ...\n",
    "\n",
    "            current_lr = current_state.props['lr']\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs, latents = model(batch.to(device))\n",
    "            current_results = InferenceResult(outputs, latents)\n",
    "\n",
    "            total_loss = torch.tensor(0.0, device=device)\n",
    "            losses_dict: dict[str, float] = {}\n",
    "            for name, criterion in loss_criteria.items():\n",
    "                weight = current_state.props.get(name, 0.0)\n",
    "                if weight == 0:\n",
    "                    continue\n",
    "\n",
    "                if isinstance(criterion, SpecialLossCriterion):\n",
    "                    # Special criteria might run on their own data (like Anchor)\n",
    "                    # or potentially use the current batch (depends on implementation).\n",
    "                    # The forward method gets the model and the *current batch*\n",
    "                    special_results = criterion.forward(model, batch)\n",
    "                    if special_results is None:\n",
    "                        continue\n",
    "                    term_loss = criterion(batch, special_results)\n",
    "                else:\n",
    "                    term_loss = criterion(batch, current_results)\n",
    "\n",
    "                total_loss += term_loss * weight\n",
    "                losses_dict[name] = term_loss.item()\n",
    "\n",
    "            if total_loss > 0:\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "            # --- End Training Step ---\n",
    "\n",
    "            # Emit step metrics event\n",
    "            step_metrics_event = StepMetricsEvent(\n",
    "                name='step-metrics',\n",
    "                **event_template,\n",
    "                total_loss=total_loss.item(),\n",
    "                losses=losses_dict,\n",
    "            )\n",
    "            event_handlers.step_metrics.emit('step-metrics', step_metrics_event)\n",
    "\n",
    "            # --- Post-Step Event Handling ---\n",
    "            if current_state.is_phase_end:\n",
    "                # Trigger phase-end for the *current* phase\n",
    "                _, validation_data = datasets[current_phase_name]\n",
    "                with torch.no_grad():\n",
    "                    val_outputs, val_latents = model(validation_data.to(device))\n",
    "                event = PhaseEndEvent(\n",
    "                    name=f'phase-end:{current_phase_name}',\n",
    "                    **event_template,\n",
    "                    validation_data=validation_data,\n",
    "                    inference_result=InferenceResult(val_outputs, val_latents),\n",
    "                )\n",
    "                event_handlers.phase_end.emit(event.name, event)\n",
    "                event_handlers.phase_end.emit('phase-end', event)\n",
    "            # --- End Event Handling ---\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.metrics = {\n",
    "                'PHASE': current_phase_name,\n",
    "                'lr': f'{current_lr:.6f}',\n",
    "                'loss': f'{total_loss.item():.4f}',\n",
    "                **{name: f'{lt:.4f}' for name, lt in losses_dict.items()},\n",
    "            }\n",
    "\n",
    "            # Advance timeline *after* processing the current step\n",
    "            if step < total_steps:  # Avoid stepping past the end\n",
    "                timeline.step()\n",
    "\n",
    "    log.info('Training finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase plotting callback\n",
    "\n",
    "This `PhasePlotter` class acts as an event handler. It listens for `phase-end` events emitted by the training loop. When a phase ends, it captures the model's latent representations for the validation data of that phase and generates plots showing the state of the latent space (projected onto different 2D planes). This allows us to visualize how the structure evolves across the curriculum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from torch import Tensor\n",
    "from IPython.display import HTML\n",
    "\n",
    "from utils.nb import save_fig\n",
    "\n",
    "\n",
    "class PhasePlotter:\n",
    "    \"\"\"Event handler to plot latent space at the end of each phase.\"\"\"\n",
    "\n",
    "    def __init__(self, dim_pairs: list[tuple[int, int]] | None = None):\n",
    "        from utils.nb import displayer\n",
    "\n",
    "        # Store (phase_name, end_step, data, result) - data comes from event now\n",
    "        self.history: list[tuple[str, int, Tensor, InferenceResult]] = []\n",
    "        self.display = displayer()\n",
    "        self.dim_pairs = dim_pairs or [(0, 1), (0, 2)]\n",
    "\n",
    "    # Expect PhaseEndEvent specifically\n",
    "    def __call__(self, event: PhaseEndEvent):\n",
    "        \"\"\"Handle phase-end events.\"\"\"\n",
    "        if not isinstance(event, PhaseEndEvent):\n",
    "            raise TypeError(f'Expected PhaseEndEvent, got {type(event)}')\n",
    "\n",
    "        # TODO: Don't assume device = CPU\n",
    "        # TODO: Split this class so that the event handler is separate from the plotting, and so the plotting can happen locally with @run.hither\n",
    "        phase_name = event.timeline_state.phase\n",
    "        end_step = event.step\n",
    "        phase_dataset = event.validation_data\n",
    "        inference_result = event.inference_result\n",
    "\n",
    "        log.info(f'Plotting end of phase: {phase_name} at step {end_step} using provided results.')\n",
    "\n",
    "        # Append to history\n",
    "        self.history.append((phase_name, end_step, phase_dataset.cpu(), inference_result.cpu()))\n",
    "\n",
    "        # Plotting logic remains the same as it already expected CPU tensors\n",
    "        fig = self._plot_phase_history()\n",
    "        self.display(\n",
    "            HTML(\n",
    "                save_fig(\n",
    "                    fig,\n",
    "                    'large-assets/ex-1.5-color-phase-history.png',\n",
    "                    alt_text='Visualizations of latent space at the end of each curriculum phase.',\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _plot_phase_history(self):\n",
    "        num_phases = len(self.history)\n",
    "        plt.style.use('dark_background')\n",
    "        if num_phases == 0:\n",
    "            fig, ax = plt.subplots()\n",
    "            fig.set_facecolor('#333')\n",
    "            ax.set_facecolor('#222')\n",
    "            ax.text(0.5, 0.5, 'Waiting...', ha='center', va='center')\n",
    "            return fig\n",
    "\n",
    "        fig, axes = plt.subplots(\n",
    "            num_phases, len(self.dim_pairs), figsize=(5 * len(self.dim_pairs), 5 * num_phases), squeeze=False\n",
    "        )\n",
    "        fig.set_facecolor('#333')\n",
    "\n",
    "        for row_idx, (phase_name, end_step, data, res) in enumerate(self.history):\n",
    "            _latents = res.latents.numpy()\n",
    "            _colors = data.numpy()\n",
    "\n",
    "            for col_idx, (dim1, dim2) in enumerate(self.dim_pairs):\n",
    "                ax = axes[row_idx, col_idx]\n",
    "                ax.set_facecolor('#222')\n",
    "                ax.scatter(_latents[:, dim1], _latents[:, dim2], c=_colors, s=50, alpha=0.7)\n",
    "\n",
    "                # Set y-label differently for the first column\n",
    "                if col_idx == 0:\n",
    "                    ax.set_ylabel(\n",
    "                        f'Phase: {phase_name}\\n(End Step: {end_step})',\n",
    "                        fontsize='medium',\n",
    "                        rotation=90,  # Rotate vertically\n",
    "                        labelpad=15,  # Adjust padding\n",
    "                        verticalalignment='center',\n",
    "                        horizontalalignment='center',\n",
    "                    )\n",
    "                else:\n",
    "                    # Standard y-label for other columns\n",
    "                    ax.set_ylabel(f'Dim {dim2}')\n",
    "\n",
    "                # Set title only for the top row\n",
    "                if row_idx == 0:\n",
    "                    ax.set_title(f'Dims {dim1} vs {dim2}')\n",
    "\n",
    "                # Standard x-label for all columns\n",
    "                ax.set_xlabel(f'Dim {dim1}')\n",
    "\n",
    "                # Keep other plot settings\n",
    "                ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "                ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "                ax.add_patch(Circle((0, 0), 1, fill=False, linestyle='--', color='gray', alpha=0.3))\n",
    "                ax.set_aspect('equal')\n",
    "\n",
    "        fig.tight_layout()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth curriculum via dopesheet\n",
    "\n",
    "Instead of defining discrete phases with fixed parameters, we now use a [dopesheet (as CSV)](ex-1.5-dopesheet.csv) to define keyframes for our hyperparameters. The `Timeline` class interpolates these values smoothly between keyframes using the minimum jerk approach from Experiment 1.4.\n",
    "\n",
    "The dopesheet controls:\n",
    "- **Learning Rate (`lr`):** Gradually decreased over training.\n",
    "- **Reconstruction Loss Weight (`loss-recon`):** Kept constant.\n",
    "- **Regularization Weights (`reg-separate`, `reg-planar`, `reg-norm`, `reg-anchor`):** Faded in and out to guide the model. For example, `reg-separate` and `reg-planar` are strong early on to establish the color wheel, while `reg-anchor` activates later to lock it in place.\n",
    "- **Data Fraction (`data-fraction`):** Controls the `DynamicWeightedRandomBatchSampler` to smoothly transition the training data distribution from vibrant colors towards the full color space (details below).\n",
    "- **Actions (`ACTION`):** Triggers specific events, like the `anchor` action which tells the `Anchor` regularizer to capture the current latent positions of the primary/secondary colors.\n",
    "\n",
    "This allows for a more continuous and potentially more stable learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Parameter schedule\n",
       "|   STEP | PHASE               | ACTION   |      lr |   loss-recon |   reg-separate |   reg-planar |   reg-norm |   reg-anchor |   data-fraction |\n",
       "|-------:|:--------------------|:---------|--------:|-------------:|---------------:|-------------:|-----------:|-------------:|----------------:|\n",
       "|      0 | Primary & secondary |          |         |          1   |            0   |          0.2 |            |              |                 |\n",
       "|   1200 |                     |          |         |          0.8 |            0.3 |              |            |              |                 |\n",
       "|   1800 |                     |          |         |              |                |          0.4 |       0.1  |              |                 |\n",
       "|   3000 | All hues            | anchor   |         |              |            0   |              |       0.25 |         0    |            0    |\n",
       "|   3350 |                     |          |   0.01  |              |                |              |            |              |                 |\n",
       "|   6500 |                     |          |         |          0.8 |                |          0   |            |              |                 |\n",
       "|   8600 |                     |          |         |              |                |              |            |         0.3  |                 |\n",
       "|  10000 | Full color space    |          |         |              |                |              |            |              |            0.25 |\n",
       "|  10500 |                     |          |         |              |                |              |            |              |                 |\n",
       "|  13000 |                     |          |         |          1   |                |              |            |         0.1  |            1    |\n",
       "|  20000 |                     |          |   0.001 |              |                |              |            |         0.75 |                 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 386.3 ut.nb: Figure saved: 'large-assets/ex-1.5-color-timeline.png'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.5-color-timeline.png?v=450161\" alt=\"Line chart showing the hyperparameter schedule over time.\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML, Markdown\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "from mini.temporal.vis import group_properties_by_scale, plot_timeline, realize_timeline\n",
    "from mini.temporal.dopesheet import Dopesheet\n",
    "from mini.temporal.timeline import Timeline\n",
    "from utils.nb import save_fig\n",
    "\n",
    "dopesheet = Dopesheet.from_csv('ex-1.5-dopesheet.csv')\n",
    "display(\n",
    "    Markdown(f\"\"\"\n",
    "## Parameter schedule\n",
    "{dopesheet.to_markdown()}\n",
    "\"\"\")\n",
    ")\n",
    "\n",
    "timeline = Timeline(dopesheet)\n",
    "history_df = realize_timeline(timeline)\n",
    "keyframes_df = dopesheet.as_df()\n",
    "\n",
    "groups = group_properties_by_scale(keyframes_df[dopesheet.props])\n",
    "fig, ax = plot_timeline(history_df, keyframes_df, groups)\n",
    "# Add assertion to satisfy type checker\n",
    "assert isinstance(fig, Figure), 'plot_timeline should return a Figure'\n",
    "display(\n",
    "    HTML(\n",
    "        save_fig(\n",
    "            fig,  # Now type checker is happy\n",
    "            'large-assets/ex-1.5-color-timeline.png',\n",
    "            alt_text='Timeline visualization showing the dopesheet parameter schedule with keyframes and interpolated values over time.',\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions and regularizers\n",
    "\n",
    "We use mean squared error for the main reconstruction loss (`loss-recon`). The following regularizers, weighted according to the dopesheet schedule, guide the latent space structure:\n",
    "- **`unitarity` (`reg-norm`):** Encourages latent vectors to lie on a unit hypersphere by penalizing deviations from a norm of 1.\n",
    "- **`planarity` (`reg-planar`):** Pushes dimensions beyond the first two towards zero, encouraging the primary hue structure to form in the first two dimensions.\n",
    "- **`Separate` (`reg-separate`):** Pushes latent points away from each other, primarily used in the early phase to spread out the primary/secondary colors.\n",
    "- **`Anchor` (`reg-anchor`):** This is a `SpecialLossCriterion`. When the `anchor` action is triggered by the timeline, its `on_anchor` method captures the current latent positions of a reference dataset (the primary/secondary colors). Subsequently, its `__call__` method calculates a loss based on how far the *current* model places those reference colors from their *captured* anchor positions. This penalizes drift in the established structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import linalg as LA\n",
    "\n",
    "from ex_color.data.color_cube import ColorCube\n",
    "from ex_color.data.cyclic import arange_cyclic\n",
    "\n",
    "\n",
    "def objective(fn):\n",
    "    \"\"\"Adapt loss function to look like a regularizer\"\"\"\n",
    "\n",
    "    def wrapper(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "        return fn(data, res.outputs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def unitarity(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "    \"\"\"Regularize latents to have unit norm (vectors of length 1)\"\"\"\n",
    "    norms = LA.vector_norm(res.latents, dim=-1)\n",
    "    return torch.mean((norms - 1.0) ** 2)\n",
    "\n",
    "\n",
    "def planarity(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "    \"\"\"Regularize latents to be planar in the first two channels (so zero in other channels)\"\"\"\n",
    "    return torch.mean(res.latents[:, 2:] ** 2)\n",
    "\n",
    "\n",
    "class Separate(LossCriterion):\n",
    "    def __init__(self, channels: tuple[int, ...] = (0, 1)):\n",
    "        self.channels = channels\n",
    "\n",
    "    def __call__(self, data: Tensor, res: InferenceResult) -> Tensor:\n",
    "        \"\"\"Regularize latents to be separated from each other in first two channels\"\"\"\n",
    "        # Get pairwise differences in the first two dimensions\n",
    "        points = res.latents[:, self.channels]  # [B, C]\n",
    "        diffs = points.unsqueeze(1) - points.unsqueeze(0)  # [B, B, C]\n",
    "\n",
    "        # Calculate squared distances\n",
    "        sq_dists = torch.sum(diffs**2, dim=-1)  # [B, B]\n",
    "\n",
    "        # Remove self-distances (diagonal)\n",
    "        mask = 1.0 - torch.eye(sq_dists.shape[0], device=sq_dists.device)\n",
    "        masked_sq_dists = sq_dists * mask\n",
    "\n",
    "        # Encourage separation by minimizing inverse distances (stronger repulsion between close points)\n",
    "        epsilon = 1e-6  # Prevent division by zero\n",
    "        return torch.mean(1.0 / (masked_sq_dists + epsilon))\n",
    "\n",
    "\n",
    "class Anchor(SpecialLossCriterion):\n",
    "    \"\"\"Regularize latents to be close to their position in the reference phase\"\"\"\n",
    "\n",
    "    ref_data: Tensor\n",
    "    _ref_latents: Tensor | None = None\n",
    "\n",
    "    def __init__(self, ref_data: Tensor):\n",
    "        self.ref_data = ref_data\n",
    "        self._ref_latents = None\n",
    "        log.info(f'Anchor initialized with reference data shape: {ref_data.shape}')\n",
    "\n",
    "    def forward(self, model: ColorMLP, data: Tensor) -> InferenceResult | None:\n",
    "        \"\"\"Run the *stored reference data* through the *current* model.\"\"\"\n",
    "        # Note: The 'data' argument passed by the training loop for SpecialLossCriterion\n",
    "        # is the *current training batch*, which we IGNORE here.\n",
    "        # We only care about running our stored _ref_data through the model.\n",
    "        device = next(model.parameters()).device\n",
    "        ref_data = self.ref_data.to(device)\n",
    "\n",
    "        outputs, latents = model(ref_data)\n",
    "        return InferenceResult(outputs, latents)\n",
    "\n",
    "    def __call__(self, data: Tensor, special: InferenceResult) -> Tensor:\n",
    "        \"\"\"Calculates loss between current model's latents (for ref_data) and the stored reference latents.\"\"\"\n",
    "        if self._ref_latents is None:\n",
    "            # This means on_anchor hasn't been called yet, so the anchor loss is zero.\n",
    "            # This prevents errors during the very first phase before the anchor point is set.\n",
    "            log.debug('Anchor.__call__ invoked before reference latents captured. Returning zero loss.')\n",
    "            return torch.tensor(0.0, device=special.latents.device)\n",
    "        ref_latents = self._ref_latents.to(special.latents.device)\n",
    "        return torch.mean((special.latents - ref_latents) ** 2)\n",
    "\n",
    "    def on_anchor(self, event: Event):\n",
    "        # Called when the 'anchor' event is triggered\n",
    "        log.info(f'Capturing anchor latents via Anchor.on_anchor at step {event.step}')\n",
    "\n",
    "        device = next(event.model.parameters()).device\n",
    "        ref_data = self.ref_data.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, latents = event.model(ref_data)\n",
    "        self._ref_latents = latents.detach().cpu()\n",
    "        log.info(f'Anchor state captured internally. Ref data: {ref_data.shape}, Ref latents: {latents.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading, sampling, and event handling\n",
    "\n",
    "Here we set up:\n",
    "- **Datasets:** Define the datasets used (primary/secondary colors, full color grid).\n",
    "- **Sampler:** Use `DynamicWeightedRandomBatchSampler` for the full dataset. Its weights are updated by the `update_sampler_weights` callback, which responds to the `data-fraction` parameter from the dopesheet. This smoothly shifts the sampling focus from highly vibrant colors early on to the full range of colors later.\n",
    "- **Recorders:** `ModelRecorder` and `MetricsRecorder` are event handlers that save the model state and loss values at each step.\n",
    "- **Event bindings:** Connect event handlers to specific events (e.g., `plotter` to `phase-end`, `reg_anchor.on_anchor` to `action:anchor`, recorders to `pre-step` and `step-metrics`).\n",
    "- **Training execution:** Finally, call `train_color_model` with the model, datasets, dopesheet, loss criteria, and configured event handlers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 420.9 no:    Model initialized with 263 trainable parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 420.9 no:    Anchor initialized with reference data shape: torch.Size([6, 3])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 100%; padding: 5px 0; font-family: monospace;\">\n",
       "            <!-- Progress bar container -->\n",
       "            <div style=\"position: relative; height: calc(1em * 5/3); width: 100%;\">\n",
       "                <!-- Triangle indicator -->\n",
       "                <div style=\"position: absolute; bottom: -4px; left: calc(100.0% - 4px);\">\n",
       "                    <div style=\"\n",
       "                        width: 0;\n",
       "                        height: 0;\n",
       "                        border-left: 4px solid transparent;\n",
       "                        border-right: 4px solid transparent;\n",
       "                        border-bottom: 4px solid currentColor;\n",
       "                    \"></div>\n",
       "                </div>\n",
       "                <!-- Progress bar -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    height: 100%;\n",
       "                    width: 100.0%;\n",
       "                    background-color: color(from currentColor srgb r g b / 0.1);\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                \"></div>\n",
       "                <!-- Text overlay -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    width: 100%;\n",
       "                    height: 100%;\n",
       "                    text-align: center;\n",
       "                    line-height: calc(1em * 5/3);\n",
       "                    font-size: 0.9em;\n",
       "                    white-space: nowrap;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5);\n",
       "                \">\n",
       "                    <b>Training Steps</b>: 100.0% [<b>20001</b>/20001] [<b>00:49</b>/<00:00, 405.11 it/s]\n",
       "                </div>\n",
       "            </div>\n",
       "        \n",
       "            <div style=\"\n",
       "                display: grid;\n",
       "                grid-template-columns: repeat(6, minmax(80px, 1fr));\n",
       "                gap: 5px 0px;\n",
       "                width: 100%;\n",
       "                margin-top: 10px;\n",
       "                font-size: 0.85em;\n",
       "            \"><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">PHASE</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">lr</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">loss</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">loss-recon</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">reg-norm</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">reg-anchor</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">Full color space</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.001000</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0002</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0002</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0001</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0000</div></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 425.3 no:    Plotting end of phase: Primary & secondary at step 2999 using provided results.\n",
      "I 425.8 ut.nb: Figure saved: 'large-assets/ex-1.5-color-phase-history.png'\n",
      "I 425.8 ut.nb: Figure saved: 'large-assets/ex-1.5-color-phase-history.png'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.5-color-phase-history.png?v=329283\" alt=\"Visualizations of latent space at the end of each curriculum phase.\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 425.8 no:    Capturing anchor latents via Anchor.on_anchor at step 3000\n",
      "I 425.8 no:    Anchor state captured internally. Ref data: torch.Size([6, 3]), Ref latents: torch.Size([6, 4])\n",
      "I 425.8 no:    Anchor state captured internally. Ref data: torch.Size([6, 3]), Ref latents: torch.Size([6, 4])\n",
      "I 444.1 no:    Plotting end of phase: All hues at step 9999 using provided results.\n",
      "I 444.1 no:    Plotting end of phase: All hues at step 9999 using provided results.\n",
      "I 444.5 ut.nb: Figure saved: 'large-assets/ex-1.5-color-phase-history.png'\n",
      "I 444.5 ut.nb: Figure saved: 'large-assets/ex-1.5-color-phase-history.png'\n",
      "I 469.6 no:    Plotting end of phase: Full color space at step 20000 using provided results.\n",
      "I 469.6 no:    Plotting end of phase: Full color space at step 20000 using provided results.\n",
      "I 470.3 ut.nb: Figure saved: 'large-assets/ex-1.5-color-phase-history.png'\n",
      "I 470.3 no:    Training finished!\n",
      "I 470.3 ut.nb: Figure saved: 'large-assets/ex-1.5-color-phase-history.png'\n",
      "I 470.3 no:    Training finished!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from ex_color.data.cube_sampler import DynamicWeightedRandomBatchSampler, vibrancy\n",
    "from ex_color.data.filters import levels\n",
    "\n",
    "\n",
    "class ModelRecorder(EventHandler):\n",
    "    \"\"\"Event handler to record model parameters.\"\"\"\n",
    "\n",
    "    history: list[tuple[int, dict[str, Tensor]]]\n",
    "    \"\"\"A list of tuples (step, state_dict) where state_dict is a copy of the model's state dict.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def __call__(self, event: Event):\n",
    "        # It's crucial to get a *copy* of the state dict and move it to the CPU\n",
    "        # so we don't hold onto GPU memory or track gradients unnecessarily.\n",
    "        model_state = {k: v.cpu().clone() for k, v in event.model.state_dict().items()}\n",
    "        self.history.append((event.step, model_state))\n",
    "        log.debug(f'Recorded model state at step {event.step}')\n",
    "\n",
    "\n",
    "class MetricsRecorder(EventHandler):\n",
    "    \"\"\"Event handler to record training metrics.\"\"\"\n",
    "\n",
    "    history: list[tuple[int, float, dict[str, float]]]\n",
    "    \"\"\"A list of tuples (step, total_loss, losses_dict).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def __call__(self, event: StepMetricsEvent):\n",
    "        # Ensure we are handling the correct event type\n",
    "        if not isinstance(event, StepMetricsEvent):\n",
    "            log.warning(f'MetricsRecorder received unexpected event type: {type(event)}')\n",
    "            return\n",
    "\n",
    "        self.history.append((event.step, event.total_loss, event.losses.copy()))\n",
    "        log.debug(f'Recorded metrics at step {event.step}: loss={event.total_loss:.4f}')\n",
    "\n",
    "\n",
    "primary_cube = ColorCube.from_hsv(h=arange_cyclic(step_size=1 / 6), s=np.ones(1), v=np.ones(1))\n",
    "primary_tensor = torch.tensor(primary_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "primary_dataset = TensorDataset(primary_tensor)\n",
    "primary_loader = DataLoader(primary_dataset, batch_size=len(primary_tensor))\n",
    "\n",
    "full_cube = ColorCube.from_hsv(\n",
    "    h=arange_cyclic(step_size=10 / 360),\n",
    "    s=np.linspace(0, 1, 10),\n",
    "    v=np.linspace(0, 1, 10),\n",
    ")\n",
    "full_tensor = torch.tensor(full_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "full_dataset = TensorDataset(full_tensor)\n",
    "full_sampler = DynamicWeightedRandomBatchSampler(\n",
    "    bias=full_cube.bias.flatten(),\n",
    "    batch_size=256,\n",
    "    steps_per_epoch=100,\n",
    ")\n",
    "vibrancy_weights = vibrancy(full_cube).flatten()\n",
    "full_loader = DataLoader(full_dataset, batch_sampler=full_sampler)\n",
    "\n",
    "rgb_cube = ColorCube.from_rgb(\n",
    "    r=np.linspace(0, 1, 10),\n",
    "    g=np.linspace(0, 1, 10),\n",
    "    b=np.linspace(0, 1, 10),\n",
    ")\n",
    "rgb_tensor = torch.tensor(rgb_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "\n",
    "\n",
    "def update_sampler_weights(event: Event):\n",
    "    frac = event.timeline_state.props['data-fraction']\n",
    "    # When the fraction is near zero, in_low is almost 1 — which means \"scale everything down to 0 except for 1\"\n",
    "    # When the fraction is 0.5, in_low and out_low are both 0, so the weights are unchanged\n",
    "    # When the fraction is 1, in_low is 0 and out_low is 1, so the weights are all scaled to 1\n",
    "    in_low = np.interp(frac, [0, 0.5], [0.99, 0])\n",
    "    out_low = np.interp(frac, [0.5, 1], [0, 1])\n",
    "    full_sampler.weights = levels(vibrancy_weights, in_low=in_low, out_low=out_low)\n",
    "\n",
    "\n",
    "recorder = ModelRecorder()\n",
    "metrics_recorder = MetricsRecorder()\n",
    "\n",
    "# Phase -> (train loader, validation tensor)\n",
    "datasets: dict[str, tuple[DataLoader, Tensor]] = {\n",
    "    'Primary & secondary': (primary_loader, primary_tensor),\n",
    "    'All hues': (full_loader, rgb_tensor),\n",
    "    'Full color space': (full_loader, rgb_tensor),\n",
    "}\n",
    "\n",
    "model = ColorMLP(normalize_bottleneck=False)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "log.info(f'Model initialized with {total_params:,} trainable parameters.')\n",
    "\n",
    "event_handlers = EventHandlers()\n",
    "event_handlers.pre_step.add_handler('pre-step', recorder)\n",
    "event_handlers.pre_step.add_handler('pre-step', update_sampler_weights)\n",
    "event_handlers.step_metrics.add_handler('step-metrics', metrics_recorder)\n",
    "\n",
    "plotter = PhasePlotter(dim_pairs=[(0, 1), (0, 2), (0, 3)])\n",
    "event_handlers.phase_end.add_handler('phase-end', plotter)\n",
    "\n",
    "reg_anchor = Anchor(ref_data=primary_tensor)\n",
    "event_handlers.action.add_handler('action:anchor', reg_anchor.on_anchor)\n",
    "\n",
    "history = await train_color_model(\n",
    "    model,\n",
    "    datasets,\n",
    "    dopesheet,\n",
    "    loss_criteria={\n",
    "        'loss-recon': objective(nn.MSELoss()),\n",
    "        'reg-separate': Separate((0, 1)),\n",
    "        'reg-planar': planarity,\n",
    "        'reg-norm': unitarity,\n",
    "        'reg-anchor': reg_anchor,\n",
    "    },\n",
    "    event_handlers=event_handlers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space evolution analysis\n",
    "\n",
    "Let's visualize how the latent space evolved over time. We use the `ModelRecorder`'s history to load the model state at each recorded step and evaluate the latent positions for a fixed set of input colors (the full RGB grid). This gives us a sequence of latent space snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 100%; padding: 5px 0; font-family: monospace;\">\n",
       "            <!-- Progress bar container -->\n",
       "            <div style=\"position: relative; height: calc(1em * 5/3); width: 100%;\">\n",
       "                <!-- Triangle indicator -->\n",
       "                <div style=\"position: absolute; bottom: -4px; left: calc(0.0% - 4px);\">\n",
       "                    <div style=\"\n",
       "                        width: 0;\n",
       "                        height: 0;\n",
       "                        border-left: 4px solid transparent;\n",
       "                        border-right: 4px solid transparent;\n",
       "                        border-bottom: 4px solid currentColor;\n",
       "                    \"></div>\n",
       "                </div>\n",
       "                <!-- Progress bar -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    height: 100%;\n",
       "                    width: 0.0%;\n",
       "                    background-color: color(from currentColor srgb r g b / 0.1);\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                \"></div>\n",
       "                <!-- Text overlay -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    width: 100%;\n",
       "                    height: 100%;\n",
       "                    text-align: center;\n",
       "                    line-height: calc(1em * 5/3);\n",
       "                    font-size: 0.9em;\n",
       "                    white-space: nowrap;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5);\n",
       "                \">\n",
       "                    <b>Evaluating latents</b>: 0.0% [<b>0</b>/20001] [<b>00:00</b>/<00:00, 0.00 it/s]\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "async def eval_latent_history(\n",
    "    recorder: ModelRecorder,\n",
    "    rgb_tensor: Tensor,\n",
    "):\n",
    "    \"\"\"Evaluate the latent space for each step in the recorder's history.\"\"\"\n",
    "    # Create a new model instance\n",
    "    from utils.progress import AsyncProgress\n",
    "\n",
    "    model = ColorMLP(normalize_bottleneck=False)\n",
    "\n",
    "    latent_history: list[tuple[int, np.ndarray]] = []\n",
    "    # Iterate over the recorded history\n",
    "    async for step, state_dict in co_op(AsyncProgress(recorder.history, description='Evaluating latents')):\n",
    "        # Load the model state dict\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get the latents for the RGB tensor\n",
    "            _, latents = model(rgb_tensor.to(next(model.parameters()).device))\n",
    "            latents = latents.cpu().numpy()\n",
    "            latent_history.append((step, latents))\n",
    "    return latent_history\n",
    "\n",
    "\n",
    "latent_history = await eval_latent_history(recorder, rgb_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation of latent space\n",
    "\n",
    "This final visualization combines multiple views into a single animation:\n",
    "- **Latent space:** Shows the 2D projection (Dims 0 vs 1) of the latent embeddings for the full RGB color grid, colored by their true RGB values. We can see the color wheel forming and potentially expanding/contracting.\n",
    "- **Hyperparameters:** Replots the parameter schedule from the dopesheet, with a vertical line indicating the current step in the animation.\n",
    "- **Training metrics:** Plots the total loss and the contribution of each individual loss/regularization term (on a log scale), again with a vertical line for the current step.\n",
    "\n",
    "*(Note: A variable stride is used for sampling frames to focus on periods of rapid change.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 100%; padding: 5px 0; font-family: monospace;\">\n",
       "            <!-- Progress bar container -->\n",
       "            <div style=\"position: relative; height: calc(1em * 5/3); width: 100%;\">\n",
       "                <!-- Triangle indicator -->\n",
       "                <div style=\"position: absolute; bottom: -4px; left: calc(100.0% - 4px);\">\n",
       "                    <div style=\"\n",
       "                        width: 0;\n",
       "                        height: 0;\n",
       "                        border-left: 4px solid transparent;\n",
       "                        border-right: 4px solid transparent;\n",
       "                        border-bottom: 4px solid currentColor;\n",
       "                    \"></div>\n",
       "                </div>\n",
       "                <!-- Progress bar -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    height: 100%;\n",
       "                    width: 100.0%;\n",
       "                    background-color: color(from currentColor srgb r g b / 0.1);\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                \"></div>\n",
       "                <!-- Text overlay -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    width: 100%;\n",
       "                    height: 100%;\n",
       "                    text-align: center;\n",
       "                    line-height: calc(1em * 5/3);\n",
       "                    font-size: 0.9em;\n",
       "                    white-space: nowrap;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5);\n",
       "                \">\n",
       "                    <b>Rendering video</b>: 100.0% [<b>1905</b>/1905] [<b>04:34</b>/<00:00, 6.95 it/s]\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <video width=\"960\" height=\"480\" controls loop>\n",
       "            <source src=\"large-assets/ex-1.5-latent-evolution-with-metrics.mp4?v=316901\" type=\"video/mp4\">\n",
       "            Your browser does not support the video tag.\n",
       "        </video>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import imageio_ffmpeg\n",
    "from matplotlib import rcParams\n",
    "import pandas as pd\n",
    "\n",
    "from mini.temporal.dopesheet import RESERVED_COLS\n",
    "from utils.progress import AsyncProgress\n",
    "from mini.temporal.vis import group_properties_by_scale, plot_timeline\n",
    "from utils.progress.progress import SyncProgress\n",
    "\n",
    "rcParams['animation.ffmpeg_path'] = imageio_ffmpeg.get_ffmpeg_exe()\n",
    "\n",
    "\n",
    "def animate_latent_evolution_with_metrics(\n",
    "    latent_history: list[tuple[int, np.ndarray]],\n",
    "    metrics_history: list[tuple[int, float, dict[str, float]]],\n",
    "    param_history_df: pd.DataFrame,\n",
    "    param_keyframes_df: pd.DataFrame,\n",
    "    colors: np.ndarray,\n",
    "    dim_pair: tuple[int, int] = (0, 1),\n",
    "    interval=1 / 30,\n",
    "):\n",
    "    \"\"\"Create an animation of the latent space evolution alongside hyperparameter and metric plots.\"\"\"\n",
    "    plt.style.use('dark_background')\n",
    "    # Create a figure with 3 subplots: 1 for latent, 2 for lines\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    gs = fig.add_gridspec(2, 2, width_ratios=[1, 1], height_ratios=[1, 1])\n",
    "\n",
    "    ax_latent = fig.add_subplot(gs[:, 0])  # Latent space on the left, spanning rows\n",
    "    ax_params = fig.add_subplot(gs[0, 1])  # Params top right\n",
    "    ax_metrics = fig.add_subplot(gs[1, 1])  # Metrics bottom right\n",
    "\n",
    "    fig.patch.set_facecolor('#333')\n",
    "    ax_latent.patch.set_facecolor('#222')\n",
    "    ax_params.patch.set_facecolor('#222')\n",
    "    ax_metrics.patch.set_facecolor('#222')\n",
    "\n",
    "    # --- Setup Latent Plot ---\n",
    "    ax_latent.set_xlim(-1.5, 1.5)\n",
    "    ax_latent.set_ylim(-1.5, 1.5)\n",
    "    ax_latent.set_aspect('equal')\n",
    "    ax_latent.set_xlabel(f'Dim {dim_pair[0]}')\n",
    "    ax_latent.set_ylabel(f'Dim {dim_pair[1]}')\n",
    "    step, current_latents = latent_history[0]\n",
    "    scatter = ax_latent.scatter(\n",
    "        current_latents[:, dim_pair[0]], current_latents[:, dim_pair[1]], c=colors, s=30, alpha=0.7\n",
    "    )\n",
    "    title_latent = ax_latent.set_title(f'Latent Space (Step {step})')\n",
    "\n",
    "    # --- Setup Parameter Plot ---\n",
    "    # Filter out 'STEP' and other reserved columns before grouping\n",
    "    param_props = param_keyframes_df.columns.difference(list(RESERVED_COLS)).tolist()\n",
    "    param_groups = group_properties_by_scale(param_keyframes_df[param_props])\n",
    "    # Pass only the first group if plotting on a specific axis\n",
    "    plot_timeline(param_history_df, param_keyframes_df, [param_groups[0]], ax=ax_params, show_legend=True)\n",
    "    param_vline = ax_params.axvline(step, color='white', linestyle='--', lw=1)\n",
    "\n",
    "    ax_params.set_title('Hyperparameters')\n",
    "    ax_params.set_xlabel('')  # Remove x-label as it shares with metrics\n",
    "    ax_params.tick_params(axis='x', labelbottom=False)\n",
    "\n",
    "    # --- Setup Metrics Plot ---\n",
    "    metrics_steps = [h[0] for h in metrics_history]\n",
    "    total_losses = [h[1] for h in metrics_history]\n",
    "    loss_components = {k: [h[2].get(k, np.nan) for h in metrics_history] for k in metrics_history[0][2].keys()}\n",
    "\n",
    "    ax_metrics.plot(metrics_steps, total_losses, label='Total Loss', lw=2)\n",
    "    for name, values in loss_components.items():\n",
    "        ax_metrics.plot(metrics_steps, values, label=name, lw=1, alpha=0.8)\n",
    "\n",
    "    ax_metrics.set_xlabel('Step')\n",
    "    ax_metrics.set_ylabel('Loss (log scale)')  # Update label\n",
    "    ax_metrics.set_title('Training Metrics')\n",
    "    ax_metrics.legend(fontsize='small')\n",
    "    ax_metrics.set_yscale('log')  # Set log scale\n",
    "    ax_metrics.set_ylim(bottom=1e-6)  # Set bottom slightly above zero for log scale\n",
    "    metrics_vline = ax_metrics.axvline(step, color='white', linestyle='--', lw=1)\n",
    "    # Use uppercase 'STEP' for accessing the history_df column\n",
    "    max_step = param_history_df['STEP'].max()\n",
    "    ax_metrics.set_xlim(left=0, right=max_step)\n",
    "    ax_params.set_xlim(left=0, right=max_step)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    def update(frame: int):\n",
    "        # frame is the index in the *sampled* history\n",
    "        latent_step, current_latents = latent_history[frame]\n",
    "\n",
    "        # Update latent space\n",
    "        scatter.set_offsets(current_latents[:, dim_pair])\n",
    "        title_latent.set_text(f'Latent Space (Step {latent_step})')\n",
    "\n",
    "        # Update vertical lines\n",
    "        param_vline.set_xdata([latent_step])\n",
    "        metrics_vline.set_xdata([latent_step])\n",
    "\n",
    "        return scatter, title_latent, param_vline, metrics_vline\n",
    "\n",
    "    # Use the length of the (potentially strided) latent_history for frames\n",
    "    num_frames = len(latent_history)\n",
    "    ani = animation.FuncAnimation(fig, update, frames=num_frames, interval=interval * 1000, blit=True)\n",
    "    return fig, ani\n",
    "\n",
    "\n",
    "# --- Variable Stride Logic ---\n",
    "def get_stride(step: int):\n",
    "    import math\n",
    "\n",
    "    a = 7.9236\n",
    "    b = 0.0005\n",
    "    return a * math.log(b * step + 1) + 1\n",
    "\n",
    "\n",
    "sampled_indices = [0]\n",
    "last_sampled_index = 0\n",
    "while True:\n",
    "    # Get the step number corresponding to the last sampled frame\n",
    "    current_step = latent_history[round(last_sampled_index)][0]\n",
    "    # Determine the stride based on that step number\n",
    "    stride = get_stride(current_step)\n",
    "    # Calculate the index of the next potential frame\n",
    "    next_index = last_sampled_index + stride\n",
    "    # Stop if we've gone past the end of the history\n",
    "    if round(next_index) >= len(latent_history):\n",
    "        break\n",
    "    # Add the calculated index to our list\n",
    "    sampled_indices.append(round(next_index))\n",
    "    # Update the last sampled index for the next iteration\n",
    "    last_sampled_index = next_index\n",
    "\n",
    "# Use the sampled indices to select frames from the full history\n",
    "sampled_latent_history = [latent_history[i] for i in sampled_indices]\n",
    "# --- End Variable Stride Logic ---\n",
    "\n",
    "# Filter metrics history to align with the *new* sampled latent history steps\n",
    "sampled_steps_set = {step for step, _ in sampled_latent_history}\n",
    "filtered_metrics_history = [h for h in metrics_recorder.history if h[0] in sampled_steps_set]\n",
    "\n",
    "# Make sure we have the parameter history dataframes (they were created earlier)\n",
    "# history_df, keyframes_df\n",
    "\n",
    "fig, ani = animate_latent_evolution_with_metrics(\n",
    "    latent_history=sampled_latent_history,  # Use variable stride history\n",
    "    metrics_history=filtered_metrics_history,  # Use filtered metrics\n",
    "    param_history_df=history_df,  # Full parameter history\n",
    "    param_keyframes_df=keyframes_df,  # Keyframes for plotting\n",
    "    colors=rgb_tensor.cpu().numpy(),\n",
    "    dim_pair=(0, 1),\n",
    ")\n",
    "\n",
    "video_file = 'large-assets/ex-1.5-latent-evolution-with-metrics.mp4'  # New filename\n",
    "num_frames_to_render = len(sampled_latent_history)  # Update frame count\n",
    "with SyncProgress(total=num_frames_to_render, description='Rendering video') as pbar:\n",
    "    ani.save(\n",
    "        video_file,\n",
    "        fps=30,\n",
    "        extra_args=['-vcodec', 'libx264'],\n",
    "        progress_callback=lambda i, n: pbar.update(total=n, count=i),\n",
    "    )\n",
    "plt.close(fig)\n",
    "\n",
    "from random import randint\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "cache_buster = randint(1, 1_000_000)\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        f\"\"\"\n",
    "        <video width=\"960\" height=\"480\" controls loop>\n",
    "            <source src=\"{video_file}?v={cache_buster:d}\" type=\"video/mp4\">\n",
    "            Your browser does not support the video tag.\n",
    "        </video>\n",
    "        \"\"\"\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex-color-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
