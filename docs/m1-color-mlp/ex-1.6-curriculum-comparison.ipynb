{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1.6: Smooth vs. stepped hyperparameter transitions\n",
    "\n",
    "In previous experiments, we explored curriculum learning ([Ex 1.3](./ex-1.3-color-mlp-curriculum.ipynb)) with abrupt phase changes and later introduced smooth hyperparameter transitions using a dopesheet and minimum jerk interpolation ([Ex 1.5](./ex-1.5-color-mlp-anchoring.ipynb)).\n",
    "\n",
    "This notebook directly compares these two approaches:\n",
    "\n",
    "1.  **Stepped transitions:** Mimicking the traditional approach with discrete phases and sharp parameter changes at boundaries. We'll simulate the LR warmup used in Ex 1.3 within the dopesheet.\n",
    "2.  **Smooth transitions:** Using the minimum jerk trajectories from Ex 1.5 for all hyperparameters.\n",
    "\n",
    "Both methods will use the same 4D bottleneck model architecture, initialization seeds, loss functions (including anchoring), and target the same final hyperparameter values at equivalent points in the curriculum.\n",
    "\n",
    "While both approaches might reach similar final performance, we hypothesize that the smooth transitions will lead to:\n",
    "\n",
    "- **More stable training:** Fewer and smaller loss spikes, especially during periods corresponding to phase transitions in the stepped approach.\n",
    "- **Smoother latent space evolution:** A more gradual and less chaotic development of the final representation structure.\n",
    "\n",
    "We'll train the 4D MLP autoencoder using two different dopesheets representing the stepped and smooth schedules. We will track:\n",
    "\n",
    "- Training loss curves (total and components).\n",
    "- Loss variance over time.\n",
    "- Final latent space structure.\n",
    "- Evolution of the latent space (via animation, similar to Ex 1.5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from utils.logging import SimpleLoggingConfig\n",
    "\n",
    "logging_config = SimpleLoggingConfig().info('notebook', 'utils', 'mini', 'ex_color')\n",
    "logging_config.apply()\n",
    "\n",
    "# ID for tagging assets\n",
    "nbid = '1.6'\n",
    "# This is the logger for this notebook\n",
    "log = logging.getLogger(f'notebook.{nbid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "We use the same simple 2-layer MLP autoencoder with a bottleneck as in previous experiments. The key difference lies not in the architecture, but in the training process governed by the smooth curriculum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "E = 4\n",
    "\n",
    "\n",
    "class ColorMLP(nn.Module):\n",
    "    def __init__(self, normalize_bottleneck=False):\n",
    "        super().__init__()\n",
    "        # RGB input (3D) → hidden layer → bottleneck → hidden layer → RGB output\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.GELU(),\n",
    "            # nn.Linear(16, 16),\n",
    "            # nn.GELU(),\n",
    "            nn.Linear(16, E),  # Our critical bottleneck!\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(E, 16),\n",
    "            nn.GELU(),\n",
    "            # nn.Linear(16, 16),\n",
    "            # nn.GELU(),\n",
    "            nn.Linear(16, 3),\n",
    "            nn.Sigmoid(),  # Keep RGB values in [0,1]\n",
    "        )\n",
    "\n",
    "        self.normalize = normalize_bottleneck\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Get our bottleneck representation\n",
    "        bottleneck = self.encoder(x)\n",
    "\n",
    "        # Optionally normalize to unit vectors (like nGPT)\n",
    "        if self.normalize:\n",
    "            norm = torch.norm(bottleneck, dim=1, keepdim=True)\n",
    "            bottleneck = bottleneck / (norm + 1e-8)  # Avoid division by zero\n",
    "\n",
    "        # Decode back to RGB\n",
    "        output = self.decoder(bottleneck)\n",
    "        return output, bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training machinery with timeline and events\n",
    "\n",
    "The `train_color_model` function orchestrates the training process based on a `Timeline` derived from the dopesheet. It handles:\n",
    "\n",
    "- Iterating through training steps.\n",
    "- Fetching the correct data loader for the current phase.\n",
    "- Updating hyperparameters (like learning rate and loss weights) smoothly based on the timeline state.\n",
    "- Calculating the combined loss from reconstruction and various regularizers.\n",
    "- Executing the optimizer step.\n",
    "- Emitting events at different points (phase start/end, pre-step, actions like 'anchor', step metrics) to trigger callbacks like plotting, recording, or updating loss terms.\n",
    "\n",
    "### Improvements since Ex 1.5\n",
    "\n",
    "This training loop has one big improvement over the previous experiment: each training sample can have a different learning rate. This was needed to allow previously out-of-distribution data to be gradually introduced. See the regularizers and data loaders below for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Protocol, runtime_checkable\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "from mini.temporal.model import TStep\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InferenceResult:\n",
    "    outputs: Tensor\n",
    "    latents: Tensor\n",
    "\n",
    "    def detach(self):\n",
    "        return InferenceResult(self.outputs.detach(), self.latents.detach())\n",
    "\n",
    "    def clone(self):\n",
    "        return InferenceResult(self.outputs.clone(), self.latents.clone())\n",
    "\n",
    "    def cpu(self):\n",
    "        return InferenceResult(self.outputs.cpu(), self.latents.cpu())\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class LossCriterion(Protocol):\n",
    "    def __call__(self, data: Tensor, res: InferenceResult) -> Tensor: ...\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class SpecialLossCriterion(LossCriterion, Protocol):\n",
    "    def forward(self, model: ColorMLP, data: Tensor) -> InferenceResult | None: ...\n",
    "\n",
    "\n",
    "@dataclass(eq=False, frozen=True)\n",
    "class Event:\n",
    "    name: str\n",
    "    step: int\n",
    "    model: ColorMLP\n",
    "    timeline_state: TStep\n",
    "    optimizer: optim.Optimizer\n",
    "\n",
    "\n",
    "@dataclass(eq=False, frozen=True)\n",
    "class PhaseEndEvent(Event):\n",
    "    validation_data: Tensor\n",
    "    inference_result: InferenceResult\n",
    "\n",
    "\n",
    "@dataclass(eq=False, frozen=True)\n",
    "class StepMetricsEvent(Event):\n",
    "    \"\"\"Event carrying metrics calculated during a training step.\"\"\"\n",
    "\n",
    "    total_loss: float\n",
    "    losses: dict[str, float]\n",
    "\n",
    "\n",
    "class EventHandler[T](Protocol):\n",
    "    def __call__(self, event: T) -> None: ...\n",
    "\n",
    "\n",
    "class EventBinding[T]:\n",
    "    \"\"\"A class to bind events to handlers.\"\"\"\n",
    "\n",
    "    def __init__(self, event_name: str):\n",
    "        self.event_name = event_name\n",
    "        self.handlers: list[tuple[str, EventHandler[T]]] = []\n",
    "\n",
    "    def add_handler(self, event_name: str, handler: EventHandler[T]) -> None:\n",
    "        self.handlers.append((event_name, handler))\n",
    "\n",
    "    def emit(self, event_name: str, event: T) -> None:\n",
    "        for name, handler in self.handlers:\n",
    "            if name == event_name:\n",
    "                handler(event)\n",
    "\n",
    "\n",
    "class EventHandlers:\n",
    "    \"\"\"A simple event system to allow for custom callbacks.\"\"\"\n",
    "\n",
    "    phase_start: EventBinding[Event]\n",
    "    pre_step: EventBinding[Event]\n",
    "    action: EventBinding[Event]\n",
    "    phase_end: EventBinding[PhaseEndEvent]\n",
    "    step_metrics: EventBinding[StepMetricsEvent]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.phase_start = EventBinding[Event]('phase-start')\n",
    "        self.pre_step = EventBinding[Event]('pre-step')\n",
    "        self.action = EventBinding[Event]('action')\n",
    "        self.phase_end = EventBinding[PhaseEndEvent]('phase-end')\n",
    "        self.step_metrics = EventBinding[StepMetricsEvent]('step-metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Iterable, Iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from mini.temporal.dopesheet import Dopesheet\n",
    "from mini.temporal.timeline import Timeline\n",
    "from utils.progress import co_op, AsyncProgress\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    log.info(f'Global random seed set to {seed}')\n",
    "\n",
    "\n",
    "def set_deterministic_mode(seed: int):\n",
    "    \"\"\"Make experiments reproducible.\"\"\"\n",
    "    seed_everything(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    log.info('PyTorch set to deterministic mode')\n",
    "\n",
    "\n",
    "def reiterate[T](it: Iterable[T]) -> Iterator[T]:\n",
    "    \"\"\"\n",
    "    Iterates over an iterable indefinitely.\n",
    "\n",
    "    When the iterable is exhausted, it starts over from the beginning. Unlike\n",
    "    `itertools.cycle`, yielded values are not cached — so each iteration may be\n",
    "    different.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        yield from it\n",
    "\n",
    "\n",
    "async def train_color_model(  # noqa: C901\n",
    "    model: ColorMLP,\n",
    "    datasets: dict[str, tuple[DataLoader, Tensor]],\n",
    "    dopesheet: Dopesheet,\n",
    "    loss_criteria: dict[str, LossCriterion | SpecialLossCriterion],\n",
    "    event_handlers: EventHandlers | None = None,\n",
    "):\n",
    "    if event_handlers is None:\n",
    "        event_handlers = EventHandlers()\n",
    "\n",
    "    # --- Validate inputs ---\n",
    "    # Check if all phases in dopesheet have corresponding data\n",
    "    dopesheet_phases = dopesheet.phases\n",
    "    missing_data = dopesheet_phases - set(datasets.keys())\n",
    "    if missing_data:\n",
    "        raise ValueError(f'Missing data for dopesheet phases: {missing_data}')\n",
    "\n",
    "    # Check if 'lr' is defined in the dopesheet properties\n",
    "    if 'lr' not in dopesheet.props:\n",
    "        raise ValueError(\"Dopesheet must define the 'lr' property column.\")\n",
    "    # --- End Validation ---\n",
    "\n",
    "    timeline = Timeline(dopesheet)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    data_iterators = {\n",
    "        phase_name: iter(reiterate(dataloader))  #\n",
    "        for phase_name, (dataloader, _) in datasets.items()\n",
    "    }\n",
    "\n",
    "    total_steps = len(timeline)\n",
    "\n",
    "    async with AsyncProgress(total=total_steps, description='Training Steps') as pbar:\n",
    "        async for step in co_op(pbar):\n",
    "            # Get state *before* advancing timeline for this step's processing\n",
    "            current_state = timeline.state\n",
    "            current_phase_name = current_state.phase\n",
    "\n",
    "            # Assuming TensorDataset yields a tuple with two elements\n",
    "            batch_data, batch_weights = next(data_iterators[current_phase_name])\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_weights = batch_weights.to(device)\n",
    "\n",
    "            # --- Event Handling ---\n",
    "            event_template = {\n",
    "                'step': step,\n",
    "                'model': model,\n",
    "                'timeline_state': current_state,\n",
    "                'optimizer': optimizer,\n",
    "            }\n",
    "\n",
    "            if current_state.is_phase_start:\n",
    "                event = Event(name=f'phase-start:{current_phase_name}', **event_template)\n",
    "                event_handlers.phase_start.emit(event.name, event)\n",
    "                event_handlers.phase_start.emit('phase-start', event)\n",
    "\n",
    "            for action in current_state.actions:\n",
    "                event = Event(name=f'action:{action}', **event_template)\n",
    "                event_handlers.action.emit(event.name, event)\n",
    "                event_handlers.action.emit('action', event)\n",
    "\n",
    "            event = Event(name='pre-step', **event_template)\n",
    "            event_handlers.pre_step.emit('pre-step', event)\n",
    "\n",
    "            # --- Training Step ---\n",
    "            # ... (get data, update LR, zero grad, forward pass, calculate loss, backward, step) ...\n",
    "\n",
    "            current_lr = current_state.props['lr']\n",
    "            # REF_BATCH_SIZE = 32\n",
    "            # lr_scale_factor = batch.shape[0] / REF_BATCH_SIZE\n",
    "            # current_lr = current_lr * lr_scale_factor\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs, latents = model(batch_data)\n",
    "            current_results = InferenceResult(outputs, latents)\n",
    "\n",
    "            total_loss = torch.tensor(0.0, device=device)\n",
    "            losses_dict: dict[str, float] = {}\n",
    "            for name, criterion in loss_criteria.items():\n",
    "                weight = current_state.props.get(name, 0.0)\n",
    "                if weight == 0:\n",
    "                    continue\n",
    "\n",
    "                if isinstance(criterion, SpecialLossCriterion):\n",
    "                    # Special criteria might run on their own data (like Anchor)\n",
    "                    # or potentially use the current batch (depends on implementation).\n",
    "                    # The forward method gets the model and the *current batch*\n",
    "                    special_results = criterion.forward(model, batch_data)\n",
    "                    if special_results is None:\n",
    "                        continue\n",
    "                    term_loss = criterion(batch_data, special_results)\n",
    "                else:\n",
    "                    term_loss = criterion(batch_data, current_results)\n",
    "\n",
    "                if len(term_loss.shape) > 0:\n",
    "                    # If the loss is per-sample, we need to weight it\n",
    "                    if term_loss.shape[0] != batch_weights.shape[0]:\n",
    "                        raise ValueError(f'Batch size mismatch for {name}: {term_loss.shape} != {batch_weights.shape}')\n",
    "                    term_loss = (term_loss * batch_weights).mean()\n",
    "                else:\n",
    "                    # Otherwise, we assume it's already weighted (and probably scalar)\n",
    "                    term_loss = term_loss.mean()\n",
    "\n",
    "                losses_dict[name] = term_loss.item()\n",
    "                if not torch.isfinite(term_loss):\n",
    "                    log.warning(f'Loss {name} at step {step} is not finite: {term_loss}')\n",
    "                    continue\n",
    "                total_loss += term_loss * weight\n",
    "\n",
    "            if total_loss > 0:\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "            # --- End Training Step ---\n",
    "\n",
    "            # Emit step metrics event\n",
    "            step_metrics_event = StepMetricsEvent(\n",
    "                name='step-metrics',\n",
    "                **event_template,\n",
    "                total_loss=total_loss.item(),\n",
    "                losses=losses_dict,\n",
    "            )\n",
    "            event_handlers.step_metrics.emit('step-metrics', step_metrics_event)\n",
    "\n",
    "            # --- Post-Step Event Handling ---\n",
    "            if current_state.is_phase_end:\n",
    "                # Trigger phase-end for the *current* phase\n",
    "                _, validation_data = datasets[current_phase_name]\n",
    "                # validation_data = batch_data\n",
    "                with torch.no_grad():\n",
    "                    val_outputs, val_latents = model(validation_data.to(device))\n",
    "                event = PhaseEndEvent(\n",
    "                    name=f'phase-end:{current_phase_name}',\n",
    "                    **event_template,\n",
    "                    validation_data=validation_data,\n",
    "                    inference_result=InferenceResult(val_outputs, val_latents),\n",
    "                )\n",
    "                event_handlers.phase_end.emit(event.name, event)\n",
    "                event_handlers.phase_end.emit('phase-end', event)\n",
    "            # --- End Event Handling ---\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.metrics = {\n",
    "                'PHASE': current_phase_name,\n",
    "                'lr': f'{current_lr:.6f}',\n",
    "                'loss': f'{total_loss.item():.4f}',\n",
    "                **{name: f'{lt:.4f}' for name, lt in losses_dict.items()},\n",
    "            }\n",
    "\n",
    "            # Advance timeline *after* processing the current step\n",
    "            if step < total_steps:  # Avoid stepping past the end\n",
    "                timeline.step()\n",
    "\n",
    "    log.info('Training finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from torch import Tensor\n",
    "from IPython.display import HTML\n",
    "\n",
    "from utils.nb import save_fig\n",
    "\n",
    "\n",
    "class PhasePlotter:\n",
    "    \"\"\"Event handler to plot latent space at the end of each phase.\"\"\"\n",
    "\n",
    "    def __init__(self, *, dim_pairs: list[tuple[int, int]], variant: str):\n",
    "        from utils.nb import displayer\n",
    "\n",
    "        # Store (phase_name, end_step, data, result) - data comes from event now\n",
    "        self.history: list[tuple[str, int, Tensor, InferenceResult]] = []\n",
    "        self.display = displayer()\n",
    "        self.dim_pairs = dim_pairs\n",
    "        self.variant = variant\n",
    "\n",
    "    # Expect PhaseEndEvent specifically\n",
    "    def __call__(self, event: PhaseEndEvent):\n",
    "        \"\"\"Handle phase-end events.\"\"\"\n",
    "        if not isinstance(event, PhaseEndEvent):\n",
    "            raise TypeError(f'Expected PhaseEndEvent, got {type(event)}')\n",
    "\n",
    "        # TODO: Don't assume device = CPU\n",
    "        # TODO: Split this class so that the event handler is separate from the plotting, and so the plotting can happen locally with @run.hither\n",
    "        phase_name = event.timeline_state.phase\n",
    "        end_step = event.step\n",
    "        phase_dataset = event.validation_data\n",
    "        inference_result = event.inference_result\n",
    "\n",
    "        log.info(f'Plotting end of phase: {phase_name} at step {end_step} using provided results.')\n",
    "\n",
    "        # Append to history\n",
    "        self.history.append((phase_name, end_step, phase_dataset.cpu(), inference_result.cpu()))\n",
    "\n",
    "        # Plotting logic remains the same as it already expected CPU tensors\n",
    "        fig = self._plot_phase_history()\n",
    "        self.display(\n",
    "            HTML(\n",
    "                save_fig(\n",
    "                    fig,\n",
    "                    f'large-assets/ex-{nbid}-color-phase-history-{self.variant}.png',\n",
    "                    alt_text=f'Visualizations of latent space at the end of each {self.variant} curriculum phase.',\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _plot_phase_history(self):\n",
    "        num_phases = len(self.history)\n",
    "        plt.style.use('dark_background')\n",
    "        if num_phases == 0:\n",
    "            fig, ax = plt.subplots()\n",
    "            fig.set_facecolor('#333')\n",
    "            ax.set_facecolor('#222')\n",
    "            ax.text(0.5, 0.5, 'Waiting...', ha='center', va='center')\n",
    "            return fig\n",
    "\n",
    "        fig, axes = plt.subplots(\n",
    "            num_phases, len(self.dim_pairs), figsize=(5 * len(self.dim_pairs), 5 * num_phases), squeeze=False\n",
    "        )\n",
    "        fig.set_facecolor('#333')\n",
    "\n",
    "        for row_idx, (phase_name, end_step, data, res) in enumerate(self.history):\n",
    "            _latents = res.latents.numpy()\n",
    "            _colors = data.numpy()\n",
    "\n",
    "            for col_idx, (dim1, dim2) in enumerate(self.dim_pairs):\n",
    "                ax = axes[row_idx, col_idx]\n",
    "                ax.set_facecolor('#222')\n",
    "                ax.scatter(_latents[:, dim1], _latents[:, dim2], c=_colors, s=200, alpha=0.7)\n",
    "\n",
    "                # Set y-label differently for the first column\n",
    "                if col_idx == 0:\n",
    "                    ax.set_ylabel(\n",
    "                        f'Phase: {phase_name}\\n(End Step: {end_step})',\n",
    "                        fontsize='medium',\n",
    "                        rotation=90,  # Rotate vertically\n",
    "                        labelpad=15,  # Adjust padding\n",
    "                        verticalalignment='center',\n",
    "                        horizontalalignment='center',\n",
    "                    )\n",
    "                else:\n",
    "                    # Standard y-label for other columns\n",
    "                    ax.set_ylabel(f'Dim {dim2}')\n",
    "\n",
    "                # Set title only for the top row\n",
    "                if row_idx == 0:\n",
    "                    ax.set_title(f'Dims {dim1} vs {dim2}')\n",
    "\n",
    "                # Standard x-label for all columns\n",
    "                ax.set_xlabel(f'Dim {dim1}')\n",
    "\n",
    "                # Keep other plot settings\n",
    "                ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "                ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "                ax.add_patch(Circle((0, 0), 1, fill=False, linestyle='--', color='gray', alpha=0.3))\n",
    "                ax.set_aspect('equal')\n",
    "\n",
    "        fig.suptitle(\n",
    "            f'Latent space at the end of each phase ({self.variant})',\n",
    "            fontsize=16,\n",
    "            fontweight='bold',\n",
    "            color='white',\n",
    "        )\n",
    "        fig.tight_layout()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dopesheets for smooth and stepped curricula\n",
    "\n",
    "We'll define two dopesheets (timelines):\n",
    "1. The [smooth dopesheet](./ex-1.6-smooth-dopesheet.csv) uses the eased timing function that was used in the previous experiment.\n",
    "2. The [stepped dopesheet](./ex-1.6-stepped-dopesheet.csv) uses a step-end timing function.\n",
    "\n",
    "Apart from the stepped nature of the second, the curricula are almost the same, and both were tuned to give the best performance possible (within reasonable effort limits). To make it fair, the stepped sheet:\n",
    "- Has more phases, to allow more hyperparameter values\n",
    "- Uses a learning rate warmup at the start of each phase, since this is already a common practice in curriculum learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Parameter schedule (smooth)\n",
       "|   STEP | PHASE               | ACTION   |      lr |   loss-recon |   reg-separate |   reg-planar |   reg-norm |   reg-anchor |   data-hues |   data-vibrancies |\n",
       "|-------:|:--------------------|:---------|--------:|-------------:|---------------:|-------------:|-----------:|-------------:|------------:|------------------:|\n",
       "|      0 | Primary & secondary |          |   1e-08 |            1 |            0   |          0   |       0.01 |         0    |           0 |                 0 |\n",
       "|     10 |                     |          |   0.01  |              |                |              |            |              |             |                   |\n",
       "|    200 |                     |          |         |              |            0.2 |          0.4 |       0.25 |              |             |                   |\n",
       "|    300 |                     |          |         |              |                |              |            |              |             |                   |\n",
       "|    499 |                     |          |         |              |                |              |            |         0    |             |                   |\n",
       "|    500 | All hues            | anchor   |         |              |            0   |          0.2 |       0.25 |         0.25 |           0 |                   |\n",
       "|    950 |                     |          |         |              |                |              |       0.5  |              |           1 |                   |\n",
       "|   1400 |                     |          |         |              |                |          0   |            |              |             |                   |\n",
       "|   4999 |                     |          |         |              |                |              |            |              |             |                   |\n",
       "|   5000 | Full color space    |          |   0.02  |              |                |              |            |              |             |                 0 |\n",
       "|   5010 |                     |          |         |              |                |              |            |              |             |                   |\n",
       "|   6500 |                     |          |         |              |                |              |       0.25 |              |             |                 1 |\n",
       "|   7500 |                     |          |         |              |                |              |            |              |             |                   |\n",
       "|  10000 |                     |          |   0.001 |              |                |              |            |              |             |                   |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 3332.5 ut.nb:Figure saved: 'large-assets/ex-1.6-color-timeline-smooth.png'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.6-color-timeline-smooth.png?v=i_Mb0ieW65ZrHKwdWvgOPoUvuOsdcMOiA49bMDTmhpw\" alt=\"Line chart showing the smooth hyperparameter schedule over time.\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Parameter schedule (stepped)\n",
       "|   STEP | PHASE               | ACTION   |     lr |   loss-recon |   reg-separate |   reg-planar |   reg-norm |   reg-anchor |   data-hues |   data-vibrancies |\n",
       "|-------:|:--------------------|:---------|-------:|-------------:|---------------:|-------------:|-----------:|-------------:|------------:|------------------:|\n",
       "|      0 | Primary & secondary |          | 1e-08  |            1 |           0    |          0.2 |       0.01 |         0    |         0   |               0   |\n",
       "|     10 |                     |          | 0.01   |              |                |              |            |              |             |                   |\n",
       "|     90 |                     |          | 0.01   |              |                |              |            |              |             |                   |\n",
       "|    100 | Primary & secondary |          | 1e-08  |              |           0.15 |          0.3 |       0.25 |              |             |                   |\n",
       "|    110 |                     |          | 0.01   |              |                |              |            |              |             |                   |\n",
       "|    290 |                     |          | 0.01   |              |                |              |            |              |             |                   |\n",
       "|    300 | Primary & secondary |          | 1e-08  |              |           0    |          0.2 |            |              |             |                   |\n",
       "|    310 |                     |          | 0.01   |              |                |              |            |              |             |                   |\n",
       "|    490 |                     |          | 0.01   |              |                |              |            |              |             |                   |\n",
       "|    500 | All hues            | anchor   | 1e-08  |              |                |          0.1 |            |         0.25 |         0.5 |               0   |\n",
       "|    510 |                     |          | 0.01   |              |                |              |            |              |             |                   |\n",
       "|    990 |                     |          | 0.0125 |              |                |              |            |              |             |                   |\n",
       "|   1000 | All hues            |          | 1e-08  |              |                |          0   |       0.4  |              |         1   |                   |\n",
       "|   1010 |                     |          | 0.0125 |              |                |              |            |              |             |                   |\n",
       "|   4990 |                     |          | 0.02   |              |                |              |            |              |             |                   |\n",
       "|   5000 | Full color space    |          | 1e-08  |              |                |              |       0.25 |              |             |               0.5 |\n",
       "|   5010 |                     |          | 0.02   |              |                |              |            |              |             |                   |\n",
       "|   6490 |                     |          | 0.016  |              |                |              |            |              |             |                   |\n",
       "|   6500 | Full color space    |          | 1e-08  |              |                |              |            |              |             |               1   |\n",
       "|   6510 |                     |          | 0.016  |              |                |              |            |              |             |                   |\n",
       "|  10000 |                     |          | 0.001  |              |                |              |            |              |             |                   |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 3333.4 ut.nb:Figure saved: 'large-assets/ex-1.6-color-timeline-stepped.png'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.6-color-timeline-stepped.png?v=kJbTIQPGEKQiiW3EXxKuL4_pTsxlbqoznbLRS0BvYKE\" alt=\"Line chart showing the stepped hyperparameter schedule over time.\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "from mini.temporal.vis import group_properties_by_scale, plot_timeline, realize_timeline, ParamGroup\n",
    "from mini.temporal.dopesheet import Dopesheet\n",
    "from mini.temporal.timeline import Timeline\n",
    "from utils.nb import save_fig\n",
    "\n",
    "\n",
    "line_styles = [\n",
    "    (re.compile(r'^data-'), {'linewidth': 5, 'zorder': -1, 'alpha': 0.5}),\n",
    "    # (re.compile(r'-(anchor|norm)$'), {'linewidth': 2, 'linestyle': (0, (8, 1, 1, 1))}),\n",
    "]\n",
    "\n",
    "\n",
    "def load_dopesheet(variant: str):\n",
    "    dopesheet = Dopesheet.from_csv(f'ex-{nbid}-{variant}-dopesheet.csv')\n",
    "    # display(Markdown(f\"\"\"## Parameter schedule ({variant})\\n{dopesheet.to_markdown()}\"\"\"))\n",
    "\n",
    "    timeline = Timeline(dopesheet)\n",
    "    history_df = realize_timeline(timeline)\n",
    "    keyframes_df = dopesheet.as_df()\n",
    "\n",
    "    groups = (\n",
    "        ParamGroup(\n",
    "            name='',\n",
    "            params=[p for p in dopesheet.props if p not in {'lr'}],\n",
    "            height_ratio=2,\n",
    "        ),\n",
    "        ParamGroup(\n",
    "            name='',\n",
    "            params=[p for p in dopesheet.props if p in {'lr'}],\n",
    "            height_ratio=1,\n",
    "        ),\n",
    "    )\n",
    "    # groups = group_properties_by_scale(keyframes_df[dopesheet.props])\n",
    "    fig, ax = plot_timeline(history_df, keyframes_df, groups, title=f'Timeline ({variant})', line_styles=line_styles)\n",
    "    # Add assertion to satisfy type checker\n",
    "    assert isinstance(fig, Figure), 'plot_timeline should return a Figure'\n",
    "    display(\n",
    "        HTML(\n",
    "            save_fig(\n",
    "                fig,\n",
    "                f'large-assets/ex-{nbid}-color-timeline-{variant}.png',\n",
    "                alt_text=f'Timeline visualization showing the {variant} dopesheet parameter schedule with keyframes and interpolated values over time.',\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return dopesheet\n",
    "\n",
    "\n",
    "smooth_dopesheet = load_dopesheet('smooth')\n",
    "stepped_dopesheet = load_dopesheet('stepped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These schedules seem pretty well matched for a fair comparison. The core hyperparameter targets are hit at similar times, with the main difference being, well, the smoothness. This should give us a good basis for seeing what impact the transition style has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions and regularizers\n",
    "\n",
    "Like Ex 1.5, we use mean squared error for the main reconstruction loss (`loss-recon`), and regularizers that encourage embeddings of unit length, and for primary colors to be on the plane of the first two dimensions.\n",
    "\n",
    "Unlike Ex 1.5, most of the criteria and regularizers now return per-sample loss, which allows new samples to be given lower weight (see data loaders below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import linalg as LA\n",
    "\n",
    "from ex_color.data.color_cube import ColorCube\n",
    "from ex_color.data.cyclic import arange_cyclic\n",
    "\n",
    "\n",
    "def objective(fn):\n",
    "    \"\"\"Adapt loss function to look like a regularizer\"\"\"\n",
    "\n",
    "    def wrapper(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "        loss = fn(data, res.outputs)\n",
    "        # Reduce element-wise loss to per-sample loss by averaging over feature dimensions\n",
    "        if loss.ndim > 1:\n",
    "            # Calculate mean over all dimensions except the first (batch) dimension\n",
    "            reduce_dims = tuple(range(1, loss.ndim))\n",
    "            loss = torch.mean(loss, dim=reduce_dims)\n",
    "        return loss\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def unitarity(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "    \"\"\"Regularize latents to have unit norm (vectors of length 1)\"\"\"\n",
    "    norms = LA.vector_norm(res.latents, dim=-1)\n",
    "    # Return per-sample loss, shape [B]\n",
    "    return (norms - 1.0) ** 2\n",
    "\n",
    "\n",
    "def planarity(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "    \"\"\"Regularize latents to be planar in the first two channels (so zero in other channels)\"\"\"\n",
    "    if res.latents.shape[1] <= 2:\n",
    "        # No dimensions beyond the first two, return zero loss per sample\n",
    "        return torch.zeros(res.latents.shape[0], device=res.latents.device)\n",
    "    # Sum squares across the extra dimensions for each sample, shape [B]\n",
    "    return torch.sum(res.latents[:, 2:] ** 2, dim=-1)\n",
    "\n",
    "\n",
    "class Separate(LossCriterion):\n",
    "    def __init__(self, channels: tuple[int, ...] = (0, 1)):\n",
    "        self.channels = channels\n",
    "\n",
    "    def __call__(self, data: Tensor, res: InferenceResult) -> Tensor:\n",
    "        \"\"\"\n",
    "        Regularize latents to be separated from each other in first two channels.\n",
    "\n",
    "        Returns:\n",
    "            loss: Per-sample loss, shape [B].\n",
    "        \"\"\"\n",
    "        # Get pairwise differences in the first two dimensions\n",
    "        points = res.latents[:, self.channels]  # [B, C]\n",
    "        diffs = points.unsqueeze(1) - points.unsqueeze(0)  # [B, B, C]\n",
    "\n",
    "        # Calculate squared distances\n",
    "        sq_dists = torch.sum(diffs**2, dim=-1)  # [B, B]\n",
    "\n",
    "        # Remove self-distances (diagonal)\n",
    "        mask = 1.0 - torch.eye(sq_dists.shape[0], device=sq_dists.device)\n",
    "        masked_sq_dists = sq_dists * mask\n",
    "\n",
    "        # Encourage separation by minimizing inverse distances (stronger repulsion between close points)\n",
    "        epsilon = 1e-6  # Prevent division by zero\n",
    "        return torch.mean(1.0 / (masked_sq_dists + epsilon))\n",
    "\n",
    "\n",
    "class Anchor(SpecialLossCriterion):\n",
    "    \"\"\"Regularize latents to be close to their position in the reference phase\"\"\"\n",
    "\n",
    "    ref_data: Tensor\n",
    "    _ref_latents: Tensor | None = None\n",
    "\n",
    "    def __init__(self, ref_data: Tensor):\n",
    "        self.ref_data = ref_data\n",
    "        self._ref_latents = None\n",
    "        log.info(f'Anchor initialized with reference data shape: {ref_data.shape}')\n",
    "\n",
    "    def forward(self, model: ColorMLP, data: Tensor) -> InferenceResult | None:\n",
    "        \"\"\"Run the *stored reference data* through the *current* model.\"\"\"\n",
    "        if self._ref_latents is None:\n",
    "            # Signal to the training loop that we haven't captured latents yet\n",
    "            return None\n",
    "\n",
    "        # Note: The 'data' argument passed by the training loop for SpecialLossCriterion\n",
    "        # is the *current training batch*, which we IGNORE here.\n",
    "        # We only care about running our stored _ref_data through the model.\n",
    "        device = next(model.parameters()).device\n",
    "        ref_data = self.ref_data.to(device)\n",
    "\n",
    "        outputs, latents = model(ref_data)\n",
    "        return InferenceResult(outputs, latents)\n",
    "\n",
    "    def __call__(self, data: Tensor, special: InferenceResult) -> Tensor:\n",
    "        \"\"\"\n",
    "        Calculates loss between current model's latents (for ref_data) and the stored reference latents.\n",
    "\n",
    "        Returns:\n",
    "            loss: Mean loss, shape [] (scalar).\n",
    "        \"\"\"\n",
    "        if self._ref_latents is None:\n",
    "            # This means on_anchor hasn't been called yet, so the anchor loss is zero.\n",
    "            raise RuntimeError('Anchor.__call__ invoked before reference latents captured. Returning zero loss.')\n",
    "        ref_latents = self._ref_latents.to(special.latents.device)\n",
    "        return torch.mean((special.latents - ref_latents) ** 2)\n",
    "\n",
    "    def on_anchor(self, event: Event):\n",
    "        # Called when the 'anchor' event is triggered\n",
    "        log.info(f'Capturing anchor latents via Anchor.on_anchor at step {event.step}')\n",
    "\n",
    "        device = next(event.model.parameters()).device\n",
    "        ref_data = self.ref_data.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, latents = event.model(ref_data)\n",
    "        self._ref_latents = latents.detach().cpu()\n",
    "        log.info(f'Anchor state captured internally. Ref data: {ref_data.shape}, Ref latents: {latents.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading, sampling, and event handling\n",
    "\n",
    "Here we set up:\n",
    "\n",
    "- **Datasets:** Define the datasets used (primary/secondary colors, full color grid).\n",
    "- **Sampler:** Use `DynamicWeightedRandomBatchSampler` for the full dataset. Its weights are updated by the `update_sampler_weights` callback, which responds to the `data-fraction` parameter from the dopesheet. This smoothly shifts the sampling focus from highly vibrant colors early on to the full range of colors later.\n",
    "- **Recorders:** `ModelRecorder` and `MetricsRecorder` are event handlers that save the model state and loss values at each step.\n",
    "- **Event bindings:** Connect event handlers to specific events (e.g., `plotter` to `phase-end`, `reg_anchor.on_anchor` to `action:anchor`, recorders to `pre-step` and `step-metrics`).\n",
    "- **Training execution:** Finally, call `train_color_model` with the model, datasets, dopesheet, loss criteria, and configured event handlers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ModelRecorder(EventHandler):\n",
    "    \"\"\"Event handler to record model parameters.\"\"\"\n",
    "\n",
    "    history: list[tuple[int, dict[str, Tensor]]]\n",
    "    \"\"\"A list of tuples (step, state_dict) where state_dict is a copy of the model's state dict.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def __call__(self, event: Event):\n",
    "        # Get a *copy* of the state dict and move it to the CPU\n",
    "        # so we don't hold onto GPU memory or track gradients unnecessarily.\n",
    "        model_state = {k: v.cpu().clone() for k, v in event.model.state_dict().items()}\n",
    "        self.history.append((event.step, model_state))\n",
    "        log.debug(f'Recorded model state at step {event.step}')\n",
    "\n",
    "\n",
    "class MetricsRecorder(EventHandler):\n",
    "    \"\"\"Event handler to record training metrics.\"\"\"\n",
    "\n",
    "    history: list[tuple[int, float, dict[str, float]]]\n",
    "    \"\"\"A list of tuples (step, total_loss, losses_dict).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def __call__(self, event: StepMetricsEvent):\n",
    "        # Ensure we are handling the correct event type\n",
    "        if not isinstance(event, StepMetricsEvent):\n",
    "            log.warning(f'MetricsRecorder received unexpected event type: {type(event)}')\n",
    "            return\n",
    "\n",
    "        self.history.append((event.step, event.total_loss, event.losses.copy()))\n",
    "        log.debug(f'Recorded metrics at step {event.step}: loss={event.total_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted samples\n",
    "\n",
    "We add a data collation function, so that as the schedule progresses, new samples are given lower weight. This prevents the optimizer from being too shocked by the previously out-of-distribution data. Without this, we found it wasn't possible to get a smooth loss metric even with the gradual introduction of less-vibrant colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Callable\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "# TODO: remove forced reload\n",
    "if True:\n",
    "    import importlib\n",
    "    import ex_color.data.cube_sampler\n",
    "\n",
    "    importlib.reload(ex_color.data.cube_sampler)\n",
    "\n",
    "from ex_color.data.cube_sampler import DynamicWeightedRandomBatchSampler, Weights, vibrancy, primary_secondary_focus\n",
    "from ex_color.data.filters import levels\n",
    "\n",
    "\n",
    "def ones_collate_fn(batch):\n",
    "    \"\"\"Collate data and add a tensor of ones for weights.\"\"\"\n",
    "    # TensorDataset yields tuples like ((data_point_tensor,), index_scalar_tensor)\n",
    "    data_tuple_list = [item[0] for item in batch]  # List of (data_tensor,) tuples\n",
    "    # indices = [item[1].item() for item in batch] # We don't need indices here\n",
    "\n",
    "    collated_data = default_collate(data_tuple_list)\n",
    "    # Create weights tensor of ones, matching batch size and on the same device\n",
    "    batch_weights = torch.ones(collated_data.shape[0], dtype=torch.float32)\n",
    "    return collated_data, batch_weights\n",
    "\n",
    "\n",
    "def weighted_collate_fn(batch, *, get: Callable[[], np.ndarray]):\n",
    "    \"\"\"\n",
    "    Custom collate function that retrieves weights for the sampled indices.\n",
    "\n",
    "    Args:\n",
    "        batch: A list of ((data_tensor,), index_tensor) tuples from TensorDataset.\n",
    "               Note: TensorDataset wraps single tensors in a tuple.\n",
    "        get: A callable that returns the current full sampler weights array.\n",
    "\n",
    "    Returns:\n",
    "        A tuple: (collated_data_tensor, collated_weights_tensor)\n",
    "    \"\"\"\n",
    "    # Separate data and indices\n",
    "    # TensorDataset yields tuples like ((data_point_tensor,), index_scalar_tensor)\n",
    "    data_tuple_list = [item[0] for item in batch]  # List of (data_tensor,) tuples\n",
    "    indices = [item[1].item() for item in batch]  # List of integer indices\n",
    "\n",
    "    # Collate the data points using the default collate function\n",
    "    # default_collate handles the list of (data_tensor,) tuples correctly\n",
    "    collated_data = default_collate(data_tuple_list)\n",
    "\n",
    "    # Look up weights for the indices in this batch\n",
    "    # Ensure weights are float32 for potential multiplication with loss\n",
    "    sampler_weights = get()\n",
    "    batch_weights = torch.tensor(sampler_weights[indices], dtype=torch.float32)\n",
    "\n",
    "    # Normalize weights within the batch? Or use raw weights?\n",
    "    # Let's use raw weights for now, as they reflect the sampling probability.\n",
    "    # If weights sum to zero (unlikely but possible if all sampled points have zero weight),\n",
    "    # avoid division by zero.\n",
    "    weight_sum = batch_weights.sum()\n",
    "    if weight_sum > 1e-6:\n",
    "        batch_weights /= weight_sum\n",
    "    else:\n",
    "        # Assign uniform weight if sum is zero\n",
    "        batch_weights = torch.ones_like(batch_weights) / len(batch_weights)\n",
    "\n",
    "    return collated_data, batch_weights\n",
    "\n",
    "\n",
    "primary_cube = ColorCube.from_hsv(h=arange_cyclic(step_size=1 / 6), s=np.ones(1), v=np.ones(1))\n",
    "primary_tensor = torch.tensor(primary_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "primary_dataset = TensorDataset(primary_tensor)\n",
    "primary_loader = DataLoader(\n",
    "    primary_dataset,\n",
    "    batch_size=len(primary_tensor),\n",
    "    collate_fn=ones_collate_fn,\n",
    ")\n",
    "\n",
    "full_cube = ColorCube.from_hsv(\n",
    "    h=arange_cyclic(step_size=10 / 360),\n",
    "    s=np.linspace(0, 1, 10),\n",
    "    v=np.linspace(0, 1, 10),\n",
    ")\n",
    "full_tensor = torch.tensor(full_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "full_dataset = TensorDataset(full_tensor, torch.arange(len(full_tensor)))\n",
    "full_sampler = DynamicWeightedRandomBatchSampler(\n",
    "    bias=full_cube.bias.flatten(),\n",
    "    batch_size=32,\n",
    "    steps_per_epoch=100,\n",
    ")\n",
    "primary_secondary_weights = primary_secondary_focus(full_cube).flatten()\n",
    "vibrancy_weights = vibrancy(full_cube).flatten()\n",
    "full_loader = DataLoader(\n",
    "    full_dataset,\n",
    "    batch_sampler=full_sampler,\n",
    "    collate_fn=partial(weighted_collate_fn, get=lambda: full_sampler.weights),\n",
    ")\n",
    "\n",
    "rgb_cube = ColorCube.from_rgb(\n",
    "    r=np.linspace(0, 1, 8),\n",
    "    g=np.linspace(0, 1, 8),\n",
    "    b=np.linspace(0, 1, 8),\n",
    ")\n",
    "rgb_tensor = torch.tensor(rgb_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "\n",
    "\n",
    "def scale_weights(weights: Weights, frac: float) -> Weights:\n",
    "    # When the fraction is near zero, in_low is almost 1 — which means \"scale everything down to 0 except for 1\"\n",
    "    # When the fraction is 0.5, in_low and out_low are both 0, so the weights are unchanged\n",
    "    # When the fraction is 1, in_low is 0 and out_low is 1, so the weights are all scaled to 1\n",
    "    in_low = np.interp(frac, [0, 0.5], [0.99, 0])\n",
    "    out_low = np.interp(frac, [0.5, 1], [0, 1])\n",
    "    return levels(weights, in_low=in_low, out_low=out_low)\n",
    "\n",
    "\n",
    "def update_sampler_weights(event: Event):\n",
    "    \"\"\"Event handler to update sampler weights based on the current hyperparameters.\"\"\"\n",
    "    hue_frac = event.timeline_state.props['data-hues']\n",
    "    vibrancy_frac = event.timeline_state.props['data-vibrancies']\n",
    "\n",
    "    scaled_vibrancy_weights = scale_weights(vibrancy_weights, vibrancy_frac)\n",
    "    scaled_primary_secondary_weights = scale_weights(primary_secondary_weights, hue_frac)\n",
    "    full_sampler.weights = scaled_vibrancy_weights * scaled_primary_secondary_weights\n",
    "\n",
    "\n",
    "async def train(dopesheet: Dopesheet, variant: str):\n",
    "    \"\"\"Train the model with the given dopesheet and variant.\"\"\"\n",
    "    log.info(f'Training with {variant} dopesheet.')\n",
    "    recorder = ModelRecorder()\n",
    "    metrics_recorder = MetricsRecorder()\n",
    "\n",
    "    seed = 0\n",
    "    set_deterministic_mode(seed)\n",
    "    full_sampler.seed = seed\n",
    "\n",
    "    # Phase -> (train loader, validation tensor)\n",
    "    datasets: dict[str, tuple[DataLoader, Tensor]] = {\n",
    "        'Primary & secondary': (primary_loader, primary_tensor),\n",
    "        'All hues': (full_loader, rgb_tensor),\n",
    "        'Full color space': (full_loader, rgb_tensor),\n",
    "    }\n",
    "\n",
    "    model = ColorMLP(normalize_bottleneck=False)\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    log.info(f'Model initialized with {total_params:,} trainable parameters.')\n",
    "\n",
    "    event_handlers = EventHandlers()\n",
    "    event_handlers.pre_step.add_handler('pre-step', recorder)\n",
    "    event_handlers.pre_step.add_handler('pre-step', update_sampler_weights)\n",
    "    event_handlers.step_metrics.add_handler('step-metrics', metrics_recorder)\n",
    "\n",
    "    plotter = PhasePlotter(dim_pairs=[(0, 1), (0, 2), (0, 3)], variant=variant)\n",
    "    event_handlers.phase_end.add_handler('phase-end', plotter)\n",
    "\n",
    "    reg_anchor = Anchor(ref_data=primary_tensor)\n",
    "    event_handlers.action.add_handler('action:anchor', reg_anchor.on_anchor)\n",
    "\n",
    "    await train_color_model(\n",
    "        model,\n",
    "        datasets,\n",
    "        dopesheet,\n",
    "        loss_criteria={\n",
    "            'loss-recon': objective(nn.MSELoss(reduction='none')),  # No reduction; allows per-sample loss weights\n",
    "            'reg-separate': Separate((0, 1)),\n",
    "            'reg-planar': planarity,\n",
    "            'reg-norm': unitarity,\n",
    "            'reg-anchor': reg_anchor,\n",
    "        },\n",
    "        event_handlers=event_handlers,\n",
    "    )\n",
    "\n",
    "    return recorder, metrics_recorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training two models\n",
    "\n",
    "We'll run the training loop twice: once for the smooth curriculum, and again for the stepped one. We'll use the same random seed to make them as closely comparable as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 3345.2 no.1.6:Training with smooth dopesheet.\n",
      "I 3345.2 no.1.6:Global random seed set to 0\n",
      "I 3345.2 no.1.6:PyTorch set to deterministic mode\n",
      "I 3345.2 no.1.6:Model initialized with 263 trainable parameters.\n",
      "I 3345.2 no.1.6:Anchor initialized with reference data shape: torch.Size([6, 3])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 100%; padding: 5px 0; font-family: monospace;\">\n",
       "            <!-- Progress bar container -->\n",
       "            <div style=\"position: relative; height: calc(1em * 5/3); width: 100%;\">\n",
       "                <!-- Triangle indicator -->\n",
       "                <div style=\"position: absolute; bottom: -4px; left: calc(100.0% - 4px);\">\n",
       "                    <div style=\"\n",
       "                        width: 0;\n",
       "                        height: 0;\n",
       "                        border-left: 4px solid transparent;\n",
       "                        border-right: 4px solid transparent;\n",
       "                        border-bottom: 4px solid currentColor;\n",
       "                    \"></div>\n",
       "                </div>\n",
       "                <!-- Progress bar -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    height: 100%;\n",
       "                    width: 100.0%;\n",
       "                    background-color: color(from currentColor srgb r g b / 0.1);\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                \"></div>\n",
       "                <!-- Text overlay -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    width: 100%;\n",
       "                    height: 100%;\n",
       "                    text-align: center;\n",
       "                    line-height: calc(1em * 5/3);\n",
       "                    font-size: 0.9em;\n",
       "                    white-space: nowrap;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5);\n",
       "                \">\n",
       "                    <b>Training Steps</b>: 100.0% [<b>10001</b>/10001] [<b>00:26</b>/<00:00, 378.04 it/s]\n",
       "                </div>\n",
       "            </div>\n",
       "        \n",
       "            <div style=\"\n",
       "                display: grid;\n",
       "                grid-template-columns: repeat(6, minmax(80px, 1fr));\n",
       "                gap: 5px 0px;\n",
       "                width: 100%;\n",
       "                margin-top: 10px;\n",
       "                font-size: 0.85em;\n",
       "            \"><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">PHASE</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">lr</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">loss</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">loss-recon</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">reg-norm</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">reg-anchor</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">Full color space</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.001000</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0000</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0000</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0000</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0000</div></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 3346.3 no.1.6:Plotting end of phase: Primary & secondary at step 499 using provided results.\n",
      "I 3346.5 ut.nb:Figure saved: 'large-assets/ex-1.6-color-phase-history-smooth.png'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.6-color-phase-history-smooth.png?v=IcRIlNKMdWoj4YR2o-RNDUUrpyzRV9fJSciFm9jL5Nk\" alt=\"Visualizations of latent space at the end of each smooth curriculum phase.\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 3346.5 no.1.6:Capturing anchor latents via Anchor.on_anchor at step 500\n",
      "I 3346.5 no.1.6:Anchor state captured internally. Ref data: torch.Size([6, 3]), Ref latents: torch.Size([6, 4])\n",
      "I 3356.1 no.1.6:Plotting end of phase: All hues at step 4999 using provided results.\n",
      "I 3356.4 ut.nb:Figure saved: 'large-assets/ex-1.6-color-phase-history-smooth.png'\n",
      "I 3370.2 no.1.6:Plotting end of phase: Full color space at step 10000 using provided results.\n",
      "I 3371.6 ut.nb:Figure saved: 'large-assets/ex-1.6-color-phase-history-smooth.png'\n",
      "I 3371.6 no.1.6:Training finished!\n"
     ]
    }
   ],
   "source": [
    "smooth_recorder, smooth_metrics = await train(smooth_dopesheet, 'smooth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 3385.8 no.1.6:Training with stepped dopesheet.\n",
      "I 3385.8 no.1.6:Global random seed set to 0\n",
      "I 3385.8 no.1.6:PyTorch set to deterministic mode\n",
      "I 3385.8 no.1.6:Model initialized with 263 trainable parameters.\n",
      "I 3385.8 no.1.6:Anchor initialized with reference data shape: torch.Size([6, 3])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 100%; padding: 5px 0; font-family: monospace;\">\n",
       "            <!-- Progress bar container -->\n",
       "            <div style=\"position: relative; height: calc(1em * 5/3); width: 100%;\">\n",
       "                <!-- Triangle indicator -->\n",
       "                <div style=\"position: absolute; bottom: -4px; left: calc(100.0% - 4px);\">\n",
       "                    <div style=\"\n",
       "                        width: 0;\n",
       "                        height: 0;\n",
       "                        border-left: 4px solid transparent;\n",
       "                        border-right: 4px solid transparent;\n",
       "                        border-bottom: 4px solid currentColor;\n",
       "                    \"></div>\n",
       "                </div>\n",
       "                <!-- Progress bar -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    height: 100%;\n",
       "                    width: 100.0%;\n",
       "                    background-color: color(from currentColor srgb r g b / 0.1);\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                \"></div>\n",
       "                <!-- Text overlay -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    width: 100%;\n",
       "                    height: 100%;\n",
       "                    text-align: center;\n",
       "                    line-height: calc(1em * 5/3);\n",
       "                    font-size: 0.9em;\n",
       "                    white-space: nowrap;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5);\n",
       "                \">\n",
       "                    <b>Training Steps</b>: 100.0% [<b>10001</b>/10001] [<b>00:26</b>/<00:00, 379.69 it/s]\n",
       "                </div>\n",
       "            </div>\n",
       "        \n",
       "            <div style=\"\n",
       "                display: grid;\n",
       "                grid-template-columns: repeat(6, minmax(80px, 1fr));\n",
       "                gap: 5px 0px;\n",
       "                width: 100%;\n",
       "                margin-top: 10px;\n",
       "                font-size: 0.85em;\n",
       "            \"><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">PHASE</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">lr</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">loss</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">loss-recon</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">reg-norm</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">reg-anchor</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">Full color space</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.001000</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0000</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0000</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0000</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0000</div></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 3386.1 no.1.6:Plotting end of phase: Primary & secondary at step 99 using provided results.\n",
      "I 3386.2 ut.nb:Figure saved: 'large-assets/ex-1.6-color-phase-history-stepped.png'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.6-color-phase-history-stepped.png?v=WXk73MmsEHlTwll_4UBxjDqXzqZuhcJIDup8oQ-6QeU\" alt=\"Visualizations of latent space at the end of each stepped curriculum phase.\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 3386.7 no.1.6:Plotting end of phase: Primary & secondary at step 299 using provided results.\n",
      "I 3387.0 ut.nb:Figure saved: 'large-assets/ex-1.6-color-phase-history-stepped.png'\n",
      "I 3387.3 no.1.6:Plotting end of phase: Primary & secondary at step 499 using provided results.\n",
      "I 3387.7 ut.nb:Figure saved: 'large-assets/ex-1.6-color-phase-history-stepped.png'\n",
      "I 3387.8 no.1.6:Capturing anchor latents via Anchor.on_anchor at step 500\n",
      "I 3387.8 no.1.6:Anchor state captured internally. Ref data: torch.Size([6, 3]), Ref latents: torch.Size([6, 4])\n",
      "I 3389.1 no.1.6:Plotting end of phase: All hues at step 999 using provided results.\n",
      "I 3389.7 ut.nb:Figure saved: 'large-assets/ex-1.6-color-phase-history-stepped.png'\n",
      "I 3397.9 no.1.6:Plotting end of phase: All hues at step 4999 using provided results.\n",
      "I 3398.7 ut.nb:Figure saved: 'large-assets/ex-1.6-color-phase-history-stepped.png'\n",
      "I 3401.9 no.1.6:Plotting end of phase: Full color space at step 6499 using provided results.\n",
      "I 3402.9 ut.nb:Figure saved: 'large-assets/ex-1.6-color-phase-history-stepped.png'\n",
      "I 3410.9 no.1.6:Plotting end of phase: Full color space at step 10000 using provided results.\n",
      "I 3412.2 ut.nb:Figure saved: 'large-assets/ex-1.6-color-phase-history-stepped.png'\n",
      "I 3412.2 no.1.6:Training finished!\n"
     ]
    }
   ],
   "source": [
    "stepped_recorder, stepped_metrics = await train(stepped_dopesheet, 'stepped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models trained fairly well! There are some differences, but they look like they have similar characteristics. Surprisingly, the smooth variant seemed to have a _noisier_ (i.e. worse) latent space at the end of the _All hues_ phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space evolution analysis\n",
    "\n",
    "Let's visualize how the latent spaces evolved over time. Like Ex 1.5, we'll use the `ModelRecorder`'s history to load the model state at each recorded step and evaluate the latent positions for a fixed set of input colors (the full RGB grid). This gives us a sequence of latent space snapshots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 100%; padding: 5px 0; font-family: monospace;\">\n",
       "            <!-- Progress bar container -->\n",
       "            <div style=\"position: relative; height: calc(1em * 5/3); width: 100%;\">\n",
       "                <!-- Triangle indicator -->\n",
       "                <div style=\"position: absolute; bottom: -4px; left: calc(100.0% - 4px);\">\n",
       "                    <div style=\"\n",
       "                        width: 0;\n",
       "                        height: 0;\n",
       "                        border-left: 4px solid transparent;\n",
       "                        border-right: 4px solid transparent;\n",
       "                        border-bottom: 4px solid currentColor;\n",
       "                    \"></div>\n",
       "                </div>\n",
       "                <!-- Progress bar -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    height: 100%;\n",
       "                    width: 100.0%;\n",
       "                    background-color: color(from currentColor srgb r g b / 0.1);\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                \"></div>\n",
       "                <!-- Text overlay -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    width: 100%;\n",
       "                    height: 100%;\n",
       "                    text-align: center;\n",
       "                    line-height: calc(1em * 5/3);\n",
       "                    font-size: 0.9em;\n",
       "                    white-space: nowrap;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5);\n",
       "                \">\n",
       "                    <b>Evaluating latents</b>: 100.0% [<b>10001</b>/10001] [<b>00:08</b>/<00:00, 1248.65 it/s]\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 100%; padding: 5px 0; font-family: monospace;\">\n",
       "            <!-- Progress bar container -->\n",
       "            <div style=\"position: relative; height: calc(1em * 5/3); width: 100%;\">\n",
       "                <!-- Triangle indicator -->\n",
       "                <div style=\"position: absolute; bottom: -4px; left: calc(100.0% - 4px);\">\n",
       "                    <div style=\"\n",
       "                        width: 0;\n",
       "                        height: 0;\n",
       "                        border-left: 4px solid transparent;\n",
       "                        border-right: 4px solid transparent;\n",
       "                        border-bottom: 4px solid currentColor;\n",
       "                    \"></div>\n",
       "                </div>\n",
       "                <!-- Progress bar -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    height: 100%;\n",
       "                    width: 100.0%;\n",
       "                    background-color: color(from currentColor srgb r g b / 0.1);\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                \"></div>\n",
       "                <!-- Text overlay -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    width: 100%;\n",
       "                    height: 100%;\n",
       "                    text-align: center;\n",
       "                    line-height: calc(1em * 5/3);\n",
       "                    font-size: 0.9em;\n",
       "                    white-space: nowrap;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5);\n",
       "                \">\n",
       "                    <b>Evaluating latents</b>: 100.0% [<b>10001</b>/10001] [<b>00:07</b>/<00:00, 1387.89 it/s]\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "async def eval_latent_history(\n",
    "    recorder: ModelRecorder,\n",
    "    rgb_tensor: Tensor,\n",
    "):\n",
    "    \"\"\"Evaluate the latent space for each step in the recorder's history.\"\"\"\n",
    "    # Create a new model instance\n",
    "    from utils.progress import co_op, AsyncProgress\n",
    "\n",
    "    model = ColorMLP(normalize_bottleneck=False)\n",
    "\n",
    "    latent_history: list[tuple[int, np.ndarray]] = []\n",
    "    # Iterate over the recorded history\n",
    "    async for step, state_dict in co_op(AsyncProgress(recorder.history, description='Evaluating latents')):\n",
    "        # Load the model state dict\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get the latents for the RGB tensor\n",
    "            _, latents = model(rgb_tensor.to(next(model.parameters()).device))\n",
    "            latents = latents.cpu().numpy()\n",
    "            latent_history.append((step, latents))\n",
    "    return latent_history\n",
    "\n",
    "\n",
    "smooth_latents = await eval_latent_history(smooth_recorder, rgb_tensor)\n",
    "stepped_latents = await eval_latent_history(stepped_recorder, rgb_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation of latent space\n",
    "\n",
    "This final visualization combines multiple views into a single animation:\n",
    "\n",
    "- **Latent space:** Shows the 2D projection (Dims 0 vs 1) of the latent embeddings for the full RGB color grid, colored by their true RGB values. We can see the color wheel forming.\n",
    "- **Hyperparameters:** Replots the parameter schedule from the dopesheet, with a vertical line indicating the current step in the animation.\n",
    "- **Training metrics:** Plots the total loss and the contribution of each individual loss/regularization term (on a log scale), again with a vertical line for the current step.\n",
    "\n",
    "_(Note: A variable stride is used for sampling frames to focus on periods of rapid change.)_\n",
    "\n",
    "The smooth training run is shown on the left, and the stepped run on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W 3444.3 ma.ax._b:Ignoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\n",
      "W 3444.3 ma.ax._b:Ignoring fixed x limits to fulfill fixed data aspect with adjustable data limits.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 100%; padding: 5px 0; font-family: monospace;\">\n",
       "            <!-- Progress bar container -->\n",
       "            <div style=\"position: relative; height: calc(1em * 5/3); width: 100%;\">\n",
       "                <!-- Triangle indicator -->\n",
       "                <div style=\"position: absolute; bottom: -4px; left: calc(100.0% - 4px);\">\n",
       "                    <div style=\"\n",
       "                        width: 0;\n",
       "                        height: 0;\n",
       "                        border-left: 4px solid transparent;\n",
       "                        border-right: 4px solid transparent;\n",
       "                        border-bottom: 4px solid currentColor;\n",
       "                    \"></div>\n",
       "                </div>\n",
       "                <!-- Progress bar -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    height: 100%;\n",
       "                    width: 100.0%;\n",
       "                    background-color: color(from currentColor srgb r g b / 0.1);\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                \"></div>\n",
       "                <!-- Text overlay -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    width: 100%;\n",
       "                    height: 100%;\n",
       "                    text-align: center;\n",
       "                    line-height: calc(1em * 5/3);\n",
       "                    font-size: 0.9em;\n",
       "                    white-space: nowrap;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5);\n",
       "                \">\n",
       "                    <b>Rendering comparison video</b>: 100.0% [<b>1342</b>/1342] [<b>03:07</b>/<00:00, 7.14 it/s]\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <video width=\"960\" height=\"540\" controls loop>\n",
       "            <source src=\"large-assets/ex-1.6-latent-evolution-comparison.mp4?v=VJOY8GVJFquZ4Hg9LkmCNFIly07b-CRCo9WgeSPlKZo\" type=\"video/mp4\">\n",
       "            Your browser does not support the video tag.\n",
       "        </video>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import imageio_ffmpeg\n",
    "from matplotlib import rcParams\n",
    "import pandas as pd\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from mini.temporal.dopesheet import RESERVED_COLS\n",
    "from utils.progress import SyncProgress\n",
    "\n",
    "# TODO: remove forced reload\n",
    "import importlib\n",
    "import mini.temporal.vis\n",
    "\n",
    "importlib.reload(mini.temporal.vis)\n",
    "from mini.temporal.vis import group_properties_by_scale, plot_timeline\n",
    "\n",
    "rcParams['animation.ffmpeg_path'] = imageio_ffmpeg.get_ffmpeg_exe()\n",
    "\n",
    "\n",
    "def animate_latent_evolution_with_metrics(\n",
    "    # Smooth variant data\n",
    "    smooth_latent_history: list[tuple[int, np.ndarray]],\n",
    "    smooth_metrics_history: list[tuple[int, float, dict[str, float]]],\n",
    "    smooth_param_history_df: pd.DataFrame,\n",
    "    smooth_param_keyframes_df: pd.DataFrame,\n",
    "    # Stepped variant data\n",
    "    stepped_latent_history: list[tuple[int, np.ndarray]],\n",
    "    stepped_metrics_history: list[tuple[int, float, dict[str, float]]],\n",
    "    stepped_param_history_df: pd.DataFrame,\n",
    "    stepped_param_keyframes_df: pd.DataFrame,\n",
    "    # Common data and settings\n",
    "    colors: np.ndarray,\n",
    "    dim_pair: tuple[int, int] = (0, 1),\n",
    "    interval=1 / 30,\n",
    "):\n",
    "    \"\"\"Create a side-by-side animation of latent space evolution, hyperparameters, and metrics.\"\"\"\n",
    "    plt.style.use('dark_background')\n",
    "    # Aim for 16:9 aspect ratio, give latent plots more height\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "    # Use the height ratios from your latest version\n",
    "    gs = GridSpec(3, 2, height_ratios=[5, 1, 1], width_ratios=[1, 1], hspace=0, wspace=0.02)\n",
    "\n",
    "    # --- Create Axes ---\n",
    "    # Latent plots (Top row) - No sharing needed initially\n",
    "    ax_latent_s = fig.add_subplot(gs[0, 0])\n",
    "    ax_latent_t = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    # Parameter plots (Middle row) - Share x-axis with metrics plot BELOW\n",
    "    ax_params_s = fig.add_subplot(gs[1, 0])\n",
    "    ax_params_t = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "    # Metrics plots (Bottom row) - Share x-axis with parameter plot ABOVE\n",
    "    ax_metrics_s = fig.add_subplot(gs[2, 0], sharex=ax_params_s)\n",
    "    ax_metrics_t = fig.add_subplot(gs[2, 1], sharex=ax_params_t)\n",
    "\n",
    "    fig.patch.set_facecolor('#333')\n",
    "    all_axes = [ax_latent_s, ax_params_s, ax_metrics_s, ax_latent_t, ax_params_t, ax_metrics_t]\n",
    "    for ax in all_axes:\n",
    "        ax.patch.set_facecolor('#222')\n",
    "\n",
    "    latent_lim = 1.1\n",
    "\n",
    "    # --- Setup Smooth Plots (Left Column) ---\n",
    "    step_s, current_latents_s = smooth_latent_history[0]\n",
    "    ax_latent_s.set_xlim(-latent_lim, latent_lim)\n",
    "    ax_latent_s.set_ylim(-latent_lim, latent_lim)\n",
    "    ax_latent_s.set_aspect('equal', adjustable='datalim')\n",
    "    # ax_latent_s.set_xlabel(f'Dim {dim_pair[0]}') # Set X label for latent plot\n",
    "    ax_latent_s.tick_params(axis='x', labelleft=False)  # Hide x labels\n",
    "    plt.setp(ax_latent_s.get_xticklabels(), visible=False)\n",
    "    # ax_latent_s.set_ylabel(f'Dim {dim_pair[1]}')\n",
    "    ax_latent_s.set_ylabel('Latent space')\n",
    "    ax_latent_s.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax_latent_s.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax_latent_s.add_patch(Circle((0, 0), 1, fill=False, linestyle='--', color='gray', alpha=0.3))\n",
    "    scatter_s = ax_latent_s.scatter(\n",
    "        current_latents_s[:, dim_pair[0]], current_latents_s[:, dim_pair[1]], c=colors, s=150, alpha=0.7\n",
    "    )\n",
    "    title_latent_s = ax_latent_s.set_title('placeholder')  # Title set in update()\n",
    "    # No need to hide x-ticks here anymore\n",
    "\n",
    "    param_props_s = smooth_param_keyframes_df.columns.difference(list(RESERVED_COLS)).tolist()\n",
    "    param_groups_s = group_properties_by_scale(smooth_param_keyframes_df[param_props_s])\n",
    "    # Pass show_legend=False, show_phase_labels=False as you did\n",
    "    plot_timeline(\n",
    "        smooth_param_history_df,\n",
    "        smooth_param_keyframes_df,\n",
    "        [param_groups_s[0]],\n",
    "        ax=ax_params_s,\n",
    "        show_legend=False,\n",
    "        show_phase_labels=False,\n",
    "        line_styles=line_styles,\n",
    "    )\n",
    "    param_vline_s = ax_params_s.axvline(step_s, color='white', linestyle='--', lw=1)\n",
    "    ax_params_s.set_ylabel('Param value', fontsize='x-small')\n",
    "    ax_params_s.set_xlabel('')  # Remove xlabel, it will be on the plot below\n",
    "    # Hide x-tick labels because they are shared with the plot below\n",
    "    plt.setp(ax_params_s.get_xticklabels(), visible=False)\n",
    "\n",
    "    metrics_steps_s = [h[0] for h in smooth_metrics_history]\n",
    "    total_losses_s = [h[1] for h in smooth_metrics_history]\n",
    "    loss_components_s = {\n",
    "        k: [h[2].get(k, np.nan) for h in smooth_metrics_history] for k in smooth_metrics_history[0][2].keys()\n",
    "    }\n",
    "    ax_metrics_s.plot(metrics_steps_s, total_losses_s, label='Total Loss', lw=latent_lim)\n",
    "    for name, values in loss_components_s.items():\n",
    "        ax_metrics_s.plot(metrics_steps_s, values, label=name, lw=1, alpha=0.8)\n",
    "    ax_metrics_s.set_xlabel('Step')  # Set X label for the bottom plot\n",
    "    ax_metrics_s.set_ylabel('Loss (log)', fontsize='x-small')\n",
    "    ax_metrics_s.set_yscale('log')\n",
    "    ax_metrics_s.set_ylim(bottom=1e-6)\n",
    "    metrics_vline_s = ax_metrics_s.axvline(step_s, color='white', linestyle='--', lw=1)\n",
    "\n",
    "    # --- Setup Stepped Plots (Right Column) ---\n",
    "    step_t, current_latents_t = stepped_latent_history[0]\n",
    "    ax_latent_t.set_xlim(-latent_lim, latent_lim)\n",
    "    ax_latent_t.set_ylim(-latent_lim, latent_lim)\n",
    "    ax_latent_t.set_aspect('equal', adjustable='datalim')\n",
    "    # ax_latent_t.set_xlabel(f'Dim {dim_pair[0]}') # Set X label for latent plot\n",
    "    ax_latent_t.tick_params(axis='x', labelleft=False)  # Hide x labels\n",
    "    plt.setp(ax_latent_t.get_xticklabels(), visible=False)\n",
    "    ax_latent_t.tick_params(axis='y', labelleft=False)  # Hide y labels\n",
    "    ax_latent_t.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax_latent_t.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax_latent_t.add_patch(Circle((0, 0), 1, fill=False, linestyle='--', color='gray', alpha=0.3))\n",
    "    scatter_t = ax_latent_t.scatter(\n",
    "        current_latents_t[:, dim_pair[0]], current_latents_t[:, dim_pair[1]], c=colors, s=150, alpha=0.7\n",
    "    )\n",
    "    title_latent_t = ax_latent_t.set_title('placeholder')  # Title set in update()\n",
    "    # No need to hide x-ticks here anymore\n",
    "\n",
    "    param_props_t = stepped_param_keyframes_df.columns.difference(list(RESERVED_COLS)).tolist()\n",
    "    param_groups_t = group_properties_by_scale(stepped_param_keyframes_df[param_props_t])\n",
    "    # Pass show_legend=False, show_phase_labels=False as you did\n",
    "    plot_timeline(\n",
    "        stepped_param_history_df,\n",
    "        stepped_param_keyframes_df,\n",
    "        [param_groups_t[0]],\n",
    "        ax=ax_params_t,\n",
    "        show_legend=False,\n",
    "        show_phase_labels=False,\n",
    "        line_styles=line_styles,\n",
    "    )\n",
    "    param_vline_t = ax_params_t.axvline(step_t, color='white', linestyle='--', lw=1)\n",
    "    ax_params_t.set_ylabel('')  # Y label only on left\n",
    "    ax_params_t.set_xlabel('')  # Remove xlabel, it will be on the plot below\n",
    "    # Hide x-tick labels because they are shared with the plot below\n",
    "    plt.setp(ax_params_t.get_xticklabels(), visible=False)\n",
    "    ax_params_t.tick_params(axis='y', labelleft=False)  # Hide y labels\n",
    "\n",
    "    metrics_steps_t = [h[0] for h in stepped_metrics_history]\n",
    "    total_losses_t = [h[1] for h in stepped_metrics_history]\n",
    "    loss_components_t = {\n",
    "        k: [h[2].get(k, np.nan) for h in stepped_metrics_history] for k in stepped_metrics_history[0][2].keys()\n",
    "    }\n",
    "    ax_metrics_t.plot(metrics_steps_t, total_losses_t, label='Total Loss', lw=1.5)\n",
    "    for name, values in loss_components_t.items():\n",
    "        ax_metrics_t.plot(metrics_steps_t, values, label=name, lw=1, alpha=0.8)\n",
    "    ax_metrics_t.set_xlabel('Step')  # Set X label for the bottom plot\n",
    "    ax_metrics_t.set_yscale('log')\n",
    "    ax_metrics_t.set_ylim(bottom=1e-6)\n",
    "    ax_metrics_t.tick_params(axis='y', labelleft=False)  # Hide y labels\n",
    "    metrics_vline_t = ax_metrics_t.axvline(step_t, color='white', linestyle='--', lw=1)\n",
    "\n",
    "    # --- Set common X limits ---\n",
    "    # Only set xlim for the timeline plots (params and metrics)\n",
    "    max_step = max(smooth_param_history_df['STEP'].max(), stepped_param_history_df['STEP'].max())\n",
    "    for ax in [ax_params_s, ax_metrics_s, ax_params_t, ax_metrics_t]:\n",
    "        ax.set_xlim(left=0, right=max_step)\n",
    "\n",
    "    # fig.tight_layout(h_pad=0, w_pad=0.5)  # Adjust padding\n",
    "    fig.subplots_adjust(\n",
    "        left=0.05,  # Smaller left margin\n",
    "        right=0.95,  # Smaller right margin\n",
    "        bottom=0.08,  # Smaller bottom margin (leave room for x-label)\n",
    "        top=0.95,  # Smaller top margin (leave room for titles)\n",
    "        wspace=0.1,  # Adjust space between columns (tweak as needed)\n",
    "        hspace=0.0,  # Keep vertical space at 0 (set in GridSpec)\n",
    "    )\n",
    "\n",
    "    def update(frame: int):\n",
    "        # ... (update logic remains the same) ...\n",
    "        # Assume smooth and stepped histories have the same length and aligned steps after sampling\n",
    "        smooth_step, current_latents_s = smooth_latent_history[frame]\n",
    "        stepped_step, current_latents_t = stepped_latent_history[frame]\n",
    "        # Use the smooth step for titles and lines, assuming they are aligned\n",
    "        current_step = smooth_step\n",
    "\n",
    "        # Update smooth plots\n",
    "        scatter_s.set_offsets(current_latents_s[:, dim_pair])\n",
    "        title_latent_s.set_text(f'Smooth curriculum (step {current_step})')  # Use current_step\n",
    "        param_vline_s.set_xdata([current_step])\n",
    "        metrics_vline_s.set_xdata([current_step])\n",
    "\n",
    "        # Update stepped plots\n",
    "        scatter_t.set_offsets(current_latents_t[:, dim_pair])\n",
    "        title_latent_t.set_text(f'Stepped curriculum (step {current_step})')  # Use current_step\n",
    "        param_vline_t.set_xdata([current_step])\n",
    "        metrics_vline_t.set_xdata([current_step])\n",
    "\n",
    "        return (\n",
    "            scatter_s,\n",
    "            title_latent_s,\n",
    "            param_vline_s,\n",
    "            metrics_vline_s,\n",
    "            scatter_t,\n",
    "            title_latent_t,\n",
    "            param_vline_t,\n",
    "            metrics_vline_t,\n",
    "        )\n",
    "\n",
    "    # Use the length of the (potentially strided) latent_history for frames\n",
    "    # Assuming both histories have the same length after sampling\n",
    "    num_frames = len(smooth_latent_history)\n",
    "    ani = animation.FuncAnimation(fig, update, frames=num_frames, interval=interval * 1000, blit=True)\n",
    "    return fig, ani\n",
    "\n",
    "\n",
    "# --- Variable Stride Logic ---\n",
    "def get_stride(step: int):\n",
    "    import math\n",
    "\n",
    "    a = 7.9236\n",
    "    b = 0.0005\n",
    "    # Ensure stride is at least 1\n",
    "    return max(1.0, a * math.log(b * step + 1) + 1)\n",
    "\n",
    "\n",
    "# Apply stride logic based on smooth history (assuming stepped is similar)\n",
    "sampled_indices = [0]\n",
    "last_sampled_index = 0\n",
    "# Use smooth_latents for stride calculation\n",
    "while True:\n",
    "    current_step = smooth_latents[round(last_sampled_index)][0]\n",
    "    stride = get_stride(current_step)\n",
    "    next_index = last_sampled_index + stride\n",
    "    # Ensure indices stay within bounds for *both* histories\n",
    "    if round(next_index) >= len(smooth_latents) or round(next_index) >= len(stepped_latents):\n",
    "        break\n",
    "    sampled_indices.append(round(next_index))\n",
    "    last_sampled_index = next_index\n",
    "\n",
    "# Ensure the last frame is included if missed\n",
    "if sampled_indices[-1] < len(smooth_latents) - 1:\n",
    "    sampled_indices.append(len(smooth_latents) - 1)\n",
    "\n",
    "# sampled_indices = sampled_indices[:200]  # Limit the number of samples during development\n",
    "\n",
    "# Sample both latent histories using the same indices\n",
    "sampled_smooth_latents = [smooth_latents[i] for i in sampled_indices]\n",
    "sampled_stepped_latents = [stepped_latents[i] for i in sampled_indices]\n",
    "\n",
    "# --- End Variable Stride Logic ---\n",
    "\n",
    "# Filter metrics history to align with the *new* sampled latent history steps\n",
    "# Use steps from the sampled smooth history (assuming alignment)\n",
    "sampled_steps_set = {step for step, _ in sampled_smooth_latents}\n",
    "filtered_smooth_metrics = [h for h in smooth_metrics.history if h[0] in sampled_steps_set]\n",
    "filtered_stepped_metrics = [h for h in stepped_metrics.history if h[0] in sampled_steps_set]\n",
    "\n",
    "# Realize timelines for both dopesheets\n",
    "smooth_timeline = Timeline(smooth_dopesheet)\n",
    "smooth_history_df = realize_timeline(smooth_timeline)\n",
    "smooth_keyframes_df = smooth_dopesheet.as_df()\n",
    "\n",
    "stepped_timeline = Timeline(stepped_dopesheet)\n",
    "stepped_history_df = realize_timeline(stepped_timeline)\n",
    "stepped_keyframes_df = stepped_dopesheet.as_df()\n",
    "\n",
    "\n",
    "# --- Call the updated animation function ---\n",
    "fig, ani = animate_latent_evolution_with_metrics(\n",
    "    # Smooth\n",
    "    smooth_latent_history=sampled_smooth_latents,\n",
    "    smooth_metrics_history=filtered_smooth_metrics,\n",
    "    smooth_param_history_df=smooth_history_df,\n",
    "    smooth_param_keyframes_df=smooth_keyframes_df,\n",
    "    # Stepped\n",
    "    stepped_latent_history=sampled_stepped_latents,\n",
    "    stepped_metrics_history=filtered_stepped_metrics,\n",
    "    stepped_param_history_df=stepped_history_df,\n",
    "    stepped_param_keyframes_df=stepped_keyframes_df,\n",
    "    # Common\n",
    "    colors=rgb_tensor.cpu().numpy(),\n",
    "    dim_pair=(0, 1),\n",
    ")\n",
    "\n",
    "# --- Save the video ---\n",
    "video_file = f'large-assets/ex-{nbid}-latent-evolution-comparison.mp4'  # Updated filename\n",
    "num_frames_to_render = len(sampled_smooth_latents)  # Base on sampled length\n",
    "with SyncProgress(total=num_frames_to_render, description='Rendering comparison video') as pbar:\n",
    "    ani.save(\n",
    "        video_file,\n",
    "        fps=30,\n",
    "        extra_args=['-vcodec', 'libx264'],\n",
    "        progress_callback=lambda i, n: pbar.update(total=n, count=i),\n",
    "    )\n",
    "plt.close(fig)\n",
    "\n",
    "# --- Display the video ---\n",
    "import secrets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "cache_buster = secrets.token_urlsafe()\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        f\"\"\"\n",
    "        <video width=\"960\" height=\"540\" controls loop>\n",
    "            <source src=\"{video_file}?v={cache_buster}\" type=\"video/mp4\">\n",
    "            Your browser does not support the video tag.\n",
    "        </video>\n",
    "        \"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "Qualitatively, we observe that:\n",
    "- The Smooth variant seems noisier overall: it's more jittery in general, and becomes more misshapen during the _All hues_ phase. This might be due to the specific values of the hyperparameters, e.g. maybe the normalization loss was too high.\n",
    "- The Stepped variant does indeed show loss spikes at the start of each phase, while the Smooth varint does not — as predicted! However, the spikes don't seem to cause any problem; perhaps they were fully mitigated by the LR warmup.\n",
    "- Even though the data are introduced to each variant differently (in chunks to the Stepped variant, and gradually to the Smooth variant), the effect is almost identical. This is particularly apparent at the start of the _Full color space_ phase: the Stepped variant bulges suddenly at the start of the phase, while the Smooth variant bulges a little later and somewhat less violently — but both end up in almost the exact same shape.\n",
    "\n",
    "Perhaps the dynamics and final latent space could be improved for the Smooth curriculum by reducing the learning rate at times when the parameters are changing a lot — but since per-phase LR schedules are already common in curriculum learning, using them _in addition_ to smooth parameter changes may not have much benefit. On the other hand, we note that the smooth curriculum was easier to specify than the stepped one, purely because it had fewer phases and fewer keyframes.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Our hypothesis seems to have been wrong: smooth parameter changes _don't_ appear to improve training dynamics compared to a traditional curriculum."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex-color-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
